{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The following uses a kind of literate programming approach to build a library of tools useful for writing unit and integration tests directly into a notebook. The library is to be articulated as a Python package built as the concatenation of a subset of the code cells of this notebook, using an ad hoc script. To help with identifying which code cells are parts of the final package and which are inline testing code, we use *tags*, which make up cell metadata in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%flake8_on --max_line_length 120 --ignore E302"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from copy import copy\n",
    "from inspect import getframeinfo, Traceback, unwrap\n",
    "from io import TextIOBase\n",
    "import itertools\n",
    "from linecache import getline\n",
    "import operator\n",
    "import re\n",
    "import sys\n",
    "from traceback import walk_tb\n",
    "from typing import Dict, List, Tuple, Iterator, Union, Iterable, Optional, Any, Callable, Mapping, Sequence, cast,\\\n",
    "        Container\n",
    "\n",
    "import colors\n",
    "from IPython import get_ipython\n",
    "from IPython.core.magic import register_cell_magic\n",
    "from pygments import highlight\n",
    "from pygments.lexers import Python3Lexer\n",
    "from pygments.formatters import TerminalFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Result(ABC):\n",
    "    \"\"\"\n",
    "    Result of a test. Indicates whether the test passed (was a success), and if it did not,\n",
    "    whether it was a failure (as opposed to any other kind of issue).\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def is_success(self) -> bool:\n",
    "        \"\"\"True when an associated test run has passed.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def is_failure(self) -> bool:\n",
    "        \"\"\"True when an associated has not passed because a designed failure condition was met.\"\"\"\n",
    "        return False\n",
    "\n",
    "    def as_dict(self) -> Dict:\n",
    "        \"\"\"Expresses this result as a dictionary suitable to structured data serialization.\"\"\"\n",
    "        return {\"type\": type(self).__name__}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual test failure\n",
    "\n",
    "Tests can be made to fail deliberately by raising a special exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class TestFailed(Exception):\n",
    "    \"\"\"\n",
    "    Exception raised by this framework in order to mark a test run as a Failure.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, reason: str) -> None:\n",
    "        super().__init__(reason)\n",
    "        self.reason = reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    raise TestFailed(\"asdf\")\n",
    "except TestFailed as err:\n",
    "    assert str(err) == \"asdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def fail(reason: str = \"\"):\n",
    "    \"Marks some ongoing test as failed, with an optional reason for failure.\"\n",
    "    raise TestFailed(reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    fail(\"asdf\")\n",
    "    assert False\n",
    "except TestFailed as err:\n",
    "    assert err.reason == \"asdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test result: success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Success(Result):\n",
    "    \"\"\"\n",
    "    Result for a test that passed.\n",
    "    \"\"\"\n",
    "    def is_success(self) -> bool:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert Success().is_success()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert Success().as_dict() == {\"type\": \"Success\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test result following the test code raising an exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traceback frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Frame:\n",
    "    \"\"\"\n",
    "    Information regarding a frame of a traceback. Provides more than the very limited\n",
    "    code context that comes from standard library introspection tools.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tb: Traceback, num_line: int, tags: Optional[List[str]] = None) -> None:\n",
    "        self.num_line = num_line\n",
    "        self.name_file = tb.filename\n",
    "        self.function = \"\" if tb.function == \"<module>\" else tb.function\n",
    "        self.tags = tags or []\n",
    "\n",
    "    def name_file_pretty(self, capitalized: bool = False) -> str:\n",
    "        m = re.match(r\"<ipython-input-(\\d+)-[a-f0-9A-F]+>\", self.name_file)\n",
    "        if m:\n",
    "            name = f\"code cell {m.group(1)}\"\n",
    "            return name.capitalize() if capitalized else name\n",
    "        return self.name_file\n",
    "\n",
    "    def context(self, before: int = 3, after: int = 3) -> List[Tuple[int, str]]:\n",
    "        ctx = [(self.num_line, getline(self.name_file, self.num_line).rstrip())]\n",
    "        for delta in range(1, before + 1):\n",
    "            ctx.insert(0, (self.num_line - delta, getline(self.name_file, self.num_line - delta).rstrip()))\n",
    "        for delta in range(1, after + 1):\n",
    "            ctx.append((self.num_line + delta, getline(self.name_file, self.num_line + delta).rstrip()))\n",
    "\n",
    "        # Clean up context: remove line-ending blanks and blank lines top and bottom\n",
    "        # of the context blob.\n",
    "        while len(ctx) > 0:\n",
    "            for i in [0, -1]:\n",
    "                if len(ctx[i][1]) == 0:\n",
    "                    del ctx[i]\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return ctx\n",
    "\n",
    "    def as_dict(self, context_before: int = 3, context_after: int = 3) -> Dict:\n",
    "        return {\n",
    "            \"file\": self.name_file_pretty(False),\n",
    "            \"line\": self.num_line,\n",
    "            \"function\": self.function,\n",
    "            \"context\": [[i, line] for i, line in self.context(context_before, context_after)],\n",
    "            \"tags\": self.tags\n",
    "        }\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{self.function + ': ' if self.function else ''}File {self.name_file}, Line {self.num_line}\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return str(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import getfile\n",
    "\n",
    "def my_function():  # noqa\n",
    "    getfile(\"asdf\")  # Will raise a TypeError\n",
    "\n",
    "frames = []  # noqa\n",
    "try:\n",
    "    my_function()\n",
    "    assert False\n",
    "except TypeError:\n",
    "    _, _, tb = sys.exc_info()\n",
    "    for frame_raw, lineno in walk_tb(tb):\n",
    "        frame = Frame(getframeinfo(frame_raw), lineno)\n",
    "        assert frame.name_file == getfile(frame_raw)\n",
    "        assert frame.num_line == lineno\n",
    "\n",
    "        if frame_raw.f_code.co_name == \"<module>\":\n",
    "            assert frame.function == \"\"\n",
    "        else:\n",
    "            assert frame.function == frame_raw.f_code.co_name\n",
    "\n",
    "        assert frame.tags == []\n",
    "        frames.append(frame)\n",
    "\n",
    "assert len(frames) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = frames[1]\n",
    "assert frame.context(0, 0) == [(4, \"    getfile(\\\"asdf\\\")  # Will raise a TypeError\")]\n",
    "assert frame.context(1, 1) == [(3, \"def my_function():  # noqa\"), (4, \"    getfile(\\\"asdf\\\")  # Will raise a TypeError\")]\n",
    "assert frame.context(3, 3) == [\n",
    "    (1, \"from inspect import getfile\"),\n",
    "    (2, \"\"),\n",
    "    (3, \"def my_function():  # noqa\"),\n",
    "    (4, \"    getfile(\\\"asdf\\\")  # Will raise a TypeError\"),\n",
    "    (5, \"\"),\n",
    "    (6, \"frames = []  # noqa\"),\n",
    "    (7, \"try:\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert frame.context(45, 0) == list(zip(range(1, 4 + 1), [\n",
    "    \"from inspect import getfile\",\n",
    "    \"\",\n",
    "    \"def my_function():  # noqa\",\n",
    "    \"    getfile(\\\"asdf\\\")  # Will raise a TypeError\"\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert frame.context(0, 9000) == list(zip(range(4, 9000), \"\"\"\\\n",
    "    getfile(\"asdf\")  # Will raise a TypeError\n",
    "\n",
    "frames = []  # noqa\n",
    "try:\n",
    "    my_function()\n",
    "    assert False\n",
    "except TypeError:\n",
    "    _, _, tb = sys.exc_info()\n",
    "    for frame_raw, lineno in walk_tb(tb):\n",
    "        frame = Frame(getframeinfo(frame_raw), lineno)\n",
    "        assert frame.name_file == getfile(frame_raw)\n",
    "        assert frame.num_line == lineno\n",
    "\n",
    "        if frame_raw.f_code.co_name == \"<module>\":\n",
    "            assert frame.function == \"\"\n",
    "        else:\n",
    "            assert frame.function == frame_raw.f_code.co_name\n",
    "\n",
    "        assert frame.tags == []\n",
    "        frames.append(frame)\n",
    "\n",
    "assert len(frames) == 3\\\n",
    "\"\"\".split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert frame.as_dict() == {\n",
    "    \"file\": \"code cell 12\",\n",
    "    \"line\": 4,\n",
    "    \"function\": \"my_function\",\n",
    "    \"tags\": [],\n",
    "    \"context\": list(list(e) for e in zip(range(1, 7 + 1), \"\"\"\\\n",
    "from inspect import getfile\n",
    "\n",
    "def my_function():  # noqa\n",
    "    getfile(\"asdf\")  # Will raise a TypeError\n",
    "\n",
    "frames = []  # noqa\n",
    "try:\\\n",
    "\"\"\".split(\"\\n\")))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The exception-driven result: errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Error(Result):\n",
    "    \"\"\"\n",
    "    Non-passing test result due to an exception being raised.\n",
    "\n",
    "    It is passed a set of common functions: the presence of these functions in the\n",
    "    traceback of the exception are expected and normal, making their eventual\n",
    "    reporting redundant and sort of trivial. The frames corresponding to these functions\n",
    "    in the traceback summary kept by this object will be tagged as such.\n",
    "    \"\"\"\n",
    "    TAG_COMMON = \"common\"\n",
    "\n",
    "    def __init__(self, fns_common: Iterable[Callable]) -> None:\n",
    "        super().__init__()\n",
    "        self._type_exc: type\n",
    "        self._value_exc: Any\n",
    "        self._type_exc, self._value_exc, tb = sys.exc_info()\n",
    "        if tb is None:\n",
    "            raise RuntimeError(\"Can only instantiate this class when an exception has been raised.\")\n",
    "\n",
    "        codes_common = {unwrap(fn).__code__ for fn in fns_common}\n",
    "        self._traceback: List[Frame] = []\n",
    "        for frame_raw, num_line in walk_tb(tb):\n",
    "            tags = []\n",
    "            if frame_raw.f_code in codes_common:\n",
    "                tags.append(Error.TAG_COMMON)\n",
    "            self._traceback.append(Frame(getframeinfo(frame_raw), num_line, tags))\n",
    "\n",
    "    def is_success(self) -> bool:\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def type_exc(self) -> type:\n",
    "        \"\"\"Returns the type of the exception associated to this result.\"\"\"\n",
    "        return self._type_exc\n",
    "\n",
    "    @property\n",
    "    def value_exc(self) -> Any:\n",
    "        \"\"\"Returns the exception raised in association to this test result.\"\"\"\n",
    "        return self._value_exc\n",
    "\n",
    "    @property\n",
    "    def traceback(self) -> List[Frame]:\n",
    "        \"\"\"\n",
    "        Returns a summary of the stack trace associated to the exception that brought this test result.\n",
    "        \"\"\"\n",
    "        return self._traceback\n",
    "\n",
    "    def as_dict(self, context_before: int = 3, context_after: int = 3) -> Dict:\n",
    "        d = super().as_dict()\n",
    "        d.update(\n",
    "            {\n",
    "                \"type_exc\": self.type_exc.__name__,\n",
    "                \"value_exc\": str(self.value_exc),\n",
    "                \"traceback\": [frame.as_dict(context_before, context_after) for frame in self.traceback]\n",
    "            }\n",
    "        )\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import getfile\n",
    "\n",
    "\n",
    "def fn_raise():\n",
    "    raise RuntimeError()\n",
    "\n",
    "\n",
    "def caller():\n",
    "    fn_raise()\n",
    "\n",
    "\n",
    "try:\n",
    "    caller()\n",
    "    assert False\n",
    "except RuntimeError:\n",
    "    err: Error = Error([caller])\n",
    "    assert not err.is_success()\n",
    "    assert not err.is_failure()\n",
    "    assert err.type_exc == RuntimeError\n",
    "    assert isinstance(err.value_exc, RuntimeError)\n",
    "    assert len(err.traceback) == 3\n",
    "    assert [frame.function for frame in err.traceback] == [\"\", \"caller\", \"fn_raise\"]\n",
    "    assert [frame.tags for frame in err.traceback] == [[], [Error.TAG_COMMON], []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    raise RuntimeError()\n",
    "    assert False\n",
    "except RuntimeError:\n",
    "    assert {\n",
    "        \"type\": \"Error\",\n",
    "        \"type_exc\": \"RuntimeError\",\n",
    "        \"value_exc\": \"\",\n",
    "        \"traceback\": [\n",
    "            {\n",
    "                \"file\": \"code cell 19\",\n",
    "                \"line\": 2,\n",
    "                \"function\": \"\",\n",
    "                \"tags\": [],\n",
    "                \"context\": [[2, \"    raise RuntimeError()\"]]\n",
    "            }\n",
    "        ]\n",
    "    } == Error([]).as_dict(0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliberate exception: failures\n",
    "\n",
    "For convenience's sake, we model `Failure`s as a subclass of `Error` to gain the exception breakdown functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "NameArg = Union[int, str]\n",
    "Arg = Tuple[NameArg, Any]\n",
    "Args = Sequence[Arg]\n",
    "\n",
    "\n",
    "def join_args(args: Sequence[Any], kwargs: Mapping[str, Any]) -> Args:\n",
    "    return list(enumerate(args)) + sorted(list(kwargs.items()), key=lambda arg: arg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Explanation:\n",
    "    \"\"\"\n",
    "    Embodies the reason why a predicate fails, including a main expectation that was not satisfied and\n",
    "    a list of assertion arguments that tripped the failure.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, main: str, culprits: Sequence[Union[Arg, Tuple[str]]]) -> None:\n",
    "        self.main = main\n",
    "        self.culprits = culprits\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{type(self).__name__}({repr(self.main)}, {repr(self.culprits)})\"\n",
    "\n",
    "    def __bool__(self) -> bool:\n",
    "        \"\"\"\n",
    "        All explanations are for failures, so when a predicate returns an explanation, it's as if it had\n",
    "        returned False.\n",
    "        \"\"\"\n",
    "        return False\n",
    "\n",
    "    def as_dict(self) -> Dict:\n",
    "        return {\n",
    "            \"main\": self.main,\n",
    "            \"culprits\": [[name, str(value)] for name, value in self.culprits]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class DetailedAssertionError(AssertionError):\n",
    "\n",
    "    def __init__(self, explanation: Explanation):\n",
    "        super().__init__(explanation.main)\n",
    "        self.explanation = explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Failure(Error):\n",
    "    \"\"\"\n",
    "    Test result stemming from a condition check that failed, or a test run marked\n",
    "    as a failure.\n",
    "    \"\"\"\n",
    "    def __init__(self, explanation: Explanation, fns_common: Iterable[Callable]):\n",
    "        super().__init__(fns_common)\n",
    "        self._explanation = explanation\n",
    "\n",
    "    @property\n",
    "    def explanation(self) -> Explanation:\n",
    "        \"Reason given by the programmer as to why the test failed.\"\n",
    "        return self._explanation\n",
    "\n",
    "    @property\n",
    "    def reason(self) -> str:\n",
    "        reason = self.explanation.main\n",
    "        if self.explanation.culprits:\n",
    "            reason += \": \" + \", \".join(str(value) for _, value in self.explanation.culprits)\n",
    "        return reason\n",
    "\n",
    "    def is_failure(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def as_dict(self, context_before: int = 3, context_after: int = 3) -> Dict:\n",
    "        d = super().as_dict(context_before, context_after)\n",
    "        d[\"explanation\"] = self.explanation.as_dict()\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert False\n",
    "except AssertionError:\n",
    "    err: Failure = Failure(Explanation(\"asdf\", [(\"a\", 1234), (\"c\", 987)]), [])\n",
    "    assert not err.is_success()\n",
    "    assert err.is_failure()\n",
    "    assert err.type_exc == AssertionError\n",
    "    assert isinstance(err.value_exc, AssertionError)\n",
    "    assert isinstance(err.traceback, list)\n",
    "    assert err.reason == \"asdf: 1234, 987\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert False\n",
    "except AssertionError:\n",
    "    assert {\n",
    "        \"type\": \"Failure\",\n",
    "        \"type_exc\": \"AssertionError\",\n",
    "        \"value_exc\": \"\",\n",
    "        \"traceback\": [\n",
    "            {\n",
    "                \"file\": \"code cell 25\",\n",
    "                \"line\": 2,\n",
    "                \"function\": \"\",\n",
    "                \"tags\": [],\n",
    "                \"context\": [[2, \"    assert False\"]]\n",
    "            }\n",
    "        ],\n",
    "        \"explanation\": {\n",
    "            \"main\": \"asdf\",\n",
    "            \"culprits\": [[0, \"1234\"], [1, \"3456\"], [\"qwer\", \"6767\"]]\n",
    "        }\n",
    "    } == Failure(Explanation(\"asdf\", [(0, 1234), (1, 3456), (\"qwer\", 6767)]), []).as_dict(0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom assertions to get detailed failure descriptions\n",
    "\n",
    "The tool `pytest` goes to great lengths to decompose the abstract syntax tree involved in an assertion in order to provide to the user a detailed description of, say, a failed equality test. While stealing this work would be ideal, the short-term solution rather is to provide custom assert functions that divide comparands into distinct function arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The main assertion verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "ExplanationOnFailure = Union[bool, str, Explanation]\n",
    "Predicate = Callable[..., ExplanationOnFailure]\n",
    "\n",
    "\n",
    "def assert_(predicate: Predicate, *ops: Any, msg: str = \"\", **kwops: Any) -> None:\n",
    "    \"\"\"\n",
    "    Evalutes a condition over multiple objects, using as condition test either an explicit function predicate\n",
    "    or a bound method provided as first (left-most) operand. Raises an AssertionError exception if the condition\n",
    "    is not met (the condition test yields a result of False).\n",
    "\n",
    "    cond\n",
    "        Predicate function (or bound method). It must take as input parameters. The function should return True\n",
    "        to signal satisfaction of the predicate. In the other case, it may return a False-equivalent value (in which\n",
    "        case the failure explanation is provided as the `msg` keyword argument), or a string corresponding to the\n",
    "        explanation, or an ExplanationFailure instance.\n",
    "    msg\n",
    "        Facultative explanation for the failure of satisfying the predicate. If provided, it will override the\n",
    "        explanation returned by the predicate, or at least its \"main expectation\" (see docstring of\n",
    "        `ExplanationFailure`).\n",
    "    ops, kwops\n",
    "        Operands to the predicate.\n",
    "    \"\"\"\n",
    "    result = predicate(*ops, **kwops)\n",
    "    if not result:\n",
    "        if isinstance(result, Explanation):\n",
    "            explanation = result\n",
    "        else:\n",
    "            explanation = Explanation(result or \"\", join_args(ops, kwops))\n",
    "\n",
    "        if msg:\n",
    "            explanation.main = msg\n",
    "        raise DetailedAssertionError(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assertions driven by simple functions.\n",
    "assert_((1).__eq__, 1)\n",
    "try:\n",
    "    assert_(operator.eq, 5, 7)\n",
    "    raise RuntimeError(\"The last assertion should have raised!\")\n",
    "except AssertionError as err:\n",
    "    assert isinstance(err, DetailedAssertionError)\n",
    "    assert not err.explanation.main\n",
    "    assert err.explanation.culprits == [(0, 5), (1, 7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assertion with explicit message and keyword arguments.\n",
    "\n",
    "\n",
    "def eq(v, x=0, y=0):\n",
    "    return v == x and x == y\n",
    "\n",
    "\n",
    "try:\n",
    "    assert_(eq, 0, x=0, y=1, msg=\"Ho\")\n",
    "except AssertionError as err:\n",
    "    assert isinstance(err, DetailedAssertionError)\n",
    "    assert err.explanation.main == \"Ho\"\n",
    "    assert str(err) == \"Ho\"\n",
    "    assert err.explanation.culprits == [(0, 0), (\"x\", 0), (\"y\", 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def same(*args, **kwargs) -> ExplanationOnFailure:\n",
    "    \"\"\"\n",
    "    Predicate satisfied when all its arguments are the same (according to operator `is`).\n",
    "    \"\"\"\n",
    "    return _assess_sequence_2more(\n",
    "        \"These objects are not the same as {name} => {value}\",\n",
    "        lambda p, r: None if p is r else hex(hash(p)),\n",
    "        args,\n",
    "        kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def _assess_sequence_2more(\n",
    "    expl_main: str,\n",
    "    pred_repr: Callable[[Any], Any],\n",
    "    args: Iterator[Any],\n",
    "    kwargs: Mapping[str, Any]\n",
    ") -> ExplanationOnFailure:\n",
    "    try:\n",
    "        (name_ref, value_ref), *rest = join_args(args, kwargs)\n",
    "        culprits = [(name_ref, pred_repr(value_ref, None))]\n",
    "        for name_comp, value_comp in rest:\n",
    "            r = pred_repr(value_comp, value_ref)\n",
    "            if r is not None:\n",
    "                culprits.append((name_comp, r))\n",
    "\n",
    "        if len(culprits) > 1:\n",
    "            return Explanation(expl_main, culprits)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Should provide at least one test argument to this function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "o1 = object()\n",
    "o2 = object()\n",
    "\n",
    "assert same(o1, o1)\n",
    "assert same(o2, o2, o2, a=o2)\n",
    "\n",
    "expl = same(o1, o2)\n",
    "assert not expl\n",
    "assert \"not the same\" in expl.main\n",
    "assert expl.culprits == [(0, hex(hash(o1))), (1, hex(hash(o2)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not providing enough arguments.\n",
    "o1 = object()\n",
    "\n",
    "try:\n",
    "    same()\n",
    "    assert False, \"Should have raised ValueError\"\n",
    "except ValueError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def eq(*args: Any, **kwargs: Any) -> ExplanationOnFailure:\n",
    "    \"\"\"\n",
    "    Exact equality predicate.\n",
    "    \"\"\"\n",
    "    return _assess_sequence_2more(\n",
    "        \"These objects are not equal\",\n",
    "        lambda p, r: None if p == r else p,\n",
    "        args,\n",
    "        kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert eq(1, 1, 1)\n",
    "\n",
    "expl = eq(0, 0, 0, 1)\n",
    "assert \"not equal\" in expl.main\n",
    "assert expl.culprits == [(0, 0), (3, 1)]\n",
    "\n",
    "assert not eq(1, 0)\n",
    "\n",
    "try:\n",
    "    eq()\n",
    "    assert False, \"Function should have raised\"\n",
    "except ValueError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def diff(*args: Any, **kwargs: Any) -> ExplanationOnFailure:\n",
    "    \"\"\"\n",
    "    Difference predicate.\n",
    "    \"\"\"\n",
    "    all_args = join_args(args, kwargs)\n",
    "    culprits_by_value: List[Tuple[arg, Set[str]]] = []\n",
    "    for i, (name_ref, value_ref) in enumerate(all_args[:-1]):\n",
    "        for name_comp, value_comp in all_args[i + 1:]:\n",
    "            if value_ref == value_comp:\n",
    "                try:\n",
    "                    names_equal = [s for v, s in culprits_by_value if v == value_ref][0]\n",
    "                except IndexError:\n",
    "                    culprits_by_value.append((value_ref, set()))\n",
    "                    _, names_equal = culprits_by_value[-1]\n",
    "                names_equal |= {str(n) for n in [name_ref, name_comp]}\n",
    "\n",
    "    if len(culprits_by_value) > 0:\n",
    "        return Explanation(\n",
    "            \"Some of the arguments were not mutually different\",\n",
    "            sorted([(\", \".join(sorted(list(names))), value) for value, names in culprits_by_value])\n",
    "        )\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference assertion.\n",
    "assert diff(0, 1)\n",
    "assert diff(1, 8, 5)\n",
    "assert diff(5, \"asdf\")\n",
    "assert diff(0, None)\n",
    "\n",
    "assert not diff(1, 1)\n",
    "\n",
    "expl = diff(1, 1, 5)\n",
    "assert \"not mutually different\" in expl.main\n",
    "assert expl.culprits == [(\"0, 1\", 1)]\n",
    "\n",
    "expl = diff(1, 5, 5)\n",
    "assert expl.culprits == [(\"1, 2\", 5)]\n",
    "\n",
    "expl = diff(1, 5, 8, 9, 9, n=9, w=5)\n",
    "assert expl.culprits == [(\"1, w\", 5), (\"3, 4, n\", 9)]\n",
    "\n",
    "assert diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def between(lower: Any, upper: Any, bounds: str = \"[]\") -> Predicate:\n",
    "    \"\"\"\n",
    "    Returns a predicate that is satisfied if all its arguments fall within an interval. Comparands\n",
    "    must all support order comparison operators (<, >, <=, >=).\n",
    "\n",
    "    lower\n",
    "        Lower bound of the interval.\n",
    "    upper\n",
    "        Upper bound of the interval.\n",
    "    bounds\n",
    "        Determines whether the interval is closed or open at either end (e.g. the boundary element is belongs or not\n",
    "        to the set formed by the interval). It is provided as a string whose first and last characters are observed.\n",
    "            First character:\n",
    "                [    Interval is closed on the left.\n",
    "                (    Interval is open on the left.\n",
    "            Last character:\n",
    "                ]    Interval is closed on the right.\n",
    "                )    Interval is open on the right.\n",
    "        Any other character in these positions raise an error.\n",
    "    \"\"\"\n",
    "    if lower > upper:\n",
    "        raise ValueError(\"Lower bound should be lesser than the upper bound.\")\n",
    "    cmp_lower, cmp_upper = _get_comparators(bounds)\n",
    "    return _assessor_sequence(\n",
    "        f\"These objects did not belong to interval {bounds[0]}{lower}, {upper}{bounds[1]}\",\n",
    "        lambda v: cmp_lower(v, lower) and cmp_upper(v, upper),\n",
    "    )\n",
    "\n",
    "\n",
    "Comparator = Callable[[Any, Any], bool]\n",
    "\n",
    "\n",
    "def _get_comparators(bounds: str) -> Tuple[Comparator, Comparator]:\n",
    "    if not re.match(r\"^(\\[|\\().*(\\]|\\))$\", bounds):\n",
    "        raise ValueError(\"Invalid open/close bounds descriptor\")\n",
    "    return (\n",
    "        {\"[\": operator.ge, \"(\": operator.gt}[bounds[0]],\n",
    "        {\"]\": operator.le, \")\": operator.lt}[bounds[-1]]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def _assessor_sequence(expl_main: str, pred: Callable[[Any], bool]) -> Predicate:\n",
    "    def _assess_sequence(*args, **kwargs) -> ExplanationOnFailure:\n",
    "        culprits = [(name, value) for name, value in join_args(args, kwargs) if not pred(value)]\n",
    "        if len(culprits) > 0:\n",
    "            return Explanation(expl_main, culprits)\n",
    "        return True\n",
    "    return _assess_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert between(1, 2)(1.38, 1 + 1e-12, 2 - 1e-12)\n",
    "\n",
    "expl = between(1, 2)(1 - 1e-12, 2 + 1e-12)\n",
    "assert \"belong to interval\" in expl.main\n",
    "assert [n for n, _ in expl.culprits] == [0, 1]\n",
    "\n",
    "assert between(1, 2, bounds=\"[asdf]\")(1, a=2)\n",
    "assert between(1, 2, bounds=\"[)\")(1, 2 - 1e-15)\n",
    "assert not between(1, 2, bounds=\"(]\")(1)\n",
    "assert between(1, 2, bounds=\"(]\")(1 + 1e-15, 2)\n",
    "assert not between(1, 2, bounds=\"[])\")(2)\n",
    "assert not between(1, 2, bounds=\"(())\")(1, 2)\n",
    "assert between(1, 2, bounds=\"()\")(1 + 1e-15, 2 - 1e-15)\n",
    "\n",
    "assert between(1, 1)(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# between: lower bound must be smaller or equal than upper bound\n",
    "try:\n",
    "    between(3, 2)(2.5)\n",
    "    assert False, \"Should have raised\"\n",
    "except ValueError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# between: many ways to bork the bound closedness descriptor\n",
    "for b in [\"\", \"[\", \"][\", \")\", \")(\"]:\n",
    "    try:\n",
    "        between(1, 2, bounds=b)\n",
    "        raise RuntimeError(\"Last call should have raised\")\n",
    "    except ValueError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def approx(centre: Any, tol: Any = 1e-6, bounds: str = \"[]\") -> Predicate:\n",
    "    \"\"\"\n",
    "    Returns a predicate satisfied when all arguments land within an interval defined by a centre and a tolerance\n",
    "    (which corresponds to its half-width). This can be used with any number-like objects that support addition and\n",
    "    some form of subtraction (either direct or by addition with opposite).\n",
    "\n",
    "    centre\n",
    "        Reference number against which we test for near-equality.\n",
    "    tol\n",
    "        Acceptable difference between arguments and the centre for the predicate to be satisfied.\n",
    "    bounds\n",
    "        Exclusivity of each implicit interval boundaries; see the docstring for `between` for further details.\n",
    "    \"\"\"\n",
    "    upper = centre + tol\n",
    "    try:\n",
    "        lower = centre - tol\n",
    "    except TypeError:\n",
    "        try:\n",
    "            lower = centre + (-tol)\n",
    "        except TypeError:\n",
    "            raise TypeError(\"Operands do not support any form of subtraction\")\n",
    "\n",
    "    return between(lower, upper, bounds=bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert approx(1, tol=1e-3)(1 + 9e-4, 1 - 9.99e-4)\n",
    "assert approx(1, tol=1e-6)(1 - 1e-6, 1.000001)\n",
    "assert not approx(1, tol=1e-6, bounds=\"()\")(1 - 1e-6, 1.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def belong_to(collection: Container) -> Predicate:\n",
    "    \"\"\"\n",
    "    Predicate satisfied when all arguments are contained by the given collection, according to the `in'\n",
    "    operator.\n",
    "    \"\"\"\n",
    "    return _assessor_sequence(\"These arguments did not belong to the collection\", lambda v: v in collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert belong_to([1, 2, 3])(3, q=2)\n",
    "assert not belong_to([1, 3])(2)\n",
    "assert belong_to([1, 2, 3])()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def is_any_of(*types: type) -> Predicate:\n",
    "    \"\"\"\n",
    "    Predicate satisfied when all arguments are instances of one of the given types.\n",
    "    \"\"\"\n",
    "    if len(types) < 1:\n",
    "        raise ValueError(\"Must provide at least one type.\")\n",
    "    return _assessor_sequence(\n",
    "        \"These arguments were not of type\" + \" or \".join(T.__name__ for T in types),\n",
    "        lambda v: any(isinstance(v, T) for T in types)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert is_any_of(int)(a=6, b=-92)\n",
    "assert is_any_of(float)(9.2)\n",
    "assert not is_any_of(float)(9.2, 3)\n",
    "assert is_any_of(int, float)(9.2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test suites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Subscriber(ABC):\n",
    "    \"\"\"\n",
    "    Object reacting to test results as they are generated by running tests.\n",
    "    \"\"\"\n",
    "\n",
    "    def on_add_to_suite(self, suite: \"Suite\") -> None:\n",
    "        pass\n",
    "\n",
    "    def on_result(self, name_test: str, result: Result) -> None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "TestFunction = Callable[..., None]\n",
    "\n",
    "\n",
    "class Suite:\n",
    "    \"\"\"\n",
    "    Suite of tests, gathering the result of multiple named test runs. Test code fragments\n",
    "    are named using the `test()` decorator, or leveraging it indirectly by registering\n",
    "    a shortcut cell magic.\n",
    "\n",
    "    Test suites gets added functionality through a publish/subscribe system. Subscriber are\n",
    "    special objects tied to the suite instance through its `|' (bit OR) operator. At the\n",
    "    moment, the only event broadcast to all subscribers is the generation of a new test\n",
    "    result (and its appending to the suite's log). For instance, the `Report` plug-in\n",
    "    supports the suite by giving immediate feedback on a test's results. Thus, to\n",
    "    instantiate a suite with this added feature, one would use code like\n",
    "\n",
    "    suite = Suite() | Report()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name_magic: str = \"test\") -> None:\n",
    "        self._tests: Dict[str, List[Result]] = {}\n",
    "        self._fns_common = [fail, self.test]\n",
    "        self._subscribers: List[Subscriber] = []\n",
    "\n",
    "    def test(\n",
    "        self,\n",
    "        fn: Optional[TestFunction] = None,\n",
    "        name: str = \"\",\n",
    "        args: Sequence[Any] = [],\n",
    "        kwargs: Mapping[str, Any] = {}\n",
    "    ) -> Union[Callable[[TestFunction], TestFunction], TestFunction]:\n",
    "        \"\"\"\n",
    "        Runs a test encoded into a function. Completing the function's execution counts as a\n",
    "        test success; tripped assertions and other exceptions generate some other Result;\n",
    "        and the test result is retained by this Suite instance.\n",
    "\n",
    "        This decorator can be used two ways. Without application, one can decorate a\n",
    "        test function without parameter:\n",
    "\n",
    "            @suite.test\n",
    "            def this_is_my_test():\n",
    "                # Test goes here!\n",
    "\n",
    "        The name of the test corresponds to that of the function. Applying the decorator can\n",
    "        supply arguments to the test function and override the name of the test.\n",
    "\n",
    "            @suite.test(name=\"My test, with spaces\", args=(3, 4))\n",
    "            def fn_test(a, b):\n",
    "                # Test goes here!\n",
    "\n",
    "        To run a test with multiple parameter sets, one may even call this function directly,\n",
    "        not as a decorator:\n",
    "\n",
    "            def fn_test(a, b):\n",
    "                # Test test test...\n",
    "\n",
    "            for a, b in [(2, 8), (3, 4)]:\n",
    "                suite.test(fn_test, name=f\"Test with {a}, {b}\", args=(a, b))\n",
    "\n",
    "        fn\n",
    "            Function that embodies the test code.\n",
    "        name\n",
    "            Name of the test; by default, the name of the function is used.\n",
    "        args\n",
    "            Positional arguments to pass to the function to run the test.\n",
    "        kwargs\n",
    "            Named arguments to pass to the function to run the test.\n",
    "        \"\"\"\n",
    "        if fn is None:\n",
    "            return lambda fn: self.test(fn, name=name, args=args, kwargs=kwargs)\n",
    "\n",
    "        try:\n",
    "            fn(*args, **kwargs)\n",
    "            result = Success()\n",
    "        except TestFailed as err:\n",
    "            result = Failure(err.reason, self._fns_common)\n",
    "        except AssertionError as err:\n",
    "            result = Failure(str(err), self._fns_common)\n",
    "        except BaseException:\n",
    "            result = Error(self._fns_common)\n",
    "\n",
    "        name_test = name\n",
    "        if not name_test:\n",
    "            name_test = fn.__name__\n",
    "            if args or kwargs:\n",
    "                str_args = \", \".join(\n",
    "                    [repr(str(a)) for a in args] +\n",
    "                    [f\"{k}={repr(str(v))}\" for k, v in kwargs.items()]\n",
    "                )\n",
    "                name_test += f\"({str_args})\"\n",
    "        self._tests.setdefault(name_test, []).append(result)\n",
    "        for subscriber in self._subscribers:\n",
    "            subscriber.on_result(name_test, result)\n",
    "\n",
    "        return fn\n",
    "\n",
    "    @property\n",
    "    def results(self) -> Iterator[Tuple[str, Iterator[Result]]]:\n",
    "        \"\"\"\n",
    "        Iterates through the gathered test results. For each named test, yields a tuple of\n",
    "        the name of the test and an iterator over each result gathered as the test has\n",
    "        been run.\n",
    "        \"\"\"\n",
    "        for name, test_results in self._tests.items():\n",
    "            yield name, iter(test_results)\n",
    "\n",
    "    def as_dict(self) -> Dict[str, List[Dict]]:\n",
    "        \"Provides a structured data representation suitable for data serialization and exportation.\"\n",
    "        return {name: [r.as_dict() for r in rez] for name, rez in self.results}\n",
    "\n",
    "    def __or__(self, subscriber: Subscriber) -> \"Suite\":\n",
    "        \"\"\"\n",
    "        Generates a clone of this suite instance, but with this subscriber subscribed to it.\n",
    "\n",
    "        The new suite will not share member data structures with `self`, but if `self` carries\n",
    "        test results already, the new suite will reference the same result objects -- we\n",
    "        assume that Result objects are immutable.\n",
    "        \"\"\"\n",
    "        suite_with_subscriber = Suite()\n",
    "        suite_with_subscriber._tests = copy(self._tests)  # Under assumption of results immutability.\n",
    "        suite_with_subscriber._subscribers = copy(self._subscribers)\n",
    "        suite_with_subscriber._subscribers.append(subscriber)\n",
    "        subscriber.on_add_to_suite(suite_with_subscriber)\n",
    "        return suite_with_subscriber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(Suite()._tests, dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = Suite()\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def succeeding():\n",
    "    assert True\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def failing_by_assert_terse():\n",
    "    assert False\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def failing_by_assert_reason():\n",
    "    assert False, \"assert reason\"\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def failing_manually_terse():\n",
    "    fail()\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def failing_manually_reason():\n",
    "    fail(\"fail reason\")\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def error():\n",
    "    raise RuntimeError(\"doh\")\n",
    "\n",
    "\n",
    "assert [\n",
    "    (\"succeeding\", [(Success, \"\")]),\n",
    "    (\"failing_by_assert_terse\", [(Failure, \"\")]),\n",
    "    (\"failing_by_assert_reason\", [(Failure, \"assert reason\")]),\n",
    "    (\"failing_manually_terse\", [(Failure, \"\")]),\n",
    "    (\"failing_manually_reason\", [(Failure, \"fail reason\")]),\n",
    "    (\"error\", [(Error, \"\")])\n",
    "] == [(name, [(type(r), r.reason if hasattr(r, \"reason\") else \"\") for r in rez]) for name, rez in suite.results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check each non-success for tagging of the first frame.\n",
    "num_non_success = 0\n",
    "for name, rez in suite.results:\n",
    "    for r in rez:\n",
    "        if not r.is_success():\n",
    "            num_non_success += 1\n",
    "            assert Error.TAG_COMMON in r.traceback[0].tags\n",
    "\n",
    "assert num_non_success == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check failures for tagging of the fail call.\n",
    "num_failures = 0\n",
    "for name, rez in suite.results:\n",
    "    if \"failing_manually\" in name:\n",
    "        for r in rez:\n",
    "            num_failures += 1\n",
    "            assert Error.TAG_COMMON in r.traceback[-1].tags\n",
    "\n",
    "assert num_failures == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = Suite()\n",
    "\n",
    "\n",
    "@suite.test(name=\"trial\")\n",
    "def t1():\n",
    "    fail()\n",
    "\n",
    "\n",
    "@suite.test(name=\"trial\")\n",
    "def t2():\n",
    "    raise RuntimeError()\n",
    "\n",
    "\n",
    "@suite.test(name=\"trial\")\n",
    "def t3():\n",
    "    pass  # Literally!\n",
    "\n",
    "\n",
    "assert [(\"trial\", [Failure, Error, Success])] == [(name, [type(r) for r in rez]) for name, rez in suite.results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = Suite()\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def first():\n",
    "    fail()\n",
    "\n",
    "\n",
    "@suite.test(name=\"first\")\n",
    "def first2():\n",
    "    pass\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def second():\n",
    "    raise RuntimeError()\n",
    "\n",
    "\n",
    "assert {name: [r[\"type\"] for r in rez] for name, rez in suite.as_dict().items()} == {\n",
    "    \"first\": [\"Failure\", \"Success\"],\n",
    "    \"second\": [\"Error\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing environment protection during test execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"x\" not in globals()\n",
    "mylist = [1, 2, 3]\n",
    "\n",
    "\n",
    "suite = Suite()\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def trial():\n",
    "    x = 5\n",
    "    assert x == 5\n",
    "    mylist.append(4)\n",
    "\n",
    "\n",
    "assert \"x\" not in globals()\n",
    "assert [(\"trial\", [Success])] == [(name, [type(r) for r in rez]) for name, rez in suite.results]\n",
    "# Accidental globals are obviously not protected.\n",
    "assert [1, 2, 3, 4] == mylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ns in [globals(), locals()]:\n",
    "    assert \"C\" not in ns\n",
    "\n",
    "\n",
    "suite = Suite()\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def check_something_about_a_new_class():\n",
    "    class C:\n",
    "        def f(self):\n",
    "            return 5\n",
    "\n",
    "    assert C().f() == 5\n",
    "\n",
    "\n",
    "for ns in [globals(), locals()]:\n",
    "    assert \"C\" not in ns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing publish/subscribe of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestSubscriber(Subscriber):\n",
    "\n",
    "    def __init__(self, lr: List[Tuple[str, Result]]):\n",
    "        self.suite = None\n",
    "        self._results: List[Tuple[str, Result]] = lr\n",
    "\n",
    "    def on_add_to_suite(self, suite: Suite) -> None:\n",
    "        self.suite = suite\n",
    "\n",
    "    def on_result(self, name_test: str, result: Result) -> None:\n",
    "        self._results.append((name_test, result))\n",
    "\n",
    "\n",
    "results: List[Tuple[str, Result]] = []\n",
    "sub = TestSubscriber(results)\n",
    "assert sub.suite is None\n",
    "suite = Suite() | sub\n",
    "assert sub.suite is suite\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def passing():\n",
    "    pass\n",
    "\n",
    "\n",
    "assert len(results) == 1\n",
    "name_last, result_last = results[-1]\n",
    "assert name_last == \"passing\"\n",
    "assert isinstance(result_last, Success)\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def error():\n",
    "    raise RuntimeError(\"asdf\")\n",
    "\n",
    "\n",
    "assert len(results) == 2\n",
    "name_last, result_last = results[-1]\n",
    "assert name_last == \"error\"\n",
    "assert isinstance(result_last, Error)\n",
    "assert str(result_last.value_exc) == \"asdf\"\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def failing():\n",
    "    fail(\"There is no why\")\n",
    "\n",
    "\n",
    "assert len(results) == 3\n",
    "name_last, result_last = results[-1]\n",
    "assert name_last == \"failing\"\n",
    "assert isinstance(result_last, Failure)\n",
    "assert result_last.reason == \"There is no why\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating test run reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report colorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Emphasis(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, s: str) -> str:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Color(Emphasis):\n",
    "\n",
    "    def __init__(self, fg=None, bg=None, style=None) -> None:\n",
    "        super().__init__()\n",
    "        self._fg = fg\n",
    "        self._bg = bg\n",
    "        self._style = style\n",
    "\n",
    "    def __call__(self, s: str) -> str:\n",
    "        return colors.color(s, fg=self._fg, bg=self._bg, style=self._style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert Color(fg=\"red\", bg=\"blue\", style=\"bold\")(\"asdf\") == '\\x1b[31;44;1masdf\\x1b[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Plain(Emphasis):\n",
    "\n",
    "    def __call__(self, s: str) -> str:\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert Plain()(\"asdf\") == \"asdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Colorizer:\n",
    "\n",
    "    def __init__(self, important: Emphasis, trivial: Emphasis, failure: Emphasis, error: Emphasis) -> None:\n",
    "        self.important: Emphasis = important\n",
    "        self.trivial: Emphasis = trivial\n",
    "        self.failure: Emphasis = failure\n",
    "        self.error: Emphasis = error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def plain() -> Colorizer:\n",
    "    return Colorizer(Plain(), Plain(), Plain(), Plain())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = plain()\n",
    "assert c.important(\"asdf\") == \"asdf\"\n",
    "assert c.trivial(\"asdf\") == \"asdf\"\n",
    "assert c.failure(\"asdf\") == \"asdf\"\n",
    "assert c.error(\"asdf\") == \"asdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def color(  # noqa\n",
    "    important: Optional[Emphasis] = None,\n",
    "    trivial: Optional[Emphasis] = None,\n",
    "    failure: Optional[Emphasis] = None,\n",
    "    error: Optional[Emphasis] = None\n",
    ") -> Colorizer:\n",
    "    return Colorizer(\n",
    "        important or Color(style=\"bold\"),\n",
    "        trivial or Color(fg=\"white\"),\n",
    "        failure or Color(fg=\"yellow\"),\n",
    "        error or Color(fg=\"red\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = color(important=Color(fg=\"white\"), trivial=Color(fg=\"black\"), failure=Color(bg=\"blue\"), error=Color(fg=\"green\"))\n",
    "assert c.important(\"asdf\") == Color(fg=\"white\")(\"asdf\")\n",
    "assert c.trivial(\"asdf\") == Color(fg=\"black\")(\"asdf\")\n",
    "assert c.failure(\"asdf\") == Color(bg=\"blue\")(\"asdf\")\n",
    "assert c.error(\"asdf\") == Color(fg=\"green\")(\"asdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raising an exception when reporting failures and errors\n",
    "\n",
    "This can be useful when running all tests after a code modification, especially if running from within a CI system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class ProblemsEncountered(Exception):\n",
    "    \"\"\"Raised (optionally) when a reporting routine must report failures and errors.\"\"\"\n",
    "\n",
    "    def __init__(self, num_failures, num_errors):\n",
    "        plural_failure = \"s\" if num_failures > 1 else \"\"\n",
    "        plural_errors = \"s\" if num_errors > 1 else \"\"\n",
    "        super().__init__(\n",
    "            f\"Problems encountered during testing: {num_failures} failure{plural_failure}, \"\n",
    "            f\"{num_errors} error{plural_errors}\"\n",
    "        )\n",
    "        self.num_failures = num_failures\n",
    "        self.num_errors = num_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "try:\n",
    "    raise ProblemsEncountered(2, 2)\n",
    "except ProblemsEncountered as err:\n",
    "    assert re.search(\"2 failures, 2 errors\", str(err))\n",
    "\n",
    "try:\n",
    "    raise ProblemsEncountered(1, 0)\n",
    "except ProblemsEncountered as err:\n",
    "    assert re.search(\"1 failure, 0 error\", str(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def raise_on_error(suite: Suite) -> None:\n",
    "    num_failures = 0\n",
    "    num_errors = 0\n",
    "    for _, rez in suite.results:\n",
    "        for r in rez:\n",
    "            if not r.is_success():\n",
    "                if r.is_failure():\n",
    "                    num_failures += 1\n",
    "                else:\n",
    "                    num_errors += 1\n",
    "    if num_failures > 0 or num_errors > 0:\n",
    "        raise ProblemsEncountered(num_failures, num_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_suite_with_two_tests():\n",
    "    suite = Suite()\n",
    "    for name in [\"one\", \"two\"]:\n",
    "        @suite.test(name=f\"passing-{name}\", args=(name,))\n",
    "        def passing(a):\n",
    "            pass\n",
    "\n",
    "    return suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = make_suite_with_two_tests()\n",
    "\n",
    "try:\n",
    "    raise_on_error(suite)\n",
    "except ProblemsEncountered:\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = make_suite_with_two_tests()\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def failure():\n",
    "    fail()\n",
    "\n",
    "\n",
    "try:\n",
    "    raise_on_error(suite)\n",
    "    assert False\n",
    "except ProblemsEncountered as err:\n",
    "    assert err.num_failures == 1\n",
    "    assert err.num_errors == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = make_suite_with_two_tests()\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def error():\n",
    "    raise RuntimeError()\n",
    "\n",
    "\n",
    "try:\n",
    "    raise_on_error(suite)\n",
    "    assert False\n",
    "except ProblemsEncountered as err:\n",
    "    assert err.num_failures == 0\n",
    "    assert err.num_errors == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exhaustive report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "TestNameFormatter = Callable[[str, int], str]\n",
    "\n",
    "\n",
    "def name_all(name_test: str, num_result: int) -> str:\n",
    "    \"\"\"\n",
    "    Test name formatter that puts out the name of a test even when it has\n",
    "    been run multiple times.\n",
    "    \"\"\"\n",
    "    return name_test\n",
    "\n",
    "\n",
    "def ladder(name_test: str, num_result: int) -> str:\n",
    "    \"\"\"\n",
    "    Test name formatter that puts out the name of a test only once, even\n",
    "    if it has been run multiple times.\n",
    "    \"\"\"\n",
    "    if num_result == 0:\n",
    "        return name_test\n",
    "    return \" \" * len(name_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def quoter(formatter: TestNameFormatter) -> TestNameFormatter:\n",
    "    \"\"\"\n",
    "    Test name formatter that surrounds the name between double quotes. Not\n",
    "    meant to be used directly by users of this module.\n",
    "    \"\"\"\n",
    "    def quoter_format(name_test: str, num_result: int) -> str:\n",
    "        return f\"\\\"{formatter(name_test, num_result)}\\\"\"\n",
    "\n",
    "    return quoter_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class PolicyReportingProblems:\n",
    "    \"\"\"What to do when reporting test results that involve problems (failures and errors).\"\"\"\n",
    "    def __init__(self, label: str) -> None:\n",
    "        self.label = label\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.label\n",
    "\n",
    "    def __eq__(self, other: Any) -> bool:\n",
    "        if not isinstance(other, PolicyReportingProblems):\n",
    "            return False\n",
    "        return self.label == other.label\n",
    "\n",
    "\n",
    "IGNORE = PolicyReportingProblems(\"ignore\")\n",
    "RAISE = PolicyReportingProblems(\"raise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def report_results(  # noqa\n",
    "    suite: Suite,\n",
    "    file: TextIOBase = sys.stdout,\n",
    "    colorizer: Colorizer = color(),\n",
    "    format_name_test: TestNameFormatter = ladder,\n",
    "    sep_name_result: str = \"\\t\",\n",
    "    quote_names: bool = False,\n",
    "    labels_result_custom: Mapping[type, str] = {},\n",
    "    on_error: PolicyReportingProblems = IGNORE\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Reports the name and result for each attempt at running a test, without details\n",
    "    as to issues encountered (failures and errors).\n",
    "\n",
    "    suite\n",
    "        Suite of test to write report from.\n",
    "    file\n",
    "        File-like object to write report to. Default is standard output.\n",
    "    colorizer\n",
    "        Policy for emphasizing the written report.\n",
    "    format_name_test\n",
    "        Some tests are run more than once (for instance, for iterative problem solving).\n",
    "        In a report written for human reading, the repeated naming of a test run more\n",
    "        than once can feel redundant; it is eliminated by setting this to `ladder`.\n",
    "        All tests will be named if the formatter used instead is `name_all`.\n",
    "    sep_name_result\n",
    "        Separating character used between test name and result label. Default is \"\\t\".\n",
    "    quote_names\n",
    "        If True, the test names will be surrounded with double quotes in the output.\n",
    "    labels_result_custom\n",
    "        Dictionary of labels to use with different result types, when the default\n",
    "        labels (*ok* for success, *failed* for failure, *ERROR* for error) should be\n",
    "        changed. The emphasis for each label is derived from the colorizer.\n",
    "    on_error\n",
    "        What to do when reporting results that include problems such as failures and\n",
    "        errors. If set to RAISE, it will raise a ProblemsEncountered exception, which\n",
    "        is useful when running the notebook as part of a CI/CD pipeline; otherwise,\n",
    "        or if set to IGNORE, nothing more is done than writing the report.\n",
    "        Default is IGNORE.\n",
    "    \"\"\"\n",
    "    len_all_names = [len(name) for name, _ in suite.results]\n",
    "    if len(len_all_names) == 0:\n",
    "        return\n",
    "    len_name_largest = max(len_all_names)\n",
    "\n",
    "    labels_result = {\n",
    "        type_result: colorize(labels_result_custom.get(type_result, label_default))\n",
    "        for type_result, colorize, label_default in [\n",
    "            (Success, Plain(), \"ok\"),\n",
    "            (Failure, colorizer.failure, \"failed\"),\n",
    "            (Error, colorizer.error, \"ERROR\")\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    if quote_names:\n",
    "        format_name_test = quoter(format_name_test)\n",
    "\n",
    "    for name, rez in suite.results:\n",
    "        p_name = f\"{name:{len_name_largest}s}\"\n",
    "        for num, r in enumerate(rez):\n",
    "            print(format_name_test(p_name, num), labels_result[type(r)], sep=sep_name_result, file=file)\n",
    "\n",
    "    if on_error is RAISE:\n",
    "        raise_on_error(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = Suite()\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def first():\n",
    "    raise RuntimeError()\n",
    "\n",
    "\n",
    "@suite.test(name=\"first\")\n",
    "def first2():\n",
    "    pass\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def second():\n",
    "    fail()\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def third():\n",
    "    pass\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def fourth():\n",
    "    assert False\n",
    "\n",
    "\n",
    "@suite.test(name=\"fourth\")\n",
    "def fourth2():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: tests `first` and `fourth` are run twice; `second` and `third` only once. The name of each test is written only once. The results are either `ok`, `failed` or `ERROR` and are emphasized as normal for success, yellow for failure and red for error. The results are also lined up cleanly into a second column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first \t\u001b[31mERROR\u001b[0m\n",
      "      \tok\n",
      "second\t\u001b[33mfailed\u001b[0m\n",
      "third \tok\n",
      "fourth\t\u001b[33mfailed\u001b[0m\n",
      "      \tok\n"
     ]
    }
   ],
   "source": [
    "report_results(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as above, but with a `RAISE` policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first \t\u001b[31mERROR\u001b[0m\n",
      "      \tok\n",
      "second\t\u001b[33mfailed\u001b[0m\n",
      "third \tok\n",
      "fourth\t\u001b[33mfailed\u001b[0m\n",
      "      \tok\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    report_results(suite, on_error=RAISE)\n",
    "    assert False\n",
    "except ProblemsEncountered as err:\n",
    "    assert err.num_failures == 2\n",
    "    assert err.num_errors == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: like previous, except that the test names are written on each line a result is reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first \t\u001b[31mERROR\u001b[0m\n",
      "first \tok\n",
      "second\t\u001b[33mfailed\u001b[0m\n",
      "third \tok\n",
      "fourth\t\u001b[33mfailed\u001b[0m\n",
      "fourth\tok\n"
     ]
    }
   ],
   "source": [
    "report_results(suite, format_name_test=name_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: like previous, but CSV-like, with test names quoted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"first \",ERROR\n",
      "\"first \",ok\n",
      "\"second\",failed\n",
      "\"third \",ok\n",
      "\"fourth\",failed\n",
      "\"fourth\",ok\n"
     ]
    }
   ],
   "source": [
    "report_results(suite, format_name_test=name_all, quote_names=True, sep_name_result=\",\", colorizer=plain())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite_all_passed = Suite()\n",
    "\n",
    "\n",
    "@suite_all_passed.test\n",
    "def first():\n",
    "    pass\n",
    "\n",
    "\n",
    "@suite_all_passed.test\n",
    "def second():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: two results, both `ok`. No exception raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first \tok\n",
      "second\tok\n"
     ]
    }
   ],
   "source": [
    "report_results(suite_all_passed, on_error=RAISE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite_empty = Suite()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: nothing written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_results(suite_empty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test result summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def summarize_results(  # noqa\n",
    "    suite: Suite,\n",
    "    file: Optional[TextIOBase] = sys.stdout,\n",
    "    colorizer: Colorizer = color(),\n",
    "    sep: str = \", \",\n",
    "    on_error: PolicyReportingProblems = IGNORE\n",
    ") -> Dict[type, int]:\n",
    "    \"\"\"\n",
    "    Writes a very short summary of a test run, counting the number of each result obtained.\n",
    "\n",
    "    suite\n",
    "        Suite of test to write report from.\n",
    "    file\n",
    "        File-like object to write report to. Default is standard output.\n",
    "    colorizer\n",
    "        Policy for emphasizing the written report.\n",
    "    sep\n",
    "        Separation string between the labeled numbers of results. Default is \", \"\n",
    "    on_error\n",
    "        What to do when reporting results that include problems such as failures and\n",
    "        errors. If set to RAISE, it will raise a ProblemsEncountered exception, which\n",
    "        is useful when running the notebook as part of a CI/CD pipeline; otherwise,\n",
    "        or if set to IGNORE, nothing more is done than writing the report.\n",
    "        Default is IGNORE.\n",
    "    \"\"\"\n",
    "    summary = {t: 0 for t in [Success, Failure, Error]}\n",
    "    for _, rez in suite.results:\n",
    "        for r in rez:\n",
    "            summary[type(r)] += 1\n",
    "\n",
    "    if file is not None:\n",
    "        print(\n",
    "            f\"{summary[Success]} passed\",\n",
    "            (colorizer.failure if summary[Failure] > 0 else colorizer.trivial)(f\"{summary[Failure]} failed\"),\n",
    "            (colorizer.error if summary[Error] > 0 else colorizer.trivial)(f\"{summary[Error]} raised an error\"),\n",
    "            file=file,\n",
    "            sep=sep\n",
    "        )\n",
    "\n",
    "    if on_error == RAISE:\n",
    "        raise_on_error(suite)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: should show 3 tests passed, 2 failures (in yellow), 1 error (in red)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 passed, \u001b[33m2 failed\u001b[0m, \u001b[31m1 raised an error\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{__main__.Success: 3, __main__.Failure: 2, __main__.Error: 1}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_results(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same, but raising an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 passed, \u001b[33m2 failed\u001b[0m, \u001b[31m1 raised an error\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    summarize_results(suite, on_error=RAISE)\n",
    "    assert False\n",
    "except ProblemsEncountered as err:\n",
    "    assert err.num_failures == 2\n",
    "    assert err.num_errors == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check visually that nothing is written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert summarize_results(suite, file=None) == {\n",
    "    Success: 3,\n",
    "    Failure: 2,\n",
    "    Error: 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite_all_passed = Suite()\n",
    "\n",
    "\n",
    "@suite_all_passed.test\n",
    "def first():\n",
    "    pass\n",
    "\n",
    "\n",
    "@suite_all_passed.test\n",
    "def second():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check visually that report is fine. Should show 2 tests passed, and failed and errors labeled in a subdued color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 passed, \u001b[37m0 failed\u001b[0m, \u001b[37m0 raised an error\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{__main__.Success: 2, __main__.Failure: 0, __main__.Error: 0}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_results(suite_all_passed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check visually that report is fine. Should show 0 for each type of test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 passed, \u001b[37m0 failed\u001b[0m, \u001b[37m0 raised an error\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{__main__.Success: 0, __main__.Failure: 0, __main__.Error: 0}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_results(suite_empty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed report of issues encountered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing one frame for a result's associated traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_frame(frame: Frame) -> Sequence[str]:\n",
    "    return ([f\"Function {frame.function}\"] if frame.function else []) + [frame.name_file_pretty(True), f\"Line {frame.num_line}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def print_frame(  # noqa\n",
    "    frame: Frame,\n",
    "    file: TextIOBase = sys.stdout,\n",
    "    colorizer: Colorizer = color(),\n",
    "    lines_context: int = 3\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Writes up a single stack frame report.\n",
    "\n",
    "    frame\n",
    "        Stack frame to report on.\n",
    "    file\n",
    "        File-like object to write report to. Default is standard output.\n",
    "    colorizer\n",
    "        Policy for emphasizing the written report.\n",
    "    lines_context\n",
    "        Number of lines of code to fetch and write up before and after the\n",
    "        line associated to the stack frame.\n",
    "    \"\"\"\n",
    "    header = (colorizer.trivial if Error.TAG_COMMON in frame.tags else Plain())(\" | \".join(get_id_frame(frame)))\n",
    "    print(header, file=file)\n",
    "    if Error.TAG_COMMON not in frame.tags:\n",
    "        context: List[Tuple[int, str]] = frame.context(before=lines_context, after=lines_context)\n",
    "        if len(context) > 0:\n",
    "            max_len_num_line = len(str(context[-1][0]))\n",
    "            for i, line in zip(\n",
    "                [i for i, _ in context],\n",
    "                highlight(\n",
    "                    \"\\n\".join(ln for _, ln in context),\n",
    "                    lexer=Python3Lexer(),\n",
    "                    formatter=TerminalFormatter()\n",
    "                ).split(\"\\n\")\n",
    "            ):\n",
    "                print(\n",
    "                    colorizer.trivial(f\"{i:{max_len_num_line}d}\"),\n",
    "                    colorizer.trivial(\"|\"),\n",
    "                    line,\n",
    "                    sep=\" \",\n",
    "                    file=file\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: show show 3 frame reports. First and third must have appropriate code context (check against the line number), three lines of context around target line max. First frame should have no function name. Second frame report should have subdued color and no code context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code cell 98 | Line 10\n",
      "\u001b[37m 9\u001b[0m \u001b[37m|\u001b[0m \u001b[34mtry\u001b[39;49;00m:\n",
      "\u001b[37m10\u001b[0m \u001b[37m|\u001b[0m     caller()\n",
      "\u001b[37m11\u001b[0m \u001b[37m|\u001b[0m     \u001b[34massert\u001b[39;49;00m \u001b[34mFalse\u001b[39;49;00m\n",
      "\u001b[37m12\u001b[0m \u001b[37m|\u001b[0m \u001b[34mexcept\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m:\n",
      "\u001b[37m13\u001b[0m \u001b[37m|\u001b[0m     _, _, tb = sys.exc_info()\n",
      "\n",
      "\u001b[37mFunction caller | Code cell 98 | Line 6\u001b[0m\n",
      "\n",
      "Function raiser | Code cell 98 | Line 2\n",
      "\u001b[37m1\u001b[0m \u001b[37m|\u001b[0m \u001b[34mdef\u001b[39;49;00m \u001b[32mraiser\u001b[39;49;00m():\n",
      "\u001b[37m2\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m()\n",
      "\u001b[37m3\u001b[0m \u001b[37m|\u001b[0m \n",
      "\u001b[37m4\u001b[0m \u001b[37m|\u001b[0m \n",
      "\u001b[37m5\u001b[0m \u001b[37m|\u001b[0m \u001b[34mdef\u001b[39;49;00m \u001b[32mcaller\u001b[39;49;00m():\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def raiser():\n",
    "    raise RuntimeError()\n",
    "\n",
    "\n",
    "def caller():\n",
    "    raiser()\n",
    "\n",
    "\n",
    "try:\n",
    "    caller()\n",
    "    assert False\n",
    "except RuntimeError:\n",
    "    _, _, tb = sys.exc_info()\n",
    "    for frame, lineno in walk_tb(tb):\n",
    "        print_frame(\n",
    "            Frame(\n",
    "                getframeinfo(frame),\n",
    "                lineno,\n",
    "                [Error.TAG_COMMON] if frame.f_code is caller.__code__ else [])\n",
    "        )\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailing one result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def detail_result(  # noqa\n",
    "    name_test: str,\n",
    "    result: Error,\n",
    "    prefix_header: str,\n",
    "    file: TextIOBase = sys.stdout,\n",
    "    colorizer: Colorizer = color(),\n",
    "    lines_context: int = 3,\n",
    "    print_border: bool = True\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Writes up a report regarding a single test result.\n",
    "\n",
    "    name_test\n",
    "        Name of the test the result was gotten for.\n",
    "    result\n",
    "        Error-type result to report on.\n",
    "    prefix_header\n",
    "        String prepended to the header of the result report.\n",
    "    file\n",
    "        File-like object to write report to. Default is standard output.\n",
    "    colorizer\n",
    "        Policy for emphasizing the written report.\n",
    "    lines_context\n",
    "        Number of lines of code to fetch and write up before and after the\n",
    "        line associated to the stack frame.\n",
    "    print_border\n",
    "        If True (default), prints a border of dashes above the result report.\n",
    "    \"\"\"\n",
    "    header = \" ** \".join([\n",
    "        prefix_header,\n",
    "        f\"Test {colorizer.important(name_test)}\",\n",
    "        {Failure: colorizer.failure, Error: colorizer.error}[type(result)](type(result).__name__)\n",
    "    ])\n",
    "    if print_border:\n",
    "        print(\"-\" * len(colors.strip_color(header)), file=file)\n",
    "    print(header, file=file)\n",
    "\n",
    "    if result.is_failure():\n",
    "        reason = result.reason\n",
    "        if not reason:\n",
    "            if result.type_exc is TestFailed:\n",
    "                reason = \"Test marked as failed.\"\n",
    "            elif result.type_exc is AssertionError:\n",
    "                reason = \"Assertion failed.\"\n",
    "            else:\n",
    "                reason = \"Ill-traced failure meaning. Please take it up to the maintainer.\"\n",
    "        print(reason, file=file)\n",
    "\n",
    "        if result.type_exc is DetailedAssertionError:\n",
    "            print(\"\\nArguments:\\n\", file=file)\n",
    "            args = deepcopy(result.kwops)\n",
    "            args.update({n: v for n, v in enumerate(result.ops)})\n",
    "            width_column_name = min(16, max([str(n) for n in args.keys()]))\n",
    "            for name, value in sorted(list(args.items()), key=lambda p: str(p[0])):\n",
    "                print(f\"    {n:{width_column_name}} = {repr(value)}\", file=file)\n",
    "    else:\n",
    "        explanation = str(result.value_exc)\n",
    "        print(\n",
    "            f\"{result.type_exc.__name__}\" + (f\": {explanation}\" if explanation else \" (no detail provided)\"),\n",
    "            file=file,\n",
    "            end=\"\\n\\n\"\n",
    "        )\n",
    "        for frame in result.traceback:  # First frame is always Suite.test, which is irrelevant.\n",
    "            print_frame(frame, file=file, colorizer=colorizer, lines_context=lines_context)\n",
    "            print(file=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: header should start with first word **HEY**, label the test as **second**, map it as a *Failure* and provide next the reason of the failure (deliberately marking the test as failed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "HEY ** Test \u001b[1msecond\u001b[0m ** \u001b[33mFailure\u001b[0m\n",
      "Test marked as failed.\n"
     ]
    }
   ],
   "source": [
    "name, rez = list(suite.results)[1]\n",
    "failure = list(rez)[0]\n",
    "assert isinstance(failure, Failure)\n",
    "detail_result(name, failure, \"HEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check:\n",
    "\n",
    "1. Header should start with first word **HEY**, label the test as **first**, map it as an *Error*.\n",
    "1. Type of exception raised, with a cue that no further detail had been provided.\n",
    "1. Backtrace dump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "HEY ** Test \u001b[1mfirst\u001b[0m ** \u001b[31mError\u001b[0m\n",
      "RuntimeError (no detail provided)\n",
      "\n",
      "\u001b[37mFunction test | Code cell 49 | Line 73\u001b[0m\n",
      "\n",
      "Function first | Code cell 80 | Line 6\n",
      "\u001b[37m4\u001b[0m \u001b[37m|\u001b[0m \u001b[90m@suite\u001b[39;49;00m.test\n",
      "\u001b[37m5\u001b[0m \u001b[37m|\u001b[0m \u001b[34mdef\u001b[39;49;00m \u001b[32mfirst\u001b[39;49;00m():\n",
      "\u001b[37m6\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m()\n",
      "\u001b[37m7\u001b[0m \u001b[37m|\u001b[0m \n",
      "\u001b[37m8\u001b[0m \u001b[37m|\u001b[0m \n",
      "\u001b[37m9\u001b[0m \u001b[37m|\u001b[0m \u001b[90m@suite\u001b[39;49;00m.test(name=\u001b[33m\"\u001b[39;49;00m\u001b[33mfirst\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name, rez = list(suite.results)[0]\n",
    "error = list(rez)[0]\n",
    "assert isinstance(error, Error)\n",
    "detail_result(name, error, \"HEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def detail_issues(  # noqa\n",
    "    suite: Suite,\n",
    "    file: TextIOBase = sys.stdout,\n",
    "    colorizer: Colorizer = color(),\n",
    "    lines_context: int = 3,\n",
    "    max_report: int = sys.maxsize,\n",
    "    on_error: PolicyReportingProblems = IGNORE\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Writes up a report detailing the issues encountered while running the test suite.\n",
    "\n",
    "    suite\n",
    "        The test suite.\n",
    "    file\n",
    "        The file-like object to write the report to. Default is standard output.\n",
    "    colorizer\n",
    "        Color scheme used for emphasizing the various bits of the report.\n",
    "    lines_context\n",
    "        Number of lines of context to provide around each line of code involved\n",
    "        in a reported problem.\n",
    "    max_report\n",
    "        Maximum number of problems to report on.\n",
    "    on_error\n",
    "        What to do when reporting results that include problems such as failures and\n",
    "        errors. If set to RAISE, it will raise a ProblemsEncountered exception, which\n",
    "        is useful when running the notebook as part of a CI/CD pipeline; otherwise,\n",
    "        or if set to IGNORE, nothing more is done than writing the report.\n",
    "        Default is IGNORE.\n",
    "    \"\"\"\n",
    "    summary = summarize_results(suite, file=None)\n",
    "    num_problems = summary[Failure] + summary[Error]\n",
    "    num_remaining: int = -1\n",
    "    if num_problems == 0:\n",
    "        if summary[Success] == 0:\n",
    "            print(\"No test run.\", file=file)\n",
    "        else:\n",
    "            print(f\"All {summary[Success]} tests passed. No failure nor error encountered.\", file=file)\n",
    "    else:\n",
    "        index = 1\n",
    "        for name, rez in suite.results:\n",
    "            if num_remaining < 0:\n",
    "                for r in rez:\n",
    "                    if not r.is_success():\n",
    "                        detail_result(name, r, f\"# {index}/{num_problems}\", lines_context=lines_context, file=file)\n",
    "                        print(file=file)\n",
    "\n",
    "                        if index >= max_report:\n",
    "                            num_remaining = num_problems - index\n",
    "                            break\n",
    "                        index += 1\n",
    "\n",
    "    if num_remaining > 0:\n",
    "        print(\n",
    "            colorizer.important(\n",
    "                f\"... plus {num_remaining} other issue{'s' if num_remaining > 1 else ''}.\"\n",
    "            ),\n",
    "            file=file\n",
    "        )\n",
    "    if on_error == RAISE:\n",
    "        raise_on_error(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: 3 problem reports expected. First one is for an undetailed error (`RuntimeError`), with a following backtrace dump. The first frame report of this dump (`Function test | `...) is in a subdued color. Second and third are for failures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "# 1/3 ** Test \u001b[1mfirst\u001b[0m ** \u001b[31mError\u001b[0m\n",
      "RuntimeError (no detail provided)\n",
      "\n",
      "\u001b[37mFunction test | Code cell 49 | Line 73\u001b[0m\n",
      "\n",
      "Function first | Code cell 80 | Line 6\n",
      "\u001b[37m4\u001b[0m \u001b[37m|\u001b[0m \u001b[90m@suite\u001b[39;49;00m.test\n",
      "\u001b[37m5\u001b[0m \u001b[37m|\u001b[0m \u001b[34mdef\u001b[39;49;00m \u001b[32mfirst\u001b[39;49;00m():\n",
      "\u001b[37m6\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m()\n",
      "\u001b[37m7\u001b[0m \u001b[37m|\u001b[0m \n",
      "\u001b[37m8\u001b[0m \u001b[37m|\u001b[0m \n",
      "\u001b[37m9\u001b[0m \u001b[37m|\u001b[0m \u001b[90m@suite\u001b[39;49;00m.test(name=\u001b[33m\"\u001b[39;49;00m\u001b[33mfirst\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "-------------------------------\n",
      "# 2/3 ** Test \u001b[1msecond\u001b[0m ** \u001b[33mFailure\u001b[0m\n",
      "Test marked as failed.\n",
      "\n",
      "-------------------------------\n",
      "# 3/3 ** Test \u001b[1mfourth\u001b[0m ** \u001b[33mFailure\u001b[0m\n",
      "Assertion failed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "detail_issues(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: show only the report for the error, and mention that two more problems have yet to be reported. Also, code context is only one line around the target line, making for a 3-line code blurb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "# 1/3 ** Test \u001b[1mfirst\u001b[0m ** \u001b[31mError\u001b[0m\n",
      "RuntimeError (no detail provided)\n",
      "\n",
      "\u001b[37mFunction test | Code cell 49 | Line 73\u001b[0m\n",
      "\n",
      "Function first | Code cell 80 | Line 6\n",
      "\u001b[37m5\u001b[0m \u001b[37m|\u001b[0m \u001b[34mdef\u001b[39;49;00m \u001b[32mfirst\u001b[39;49;00m():\n",
      "\u001b[37m6\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m()\n",
      "\n",
      "\n",
      "\u001b[1m... plus 2 other issues.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "detail_issues(suite, max_report=1, lines_context=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same, but with exception raising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "# 1/3 ** Test \u001b[1mfirst\u001b[0m ** \u001b[31mError\u001b[0m\n",
      "RuntimeError (no detail provided)\n",
      "\n",
      "\u001b[37mFunction test | Code cell 49 | Line 73\u001b[0m\n",
      "\n",
      "Function first | Code cell 80 | Line 6\n",
      "\u001b[37m5\u001b[0m \u001b[37m|\u001b[0m \u001b[34mdef\u001b[39;49;00m \u001b[32mfirst\u001b[39;49;00m():\n",
      "\u001b[37m6\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m()\n",
      "\n",
      "\n",
      "\u001b[1m... plus 2 other issues.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    detail_issues(suite, max_report=1, lines_context=1, on_error=RAISE)\n",
    "    assert False\n",
    "except ProblemsEncountered as err:\n",
    "    assert err.num_failures == 2\n",
    "    assert err.num_errors == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: now show the error and the first failure, with 0 code context (so only the target line is shown). The final message about remaining issues is singular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "# 1/3 ** Test \u001b[1mfirst\u001b[0m ** \u001b[31mError\u001b[0m\n",
      "RuntimeError (no detail provided)\n",
      "\n",
      "\u001b[37mFunction test | Code cell 49 | Line 73\u001b[0m\n",
      "\n",
      "Function first | Code cell 80 | Line 6\n",
      "\u001b[37m6\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m()\n",
      "\n",
      "\n",
      "-------------------------------\n",
      "# 2/3 ** Test \u001b[1msecond\u001b[0m ** \u001b[33mFailure\u001b[0m\n",
      "Test marked as failed.\n",
      "\n",
      "\u001b[1m... plus 1 other issue.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "detail_issues(suite, max_report=2, lines_context=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: two tests passed, no failure nor error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 2 tests passed. No failure nor error encountered.\n"
     ]
    }
   ],
   "source": [
    "detail_issues(suite_all_passed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: no test has been run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No test run.\n"
     ]
    }
   ],
   "source": [
    "detail_issues(suite_empty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On-the-fly result reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Report(Subscriber):\n",
    "    \"\"\"\n",
    "    Test suite subscriber that reports on test results on-the-fly. When a test\n",
    "    does not succeed, details on the failure can optionally be provided. See\n",
    "    documentation on class `Suite` to get an example on the usage of this\n",
    "    plug-in.\n",
    "\n",
    "    file\n",
    "        File-like object where the test results are reported. Default is\n",
    "        standard output.\n",
    "    verbose\n",
    "        If True, the feedback on test results contains traceback information\n",
    "        when problems are encountered.\n",
    "    file\n",
    "        File-like object where the feedback is put out.\n",
    "    colorizer\n",
    "        Policy on how to emphasize the feedback output.\n",
    "    lines_context\n",
    "        Number of lines of code to provide as context in traceback frames\n",
    "        around the line of code at the nexus of an issue.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        verbose: bool = True,\n",
    "        file: TextIOBase = sys.stdout,\n",
    "        colorizer: Colorizer = color(),\n",
    "        lines_context: int = 3\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._file = file\n",
    "        self._verbose = verbose\n",
    "        self._colorizer = colorizer\n",
    "        self._lines_context = lines_context\n",
    "\n",
    "    def on_result(self, name_test: str, result: Result) -> None:\n",
    "        name_test = self._colorizer.important(name_test)\n",
    "        if result.is_success():\n",
    "            print(f\"Test {name_test} passed.\", file=self._file)\n",
    "        else:\n",
    "            if self._verbose:\n",
    "                detail_result(\n",
    "                    name_test,\n",
    "                    result,\n",
    "                    \"Issue\",\n",
    "                    self._file,\n",
    "                    self._colorizer,\n",
    "                    self._lines_context,\n",
    "                    print_border=False\n",
    "                )\n",
    "            else:\n",
    "                frame_relevant = result.traceback[-1]\n",
    "                details_frame = get_id_frame(frame_relevant)\n",
    "                if result.is_failure():\n",
    "                    label = self._colorizer.failure(f\"Failed test {name_test}\")\n",
    "                    print(f\"{label}: {result.reason} | {' | '.join(details_frame)}\", file=self._file)\n",
    "                else:\n",
    "                    label = self._colorizer.error(f\"Error occured during test {name_test}\")\n",
    "                    value_exc = \"\"\n",
    "                    if str(result.value_exc):\n",
    "                        value_exc = f\" -- {str(result.value_exc)}\"\n",
    "                    print(f\"{label}: {result.type_exc.__name__}{value_exc}\", file=self._file)\n",
    "                    print_frame(\n",
    "                        frame_relevant,\n",
    "                        file=self._file,\n",
    "                        colorizer=self._colorizer,\n",
    "                        lines_context=self._lines_context\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exercise_suite(reporter):\n",
    "    suite = Suite() | reporter\n",
    "\n",
    "    @suite.test\n",
    "    def passing():\n",
    "        pass\n",
    "\n",
    "    @suite.test\n",
    "    def failing():\n",
    "        assert 1 == 0, \"one vs. zero\"\n",
    "\n",
    "    @suite.test\n",
    "    def failing_no_reason():\n",
    "        assert 1 == 0\n",
    "\n",
    "    @suite.test\n",
    "    def error_raising():\n",
    "        raise RuntimeError(\"Some error\")\n",
    "\n",
    "    @suite.test\n",
    "    def error_raising_no_str():\n",
    "        raise RuntimeError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: first and second tests pass, third test fails, fourth test raises an error, last two tests pass. Failure and error are tersely described, only the most relevant stack frame is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mpassing\u001b[0m passed.\n",
      "\u001b[33mFailed test \u001b[1mfailing\u001b[0m\u001b[0m: one vs. zero | Function failing | Code cell 110 | Line 10\n",
      "\u001b[33mFailed test \u001b[1mfailing_no_reason\u001b[0m\u001b[0m:  | Function failing_no_reason | Code cell 110 | Line 14\n",
      "\u001b[31mError occured during test \u001b[1merror_raising\u001b[0m\u001b[0m: RuntimeError -- Some error\n",
      "Function error_raising | Code cell 110 | Line 18\n",
      "\u001b[37m16\u001b[0m \u001b[37m|\u001b[0m     \u001b[90m@suite\u001b[39;49;00m.test\n",
      "\u001b[37m17\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mdef\u001b[39;49;00m \u001b[32merror_raising\u001b[39;49;00m():\n",
      "\u001b[37m18\u001b[0m \u001b[37m|\u001b[0m         \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mSome error\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\u001b[37m19\u001b[0m \u001b[37m|\u001b[0m \n",
      "\u001b[37m20\u001b[0m \u001b[37m|\u001b[0m     \u001b[90m@suite\u001b[39;49;00m.test\n",
      "\u001b[37m21\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mdef\u001b[39;49;00m \u001b[32merror_raising_no_str\u001b[39;49;00m():\n",
      "\u001b[31mError occured during test \u001b[1merror_raising_no_str\u001b[0m\u001b[0m: RuntimeError\n",
      "Function error_raising_no_str | Code cell 110 | Line 22\n",
      "\u001b[37m20\u001b[0m \u001b[37m|\u001b[0m     \u001b[90m@suite\u001b[39;49;00m.test\n",
      "\u001b[37m21\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mdef\u001b[39;49;00m \u001b[32merror_raising_no_str\u001b[39;49;00m():\n",
      "\u001b[37m22\u001b[0m \u001b[37m|\u001b[0m         \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m()\n"
     ]
    }
   ],
   "source": [
    "exercise_suite(Report(verbose=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: like previous, but with full detail of each non-success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mpassing\u001b[0m passed.\n",
      "Issue ** Test \u001b[1m\u001b[1mfailing\u001b[0m\u001b[0m ** \u001b[33mFailure\u001b[0m\n",
      "one vs. zero\n",
      "Issue ** Test \u001b[1m\u001b[1mfailing_no_reason\u001b[0m\u001b[0m ** \u001b[33mFailure\u001b[0m\n",
      "Assertion failed.\n",
      "Issue ** Test \u001b[1m\u001b[1merror_raising\u001b[0m\u001b[0m ** \u001b[31mError\u001b[0m\n",
      "RuntimeError: Some error\n",
      "\n",
      "\u001b[37mFunction test | Code cell 49 | Line 73\u001b[0m\n",
      "\n",
      "Function error_raising | Code cell 110 | Line 18\n",
      "\u001b[37m16\u001b[0m \u001b[37m|\u001b[0m     \u001b[90m@suite\u001b[39;49;00m.test\n",
      "\u001b[37m17\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mdef\u001b[39;49;00m \u001b[32merror_raising\u001b[39;49;00m():\n",
      "\u001b[37m18\u001b[0m \u001b[37m|\u001b[0m         \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mSome error\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\u001b[37m19\u001b[0m \u001b[37m|\u001b[0m \n",
      "\u001b[37m20\u001b[0m \u001b[37m|\u001b[0m     \u001b[90m@suite\u001b[39;49;00m.test\n",
      "\u001b[37m21\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mdef\u001b[39;49;00m \u001b[32merror_raising_no_str\u001b[39;49;00m():\n",
      "\n",
      "Issue ** Test \u001b[1m\u001b[1merror_raising_no_str\u001b[0m\u001b[0m ** \u001b[31mError\u001b[0m\n",
      "RuntimeError (no detail provided)\n",
      "\n",
      "\u001b[37mFunction test | Code cell 49 | Line 73\u001b[0m\n",
      "\n",
      "Function error_raising_no_str | Code cell 110 | Line 22\n",
      "\u001b[37m20\u001b[0m \u001b[37m|\u001b[0m     \u001b[90m@suite\u001b[39;49;00m.test\n",
      "\u001b[37m21\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mdef\u001b[39;49;00m \u001b[32merror_raising_no_str\u001b[39;49;00m():\n",
      "\u001b[37m22\u001b[0m \u001b[37m|\u001b[0m         \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exercise_suite(Report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a cell magic to write tests\n",
    "\n",
    "The current decorator approach involves little boilerplate, but it can be reduced further nonetheless. Let's use a cell magic, in which all code written is wrapped into an ad hoc test function. Let's use a subscriber to register this cell magic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Magic(Subscriber):\n",
    "\n",
    "    def __init__(self, name_magic: str = \"test\") -> None:\n",
    "        self._name_magic = name_magic\n",
    "\n",
    "    def on_add_to_suite(self, suite):\n",
    "        suite._fns_common.append(run_test_from_cell)\n",
    "        ipython = get_ipython()\n",
    "        if ipython and self._name_magic:\n",
    "            register_cell_magic(self._name_magic)(lambda line, cell: test_cell(suite, line, cell))\n",
    "\n",
    "\n",
    "def test_cell(suite: Suite, line: str, cell: Optional[str]) -> None:\n",
    "    \"\"\"\n",
    "    Runs a test written using a cell magic.\n",
    "    \"\"\"\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        raise ValueError(\"Please provide a title for the test (right after the cell magic invocation).\")\n",
    "    cell = (cell or \"\").strip()\n",
    "    if not cell:\n",
    "        raise ValueError(\"There is no test to execute! Please write some code in there.\")\n",
    "\n",
    "    suite.test(run_test_from_cell, name=line, args=(cell,))\n",
    "\n",
    "\n",
    "def run_test_from_cell(cell: str) -> None:\n",
    "    \"\"\"\n",
    "    Executes the body of a cell, in context of the execution of a test.\n",
    "    \"\"\"\n",
    "    code_source = \"\\n\" + cell\n",
    "    ipython = get_ipython()\n",
    "    name_cell = ipython.compile.cache(code_source)\n",
    "    code = compile(code_source, name_cell, \"exec\")\n",
    "    exec(code, ipython.user_global_ns, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple usage of the magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = Suite() | Magic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%test First test, passing\n",
    "assert True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%test Second test, failing\n",
    "fail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%test Third test, raising an error\n",
    "raise RuntimeError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_function(x, y):\n",
    "    return x * y + x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%test function in my notebook\n",
    "assert my_function(4, 5) == 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "assert {name: [r[\"type\"] for r in rez] for name, rez in suite.as_dict().items()} == {\n",
    "    \"First test, passing\": [\"Success\"],\n",
    "    \"Second test, failing\": [\"Failure\"],\n",
    "    \"Third test, raising an error\": [\"Error\"],\n",
    "    \"function in my notebook\": [\"Success\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure even with the cell magic environment is not polluted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in [\"x\", \"f\", \"C\"]:\n",
    "    for ns in [globals(), locals(), get_ipython().user_ns]:\n",
    "        assert name not in ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%test Add x, f and C\n",
    "x = 5\n",
    "\n",
    "def f():\n",
    "    return 5\n",
    "\n",
    "\n",
    "class C():\n",
    "    \n",
    "    def f(self):\n",
    "        return 5\n",
    "    \n",
    "    \n",
    "assert f() == x\n",
    "assert C().f() == f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in [\"x\", \"f\", \"C\"]:\n",
    "    for ns in [globals(), locals(), get_ipython().user_ns]:\n",
    "        try:\n",
    "            assert name not in ns\n",
    "        except AssertionError:\n",
    "            print(f\"Name {name} still defined.\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailing one result, obtained by testing with cell magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite_magic = Suite() | Magic(\"test_magic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%test_magic the label\n",
    "x = 5\n",
    "assert x == 5\n",
    "fail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: header should start with first word **Head**, label the test as **the label** and map it as a *Failure*. Its frame report shows three subdued frames without code context, sandwiching a frame showing the call to the `fail()` function (line 16) that tripped the failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Head ** Test \u001b[1mthe label\u001b[0m ** \u001b[33mFailure\u001b[0m\n",
      "Test marked as failed.\n"
     ]
    }
   ],
   "source": [
    "name, rez = list(suite_magic.results)[0]\n",
    "failure = list(rez)[0]\n",
    "assert isinstance(failure, Failure)\n",
    "detail_result(name, failure, \"Head\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation index\n",
    "\n",
    "Let's compose a docstring for the `jupytest` module (which we will export out of bits of this notebook). This docstring will act as a reference documentation index."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": [
     "header"
    ]
   },
   "source": [
    "Unit and integration testing in a notebook\n",
    "\n",
    "*** Building and running suites of tests ***\n",
    "\n",
    "class Suite\n",
    "    Method test (context manager)\n",
    "Function fail\n",
    "\n",
    "*** Reporting test results ***\n",
    "\n",
    "Function report_results\n",
    "Function summarize_results\n",
    "Function detail_issues\n",
    "Class Report (used as a subscriber plug-in to class Suite)\n",
    "\n",
    "*** Delving deeper into test results (going beyond the tools described above) ***\n",
    "\n",
    "Class Suite\n",
    "    Property results\n",
    "Class Result\n",
    "    Sub-class Success\n",
    "    Sub-class Error\n",
    "        Sub-class Failure\n",
    "    Method is_success\n",
    "    Method is_failure\n",
    "Class Frame\n",
    "\n",
    "*** Customizing result reporting ***\n",
    "\n",
    "Class Colorizer\n",
    "    Function plain\n",
    "    Function color\n",
    "Class Emphasis\n",
    "    Sub-class Plain\n",
    "    Sub-class Color\n",
    "Type TestNameFormatter\n",
    "    Function ladder\n",
    "    Function name_all\n",
    "    Function quoter\n",
    "Function detail_results\n",
    "Function print_frame\n",
    "\n",
    "*** Checking out the intricacies of test isolation ***\n",
    "\n",
    "Function protect_environment\n",
    "\n",
    "Happy jupytesting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exportation of testing tools to `jupytest.py` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean up previous\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "import shutil\n",
    "\n",
    "if os.path.isdir(\"jupytest\"):\n",
    "    print(\"Clean up previous\")\n",
    "    shutil.rmtree(\"jupytest\")\n",
    "os.makedirs(\"jupytest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "with open(\"jupytest.ipynb\", \"r\", encoding=\"utf-8\") as file_notebook:\n",
    "    nb = nbformat.read(file_notebook, nbformat.NO_CONVERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"jupytest/__init__.py\", \"w\", encoding=\"utf-8\") as file_module:\n",
    "    print(\n",
    "        \"\\n\\n\".join(\n",
    "            [\n",
    "                \"\\n\\n\".join(\n",
    "                    f'\"\"\"\\n{cell.source}\\n\"\"\"'\n",
    "                    for cell in nb.cells\n",
    "                    if cell.cell_type == 'raw' and \"header\" in cell.metadata.get(\"tags\", [])\n",
    "                )\n",
    "            ] +\n",
    "            [\n",
    "                cell.source\n",
    "                for cell in nb.cells\n",
    "                if cell.cell_type == 'code' and \"module\" in cell.metadata.get(\"tags\", [])\n",
    "            ]\n",
    "        ),\n",
    "        file=file_module,\n",
    "        end=\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "# Force the reload of the new jupytest module. If running from a fresh\n",
    "# kernel, the reload is spurious but innocuous.\n",
    "import jupytest\n",
    "importlib.reload(jupytest)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the exported module has all the tools we defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "for construct in [\n",
    "    \"Result\",\n",
    "    \"Success\",\n",
    "    \"Frame\",\n",
    "    \"Error\",\n",
    "    \"Failure\",\n",
    "    \"TestFailed\",\n",
    "    \"fail\",\n",
    "    \"Subscriber\",\n",
    "    \"Suite\",\n",
    "    \"Emphasis\",\n",
    "    \"Color\",\n",
    "    \"Plain\",\n",
    "    \"Colorizer\",\n",
    "    \"color\",\n",
    "    \"plain\",\n",
    "    \"TestNameFormatter\",\n",
    "    \"ladder\",\n",
    "    \"name_all\",\n",
    "    \"quoter\",\n",
    "    \"ProblemsEncountered\",\n",
    "    \"raise_on_error\",\n",
    "    \"PolicyReportingProblems\",\n",
    "    \"IGNORE\",\n",
    "    \"RAISE\",\n",
    "    \"report_results\",\n",
    "    \"summarize_results\",\n",
    "    \"print_frame\",\n",
    "    \"detail_result\",\n",
    "    \"detail_issues\",\n",
    "    \"Report\",\n",
    "    \"join_args\",\n",
    "    \"Explanation\",\n",
    "    \"DetailedAssertionError\",\n",
    "    \"assert_\",\n",
    "    \"_assess_sequence_2more\",\n",
    "    \"_assessor_sequence\",\n",
    "    \"same\",\n",
    "    \"eq\",\n",
    "    \"diff\",\n",
    "    \"between\",\n",
    "    \"approx\",\n",
    "    \"belong_to\",\n",
    "    \"is_any_of\"\n",
    "]:\n",
    "    assert hasattr(jupytest, construct), f\"Have not got construct {construct}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: ensure the documentation for the `jupytest` module itself corresponds to the `header` cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package jupytest:\n",
      "\n",
      "NAME\n",
      "    jupytest - Unit and integration testing in a notebook\n",
      "\n",
      "DESCRIPTION\n",
      "    *** Building and running suites of tests ***\n",
      "    \n",
      "    class Suite\n",
      "        Method test (context manager)\n",
      "    Function fail\n",
      "    \n",
      "    *** Reporting test results ***\n",
      "    \n",
      "    Function report_results\n",
      "    Function summarize_results\n",
      "    Function detail_issues\n",
      "    Class Report (used as a subscriber plug-in to class Suite)\n",
      "    \n",
      "    *** Delving deeper into test results (going beyond the tools described above) ***\n",
      "    \n",
      "    Class Suite\n",
      "        Property results\n",
      "    Class Result\n",
      "        Sub-class Success\n",
      "        Sub-class Error\n",
      "            Sub-class Failure\n",
      "        Method is_success\n",
      "        Method is_failure\n",
      "    Class Frame\n",
      "    \n",
      "    *** Customizing result reporting ***\n",
      "    \n",
      "    Class Colorizer\n",
      "        Function plain\n",
      "        Function color\n",
      "    Class Emphasis\n",
      "        Sub-class Plain\n",
      "        Sub-class Color\n",
      "    Type TestNameFormatter\n",
      "        Function ladder\n",
      "        Function name_all\n",
      "        Function quoter\n",
      "    Function detail_results\n",
      "    Function print_frame\n",
      "    \n",
      "    *** Checking out the intricacies of test isolation ***\n",
      "    \n",
      "    Function protect_environment\n",
      "    \n",
      "    Happy jupytesting!\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "\n",
      "\n",
      "CLASSES\n",
      "    abc.ABC(builtins.object)\n",
      "        Emphasis\n",
      "            Color\n",
      "            Plain\n",
      "        Result\n",
      "            Error\n",
      "                Failure\n",
      "            Success\n",
      "        Subscriber\n",
      "            Magic\n",
      "            Report\n",
      "    builtins.AssertionError(builtins.Exception)\n",
      "        DetailedAssertionError\n",
      "    builtins.Exception(builtins.BaseException)\n",
      "        ProblemsEncountered\n",
      "        TestFailed\n",
      "    builtins.object\n",
      "        Colorizer\n",
      "        Explanation\n",
      "        Frame\n",
      "        PolicyReportingProblems\n",
      "        Suite\n",
      "    \n",
      "    class Color(Emphasis)\n",
      "     |  Color(fg=None, bg=None, style=None) -> None\n",
      "     |  \n",
      "     |  Helper class that provides a standard way to create an ABC using\n",
      "     |  inheritance.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Color\n",
      "     |      Emphasis\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, s: str) -> str\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __init__(self, fg=None, bg=None, style=None) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Emphasis:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Colorizer(builtins.object)\n",
      "     |  Colorizer(important: jupytest.Emphasis, trivial: jupytest.Emphasis, failure: jupytest.Emphasis, error: jupytest.Emphasis) -> None\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, important: jupytest.Emphasis, trivial: jupytest.Emphasis, failure: jupytest.Emphasis, error: jupytest.Emphasis) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class DetailedAssertionError(builtins.AssertionError)\n",
      "     |  DetailedAssertionError(explanation: jupytest.Explanation)\n",
      "     |  \n",
      "     |  Assertion failed.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DetailedAssertionError\n",
      "     |      builtins.AssertionError\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, explanation: jupytest.Explanation)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.AssertionError:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "    \n",
      "    class Emphasis(abc.ABC)\n",
      "     |  Helper class that provides a standard way to create an ABC using\n",
      "     |  inheritance.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Emphasis\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, s: str) -> str\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'__call__'})\n",
      "    \n",
      "    class Error(Result)\n",
      "     |  Error(fns_common: Iterable[Callable]) -> None\n",
      "     |  \n",
      "     |  Non-passing test result due to an exception being raised.\n",
      "     |  \n",
      "     |  It is passed a set of common functions: the presence of these functions in the\n",
      "     |  traceback of the exception are expected and normal, making their eventual\n",
      "     |  reporting redundant and sort of trivial. The frames corresponding to these functions\n",
      "     |  in the traceback summary kept by this object will be tagged as such.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Error\n",
      "     |      Result\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, fns_common: Iterable[Callable]) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  as_dict(self, context_before: int = 3, context_after: int = 3) -> Dict\n",
      "     |      Expresses this result as a dictionary suitable to structured data serialization.\n",
      "     |  \n",
      "     |  is_success(self) -> bool\n",
      "     |      True when an associated test run has passed.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  traceback\n",
      "     |      Returns a summary of the stack trace associated to the exception that brought this test result.\n",
      "     |  \n",
      "     |  type_exc\n",
      "     |      Returns the type of the exception associated to this result.\n",
      "     |  \n",
      "     |  value_exc\n",
      "     |      Returns the exception raised in association to this test result.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  TAG_COMMON = 'common'\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Result:\n",
      "     |  \n",
      "     |  is_failure(self) -> bool\n",
      "     |      True when an associated has not passed because a designed failure condition was met.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Result:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Explanation(builtins.object)\n",
      "     |  Explanation(main: str, culprits: Sequence[Union[Tuple[Union[int, str], Any], Tuple[str]]]) -> None\n",
      "     |  \n",
      "     |  Embodies the reason why a predicate fails, including a main expectation that was not satisfied and\n",
      "     |  a list of assertion arguments that tripped the failure.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __bool__(self) -> bool\n",
      "     |      All explanations are for failures, so when a predicate returns an explanation, it's as if it had\n",
      "     |      returned False.\n",
      "     |  \n",
      "     |  __init__(self, main: str, culprits: Sequence[Union[Tuple[Union[int, str], Any], Tuple[str]]]) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Failure(Error)\n",
      "     |  Failure(reason: str, fns_common: Iterable[Callable])\n",
      "     |  \n",
      "     |  Test result stemming from a condition check that failed, or a test run marked\n",
      "     |  as a failure.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Failure\n",
      "     |      Error\n",
      "     |      Result\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, reason: str, fns_common: Iterable[Callable])\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  as_dict(self, context_before: int = 3, context_after: int = 3) -> Dict\n",
      "     |      Expresses this result as a dictionary suitable to structured data serialization.\n",
      "     |  \n",
      "     |  is_failure(self) -> bool\n",
      "     |      True when an associated has not passed because a designed failure condition was met.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  reason\n",
      "     |      Reason given by the programmer as to why the test failed.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Error:\n",
      "     |  \n",
      "     |  is_success(self) -> bool\n",
      "     |      True when an associated test run has passed.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Error:\n",
      "     |  \n",
      "     |  traceback\n",
      "     |      Returns a summary of the stack trace associated to the exception that brought this test result.\n",
      "     |  \n",
      "     |  type_exc\n",
      "     |      Returns the type of the exception associated to this result.\n",
      "     |  \n",
      "     |  value_exc\n",
      "     |      Returns the exception raised in association to this test result.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Error:\n",
      "     |  \n",
      "     |  TAG_COMMON = 'common'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Result:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Frame(builtins.object)\n",
      "     |  Frame(tb: inspect.Traceback, num_line: int, tags: Union[List[str], NoneType] = None) -> None\n",
      "     |  \n",
      "     |  Information regarding a frame of a traceback. Provides more than the very limited\n",
      "     |  code context that comes from standard library introspection tools.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, tb: inspect.Traceback, num_line: int, tags: Union[List[str], NoneType] = None) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __str__(self) -> str\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  as_dict(self, context_before: int = 3, context_after: int = 3) -> Dict\n",
      "     |  \n",
      "     |  context(self, before: int = 3, after: int = 3) -> List[Tuple[int, str]]\n",
      "     |  \n",
      "     |  name_file_pretty(self, capitalized: bool = False) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Magic(Subscriber)\n",
      "     |  Magic(name_magic: str = 'test') -> None\n",
      "     |  \n",
      "     |  Object reacting to test results as they are generated by running tests.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Magic\n",
      "     |      Subscriber\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, name_magic: str = 'test') -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  on_add_to_suite(self, suite)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Subscriber:\n",
      "     |  \n",
      "     |  on_result(self, name_test: str, result: jupytest.Result) -> None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Subscriber:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Plain(Emphasis)\n",
      "     |  Helper class that provides a standard way to create an ABC using\n",
      "     |  inheritance.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Plain\n",
      "     |      Emphasis\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, s: str) -> str\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Emphasis:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class PolicyReportingProblems(builtins.object)\n",
      "     |  PolicyReportingProblems(label: str) -> None\n",
      "     |  \n",
      "     |  What to do when reporting test results that involve problems (failures and errors).\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __init__(self, label: str) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __str__(self) -> str\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "    \n",
      "    class ProblemsEncountered(builtins.Exception)\n",
      "     |  ProblemsEncountered(num_failures, num_errors)\n",
      "     |  \n",
      "     |  Raised (optionally) when a reporting routine must report failures and errors.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ProblemsEncountered\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, num_failures, num_errors)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.Exception:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "    \n",
      "    class Report(Subscriber)\n",
      "     |  Report(verbose: bool = True, file: io.TextIOBase = <ipykernel.iostream.OutStream object at 0x7f1fd030e4f0>, colorizer: jupytest.Colorizer = <jupytest.Colorizer object at 0x7f1fb7b6c7c0>, lines_context: int = 3) -> None\n",
      "     |  \n",
      "     |  Test suite subscriber that reports on test results on-the-fly. When a test\n",
      "     |  does not succeed, details on the failure can optionally be provided. See\n",
      "     |  documentation on class `Suite` to get an example on the usage of this\n",
      "     |  plug-in.\n",
      "     |  \n",
      "     |  file\n",
      "     |      File-like object where the test results are reported. Default is\n",
      "     |      standard output.\n",
      "     |  verbose\n",
      "     |      If True, the feedback on test results contains traceback information\n",
      "     |      when problems are encountered.\n",
      "     |  file\n",
      "     |      File-like object where the feedback is put out.\n",
      "     |  colorizer\n",
      "     |      Policy on how to emphasize the feedback output.\n",
      "     |  lines_context\n",
      "     |      Number of lines of code to provide as context in traceback frames\n",
      "     |      around the line of code at the nexus of an issue.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Report\n",
      "     |      Subscriber\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, verbose: bool = True, file: io.TextIOBase = <ipykernel.iostream.OutStream object at 0x7f1fd030e4f0>, colorizer: jupytest.Colorizer = <jupytest.Colorizer object at 0x7f1fb7b6c7c0>, lines_context: int = 3) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  on_result(self, name_test: str, result: jupytest.Result) -> None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Subscriber:\n",
      "     |  \n",
      "     |  on_add_to_suite(self, suite: 'Suite') -> None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Subscriber:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Result(abc.ABC)\n",
      "     |  Result of a test. Indicates whether the test passed (was a success), and if it did not,\n",
      "     |  whether it was a failure (as opposed to any other kind of issue).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Result\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  as_dict(self) -> Dict\n",
      "     |      Expresses this result as a dictionary suitable to structured data serialization.\n",
      "     |  \n",
      "     |  is_failure(self) -> bool\n",
      "     |      True when an associated has not passed because a designed failure condition was met.\n",
      "     |  \n",
      "     |  is_success(self) -> bool\n",
      "     |      True when an associated test run has passed.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'is_success'})\n",
      "    \n",
      "    class Subscriber(abc.ABC)\n",
      "     |  Object reacting to test results as they are generated by running tests.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Subscriber\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  on_add_to_suite(self, suite: 'Suite') -> None\n",
      "     |  \n",
      "     |  on_result(self, name_test: str, result: jupytest.Result) -> None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "    \n",
      "    class Success(Result)\n",
      "     |  Result for a test that passed.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Success\n",
      "     |      Result\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  is_success(self) -> bool\n",
      "     |      True when an associated test run has passed.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Result:\n",
      "     |  \n",
      "     |  as_dict(self) -> Dict\n",
      "     |      Expresses this result as a dictionary suitable to structured data serialization.\n",
      "     |  \n",
      "     |  is_failure(self) -> bool\n",
      "     |      True when an associated has not passed because a designed failure condition was met.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Result:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Suite(builtins.object)\n",
      "     |  Suite(name_magic: str = 'test') -> None\n",
      "     |  \n",
      "     |  Suite of tests, gathering the result of multiple named test runs. Test code fragments\n",
      "     |  are named using the `test()` decorator, or leveraging it indirectly by registering\n",
      "     |  a shortcut cell magic.\n",
      "     |  \n",
      "     |  Test suites gets added functionality through a publish/subscribe system. Subscriber are\n",
      "     |  special objects tied to the suite instance through its `|' (bit OR) operator. At the\n",
      "     |  moment, the only event broadcast to all subscribers is the generation of a new test\n",
      "     |  result (and its appending to the suite's log). For instance, the `Report` plug-in\n",
      "     |  supports the suite by giving immediate feedback on a test's results. Thus, to\n",
      "     |  instantiate a suite with this added feature, one would use code like\n",
      "     |  \n",
      "     |  suite = Suite() | Report()\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, name_magic: str = 'test') -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __or__(self, subscriber: jupytest.Subscriber) -> 'Suite'\n",
      "     |      Generates a clone of this suite instance, but with this subscriber subscribed to it.\n",
      "     |      \n",
      "     |      The new suite will not share member data structures with `self`, but if `self` carries\n",
      "     |      test results already, the new suite will reference the same result objects -- we\n",
      "     |      assume that Result objects are immutable.\n",
      "     |  \n",
      "     |  as_dict(self) -> Dict[str, List[Dict]]\n",
      "     |      Provides a structured data representation suitable for data serialization and exportation.\n",
      "     |  \n",
      "     |  test(self, fn: Union[Callable[..., NoneType], NoneType] = None, name: str = '', args: Sequence[Any] = [], kwargs: Mapping[str, Any] = {}) -> Union[Callable[[Callable[..., NoneType]], Callable[..., NoneType]], Callable[..., NoneType]]\n",
      "     |      Runs a test encoded into a function. Completing the function's execution counts as a\n",
      "     |      test success; tripped assertions and other exceptions generate some other Result;\n",
      "     |      and the test result is retained by this Suite instance.\n",
      "     |      \n",
      "     |      This decorator can be used two ways. Without application, one can decorate a\n",
      "     |      test function without parameter:\n",
      "     |      \n",
      "     |          @suite.test\n",
      "     |          def this_is_my_test():\n",
      "     |              # Test goes here!\n",
      "     |      \n",
      "     |      The name of the test corresponds to that of the function. Applying the decorator can\n",
      "     |      supply arguments to the test function and override the name of the test.\n",
      "     |      \n",
      "     |          @suite.test(name=\"My test, with spaces\", args=(3, 4))\n",
      "     |          def fn_test(a, b):\n",
      "     |              # Test goes here!\n",
      "     |      \n",
      "     |      To run a test with multiple parameter sets, one may even call this function directly,\n",
      "     |      not as a decorator:\n",
      "     |      \n",
      "     |          def fn_test(a, b):\n",
      "     |              # Test test test...\n",
      "     |      \n",
      "     |          for a, b in [(2, 8), (3, 4)]:\n",
      "     |              suite.test(fn_test, name=f\"Test with {a}, {b}\", args=(a, b))\n",
      "     |      \n",
      "     |      fn\n",
      "     |          Function that embodies the test code.\n",
      "     |      name\n",
      "     |          Name of the test; by default, the name of the function is used.\n",
      "     |      args\n",
      "     |          Positional arguments to pass to the function to run the test.\n",
      "     |      kwargs\n",
      "     |          Named arguments to pass to the function to run the test.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  results\n",
      "     |      Iterates through the gathered test results. For each named test, yields a tuple of\n",
      "     |      the name of the test and an iterator over each result gathered as the test has\n",
      "     |      been run.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class TestFailed(builtins.Exception)\n",
      "     |  TestFailed(reason: str) -> None\n",
      "     |  \n",
      "     |  Exception raised by this framework in order to mark a test run as a Failure.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TestFailed\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, reason: str) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.Exception:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "\n",
      "FUNCTIONS\n",
      "    approx(centre: Any, tol: Any = 1e-06, bounds: str = '[]') -> Callable[..., Union[bool, str, jupytest.Explanation]]\n",
      "        Returns a predicate satisfied when all arguments land within an interval defined by a centre and a tolerance\n",
      "        (which corresponds to its half-width). This can be used with any number-like objects that support addition and\n",
      "        some form of subtraction (either direct or by addition with opposite).\n",
      "        \n",
      "        centre\n",
      "            Reference number against which we test for near-equality.\n",
      "        tol\n",
      "            Acceptable difference between arguments and the centre for the predicate to be satisfied.\n",
      "        bounds\n",
      "            Exclusivity of each implicit interval boundaries; see the docstring for `between` for further details.\n",
      "    \n",
      "    assert_(predicate: Callable[..., Union[bool, str, jupytest.Explanation]], *ops: Any, msg: str = '', **kwops: Any) -> None\n",
      "        Evalutes a condition over multiple objects, using as condition test either an explicit function predicate\n",
      "        or a bound method provided as first (left-most) operand. Raises an AssertionError exception if the condition\n",
      "        is not met (the condition test yields a result of False).\n",
      "        \n",
      "        cond\n",
      "            Predicate function (or bound method). It must take as input parameters. The function should return True\n",
      "            to signal satisfaction of the predicate. In the other case, it may return a False-equivalent value (in which\n",
      "            case the failure explanation is provided as the `msg` keyword argument), or a string corresponding to the\n",
      "            explanation, or an ExplanationFailure instance.\n",
      "        msg\n",
      "            Facultative explanation for the failure of satisfying the predicate. If provided, it will override the\n",
      "            explanation returned by the predicate, or at least its \"main expectation\" (see docstring of\n",
      "            `ExplanationFailure`).\n",
      "        ops, kwops\n",
      "            Operands to the predicate.\n",
      "    \n",
      "    belong_to(collection: Container) -> Callable[..., Union[bool, str, jupytest.Explanation]]\n",
      "        Predicate satisfied when all arguments are contained by the given collection, according to the `in'\n",
      "        operator.\n",
      "    \n",
      "    between(lower: Any, upper: Any, bounds: str = '[]') -> Callable[..., Union[bool, str, jupytest.Explanation]]\n",
      "        Returns a predicate that is satisfied if all its arguments fall within an interval. Comparands\n",
      "        must all support order comparison operators (<, >, <=, >=).\n",
      "        \n",
      "        lower\n",
      "            Lower bound of the interval.\n",
      "        upper\n",
      "            Upper bound of the interval.\n",
      "        bounds\n",
      "            Determines whether the interval is closed or open at either end (e.g. the boundary element is belongs or not\n",
      "            to the set formed by the interval). It is provided as a string whose first and last characters are observed.\n",
      "                First character:\n",
      "                    [    Interval is closed on the left.\n",
      "                    (    Interval is open on the left.\n",
      "                Last character:\n",
      "                    ]    Interval is closed on the right.\n",
      "                    )    Interval is open on the right.\n",
      "            Any other character in these positions raise an error.\n",
      "    \n",
      "    color(important: Union[jupytest.Emphasis, NoneType] = None, trivial: Union[jupytest.Emphasis, NoneType] = None, failure: Union[jupytest.Emphasis, NoneType] = None, error: Union[jupytest.Emphasis, NoneType] = None) -> jupytest.Colorizer\n",
      "    \n",
      "    detail_issues(suite: jupytest.Suite, file: io.TextIOBase = <ipykernel.iostream.OutStream object at 0x7f1fd030e4f0>, colorizer: jupytest.Colorizer = <jupytest.Colorizer object at 0x7f1fb7b6c5b0>, lines_context: int = 3, max_report: int = 9223372036854775807, on_error: jupytest.PolicyReportingProblems = <jupytest.PolicyReportingProblems object at 0x7f1fb7b45130>) -> None\n",
      "        Writes up a report detailing the issues encountered while running the test suite.\n",
      "        \n",
      "        suite\n",
      "            The test suite.\n",
      "        file\n",
      "            The file-like object to write the report to. Default is standard output.\n",
      "        colorizer\n",
      "            Color scheme used for emphasizing the various bits of the report.\n",
      "        lines_context\n",
      "            Number of lines of context to provide around each line of code involved\n",
      "            in a reported problem.\n",
      "        max_report\n",
      "            Maximum number of problems to report on.\n",
      "        on_error\n",
      "            What to do when reporting results that include problems such as failures and\n",
      "            errors. If set to RAISE, it will raise a ProblemsEncountered exception, which\n",
      "            is useful when running the notebook as part of a CI/CD pipeline; otherwise,\n",
      "            or if set to IGNORE, nothing more is done than writing the report.\n",
      "            Default is IGNORE.\n",
      "    \n",
      "    detail_result(name_test: str, result: jupytest.Error, prefix_header: str, file: io.TextIOBase = <ipykernel.iostream.OutStream object at 0x7f1fd030e4f0>, colorizer: jupytest.Colorizer = <jupytest.Colorizer object at 0x7f1fb7b6c3d0>, lines_context: int = 3, print_border: bool = True) -> None\n",
      "        Writes up a report regarding a single test result.\n",
      "        \n",
      "        name_test\n",
      "            Name of the test the result was gotten for.\n",
      "        result\n",
      "            Error-type result to report on.\n",
      "        prefix_header\n",
      "            String prepended to the header of the result report.\n",
      "        file\n",
      "            File-like object to write report to. Default is standard output.\n",
      "        colorizer\n",
      "            Policy for emphasizing the written report.\n",
      "        lines_context\n",
      "            Number of lines of code to fetch and write up before and after the\n",
      "            line associated to the stack frame.\n",
      "        print_border\n",
      "            If True (default), prints a border of dashes above the result report.\n",
      "    \n",
      "    diff(*args: Any, **kwargs: Any) -> Union[bool, str, jupytest.Explanation]\n",
      "        Difference predicate.\n",
      "    \n",
      "    eq(*args: Any, **kwargs: Any) -> Union[bool, str, jupytest.Explanation]\n",
      "        Exact equality predicate.\n",
      "    \n",
      "    fail(reason: str = '')\n",
      "        Marks some ongoing test as failed, with an optional reason for failure.\n",
      "    \n",
      "    is_any_of(*types: type) -> Callable[..., Union[bool, str, jupytest.Explanation]]\n",
      "        Predicate satisfied when all arguments are instances of one of the given types.\n",
      "    \n",
      "    join_args(args: Sequence[Any], kwargs: Mapping[str, Any]) -> Sequence[Tuple[Union[int, str], Any]]\n",
      "    \n",
      "    ladder(name_test: str, num_result: int) -> str\n",
      "        Test name formatter that puts out the name of a test only once, even\n",
      "        if it has been run multiple times.\n",
      "    \n",
      "    name_all(name_test: str, num_result: int) -> str\n",
      "        Test name formatter that puts out the name of a test even when it has\n",
      "        been run multiple times.\n",
      "    \n",
      "    plain() -> jupytest.Colorizer\n",
      "    \n",
      "    print_frame(frame: jupytest.Frame, file: io.TextIOBase = <ipykernel.iostream.OutStream object at 0x7f1fd030e4f0>, colorizer: jupytest.Colorizer = <jupytest.Colorizer object at 0x7f1fb7b6c1f0>, lines_context: int = 3) -> None\n",
      "        Writes up a single stack frame report.\n",
      "        \n",
      "        frame\n",
      "            Stack frame to report on.\n",
      "        file\n",
      "            File-like object to write report to. Default is standard output.\n",
      "        colorizer\n",
      "            Policy for emphasizing the written report.\n",
      "        lines_context\n",
      "            Number of lines of code to fetch and write up before and after the\n",
      "            line associated to the stack frame.\n",
      "    \n",
      "    quoter(formatter: Callable[[str, int], str]) -> Callable[[str, int], str]\n",
      "        Test name formatter that surrounds the name between double quotes. Not\n",
      "        meant to be used directly by users of this module.\n",
      "    \n",
      "    raise_on_error(suite: jupytest.Suite) -> None\n",
      "    \n",
      "    report_results(suite: jupytest.Suite, file: io.TextIOBase = <ipykernel.iostream.OutStream object at 0x7f1fd030e4f0>, colorizer: jupytest.Colorizer = <jupytest.Colorizer object at 0x7f1fb7befd30>, format_name_test: Callable[[str, int], str] = <function ladder at 0x7f1fb7b65310>, sep_name_result: str = '\\t', quote_names: bool = False, labels_result_custom: Mapping[type, str] = {}, on_error: jupytest.PolicyReportingProblems = <jupytest.PolicyReportingProblems object at 0x7f1fb7b45130>) -> None\n",
      "        Reports the name and result for each attempt at running a test, without details\n",
      "        as to issues encountered (failures and errors).\n",
      "        \n",
      "        suite\n",
      "            Suite of test to write report from.\n",
      "        file\n",
      "            File-like object to write report to. Default is standard output.\n",
      "        colorizer\n",
      "            Policy for emphasizing the written report.\n",
      "        format_name_test\n",
      "            Some tests are run more than once (for instance, for iterative problem solving).\n",
      "            In a report written for human reading, the repeated naming of a test run more\n",
      "            than once can feel redundant; it is eliminated by setting this to `ladder`.\n",
      "            All tests will be named if the formatter used instead is `name_all`.\n",
      "        sep_name_result\n",
      "            Separating character used between test name and result label. Default is \"      \".\n",
      "        quote_names\n",
      "            If True, the test names will be surrounded with double quotes in the output.\n",
      "        labels_result_custom\n",
      "            Dictionary of labels to use with different result types, when the default\n",
      "            labels (*ok* for success, *failed* for failure, *ERROR* for error) should be\n",
      "            changed. The emphasis for each label is derived from the colorizer.\n",
      "        on_error\n",
      "            What to do when reporting results that include problems such as failures and\n",
      "            errors. If set to RAISE, it will raise a ProblemsEncountered exception, which\n",
      "            is useful when running the notebook as part of a CI/CD pipeline; otherwise,\n",
      "            or if set to IGNORE, nothing more is done than writing the report.\n",
      "            Default is IGNORE.\n",
      "    \n",
      "    run_test_from_cell(cell: str) -> None\n",
      "        Executes the body of a cell, in context of the execution of a test.\n",
      "    \n",
      "    same(*args, **kwargs) -> Union[bool, str, jupytest.Explanation]\n",
      "        Predicate satisfied when all its arguments are the same (according to operator `is`).\n",
      "    \n",
      "    summarize_results(suite: jupytest.Suite, file: Union[io.TextIOBase, NoneType] = <ipykernel.iostream.OutStream object at 0x7f1fd030e4f0>, colorizer: jupytest.Colorizer = <jupytest.Colorizer object at 0x7f1fb7b6c040>, sep: str = ', ', on_error: jupytest.PolicyReportingProblems = <jupytest.PolicyReportingProblems object at 0x7f1fb7b45130>) -> Dict[type, int]\n",
      "        Writes a very short summary of a test run, counting the number of each result obtained.\n",
      "        \n",
      "        suite\n",
      "            Suite of test to write report from.\n",
      "        file\n",
      "            File-like object to write report to. Default is standard output.\n",
      "        colorizer\n",
      "            Policy for emphasizing the written report.\n",
      "        sep\n",
      "            Separation string between the labeled numbers of results. Default is \", \"\n",
      "        on_error\n",
      "            What to do when reporting results that include problems such as failures and\n",
      "            errors. If set to RAISE, it will raise a ProblemsEncountered exception, which\n",
      "            is useful when running the notebook as part of a CI/CD pipeline; otherwise,\n",
      "            or if set to IGNORE, nothing more is done than writing the report.\n",
      "            Default is IGNORE.\n",
      "    \n",
      "    test_cell(suite: jupytest.Suite, line: str, cell: Union[str, NoneType]) -> None\n",
      "        Runs a test written using a cell magic.\n",
      "\n",
      "DATA\n",
      "    Any = typing.Any\n",
      "    Arg = typing.Tuple[typing.Union[int, str], typing.Any]\n",
      "    Args = typing.Sequence[typing.Tuple[typing.Union[int, str], typing.Any...\n",
      "    Callable = typing.Callable\n",
      "    Comparator = typing.Callable[[typing.Any, typing.Any], bool]\n",
      "    Container = typing.Container\n",
      "    Dict = typing.Dict\n",
      "    ExplanationOnFailure = typing.Union[bool, str, jupytest.Explanation]\n",
      "    IGNORE = <jupytest.PolicyReportingProblems object>\n",
      "    Iterable = typing.Iterable\n",
      "    Iterator = typing.Iterator\n",
      "    List = typing.List\n",
      "    Mapping = typing.Mapping\n",
      "    NameArg = typing.Union[int, str]\n",
      "    Optional = typing.Optional\n",
      "    Predicate = typing.Callable[..., typing.Union[bool, str, jupytest.Expl...\n",
      "    RAISE = <jupytest.PolicyReportingProblems object>\n",
      "    Sequence = typing.Sequence\n",
      "    TestFunction = typing.Callable[..., NoneType]\n",
      "    TestNameFormatter = typing.Callable[[str, int], str]\n",
      "    Tuple = typing.Tuple\n",
      "    Union = typing.Union\n",
      "\n",
      "FILE\n",
      "    /data/code/jupytest/jupytest/__init__.py\n",
      "\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(jupytest))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
