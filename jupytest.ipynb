{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The following uses a kind of literate programming approach to build a library of tools useful for writing unit and integration tests directly into a notebook. The library is to be articulated as a Python package built as the concatenation of a subset of the code cells of this notebook, using an ad hoc script. To help with identifying which code cells are parts of the final package and which are inline testing code, we use *tags*, which make up cell metadata in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%flake8_on --max_line_length 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from copy import copy, deepcopy\n",
    "from inspect import getframeinfo, Traceback, unwrap\n",
    "from io import TextIOBase\n",
    "import itertools\n",
    "from linecache import getline\n",
    "import sys\n",
    "from traceback import walk_tb\n",
    "from typing import Dict, List, Tuple, Iterator, Union, Iterable, Optional, Any, Callable, Mapping, Sequence\n",
    "\n",
    "import colors\n",
    "from IPython import get_ipython\n",
    "from IPython.core.magic import register_cell_magic\n",
    "from pygments import highlight\n",
    "from pygments.lexers import Python3Lexer\n",
    "from pygments.formatters import TerminalFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Result(ABC):\n",
    "    \"\"\"\n",
    "    Result of a test. Indicates whether the test passed (was a success), and if it did not,\n",
    "    whether it was a failure (as opposed to any other kind of issue).\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def is_success(self) -> bool:\n",
    "        \"\"\"True when an associated test run has passed.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def is_failure(self) -> bool:\n",
    "        \"\"\"True when an associated has not passed because a designed failure condition was met.\"\"\"\n",
    "        return False\n",
    "\n",
    "    def as_dict(self) -> Dict:\n",
    "        \"\"\"Expresses this result as a dictionary suitable to structured data serialization.\"\"\"\n",
    "        return {\"type\": type(self).__name__}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual test failure\n",
    "\n",
    "Tests can be made to fail deliberately by raising a special exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class TestFailed(Exception):\n",
    "    \"\"\"\n",
    "    Exception raised by this framework in order to mark a test run as a Failure.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, reason: str) -> None:\n",
    "        super().__init__(reason)\n",
    "        self.reason = reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    raise TestFailed(\"asdf\")\n",
    "except TestFailed as err:\n",
    "    assert str(err) == \"asdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def fail(reason: str = \"\"):\n",
    "    \"Marks some ongoing test as failed, with an optional reason for failure.\"\n",
    "    raise TestFailed(reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    fail(\"asdf\")\n",
    "    assert False\n",
    "except TestFailed as err:\n",
    "    assert err.reason == \"asdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test result: success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Success(Result):\n",
    "    \"\"\"\n",
    "    Result for a test that passed.\n",
    "    \"\"\"\n",
    "    def is_success(self) -> bool:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert Success().is_success()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert Success().as_dict() == {\"type\": \"Success\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test result following the test code raising an exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traceback frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Frame:\n",
    "    \"\"\"\n",
    "    Information regarding a frame of a traceback. Provides more than the very limited\n",
    "    code context that comes from standard library introspection tools.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tb: Traceback, num_line: int, tags: Optional[List[str]] = None) -> None:\n",
    "        self.num_line = num_line\n",
    "        self.name_file = tb.filename\n",
    "        self.function = tb.function\n",
    "        self.tags = tags or []\n",
    "\n",
    "    def context(self, before: int = 3, after: int = 3) -> List[Tuple[int, str]]:\n",
    "        ctx = [(self.num_line, getline(self.name_file, self.num_line).rstrip())]\n",
    "        for delta in range(1, before + 1):\n",
    "            ctx.insert(0, (self.num_line - delta, getline(self.name_file, self.num_line - delta).rstrip()))\n",
    "        for delta in range(1, after + 1):\n",
    "            ctx.append((self.num_line + delta, getline(self.name_file, self.num_line + delta).rstrip()))\n",
    "\n",
    "        # Clean up context: remove line-ending blanks and blank lines top and bottom\n",
    "        # of the context blob.\n",
    "        while len(ctx) > 0:\n",
    "            for i in [0, -1]:\n",
    "                if len(ctx[i][1]) == 0:\n",
    "                    del ctx[i]\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return ctx\n",
    "\n",
    "    def as_dict(self, context_before: int = 3, context_after: int = 3) -> Dict:\n",
    "        return {\n",
    "            \"file\": self.name_file,\n",
    "            \"line\": self.num_line,\n",
    "            \"function\": self.function,\n",
    "            \"context\": [[i, line] for i, line in self.context(context_before, context_after)],\n",
    "            \"tags\": self.tags\n",
    "        }\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"File {self.name_file}, Line {self.num_line}, Function {self.function}\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return str(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import getfile\n",
    "\n",
    "def my_function():  # noqa\n",
    "    getfile(\"asdf\")  # Will raise a TypeError\n",
    "\n",
    "frames = []  # noqa\n",
    "try:\n",
    "    my_function()\n",
    "    assert False\n",
    "except TypeError:\n",
    "    _, _, tb = sys.exc_info()\n",
    "    for frame_raw, lineno in walk_tb(tb):\n",
    "        frame = Frame(getframeinfo(frame_raw), lineno)\n",
    "        assert frame.name_file == getfile(frame_raw)\n",
    "        assert frame.num_line == lineno\n",
    "        assert frame.function == frame_raw.f_code.co_name\n",
    "        assert frame.tags == []\n",
    "        frames.append(frame)\n",
    "\n",
    "assert len(frames) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = frames[1]\n",
    "assert frame.context(0, 0) == [(4, \"    getfile(\\\"asdf\\\")  # Will raise a TypeError\")]\n",
    "assert frame.context(1, 1) == [(3, \"def my_function():  # noqa\"), (4, \"    getfile(\\\"asdf\\\")  # Will raise a TypeError\")]\n",
    "assert frame.context(3, 3) == [\n",
    "    (1, \"from inspect import getfile\"),\n",
    "    (2, \"\"),\n",
    "    (3, \"def my_function():  # noqa\"),\n",
    "    (4, \"    getfile(\\\"asdf\\\")  # Will raise a TypeError\"),\n",
    "    (5, \"\"),\n",
    "    (6, \"frames = []  # noqa\"),\n",
    "    (7, \"try:\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert frame.context(45, 0) == list(zip(range(1, 4 + 1), [\n",
    "    \"from inspect import getfile\",\n",
    "    \"\",\n",
    "    \"def my_function():  # noqa\",\n",
    "    \"    getfile(\\\"asdf\\\")  # Will raise a TypeError\"\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert frame.context(0, 9000) == list(zip(range(4, 20 + 1), \"\"\"\\\n",
    "    getfile(\"asdf\")  # Will raise a TypeError\n",
    "\n",
    "frames = []  # noqa\n",
    "try:\n",
    "    my_function()\n",
    "    assert False\n",
    "except TypeError:\n",
    "    _, _, tb = sys.exc_info()\n",
    "    for frame_raw, lineno in walk_tb(tb):\n",
    "        frame = Frame(getframeinfo(frame_raw), lineno)\n",
    "        assert frame.name_file == getfile(frame_raw)\n",
    "        assert frame.num_line == lineno\n",
    "        assert frame.function == frame_raw.f_code.co_name\n",
    "        assert frame.tags == []\n",
    "        frames.append(frame)\n",
    "\n",
    "assert len(frames) == 3\\\n",
    "\"\"\".split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert frame.as_dict() == {\n",
    "    \"file\": getfile(my_function),\n",
    "    \"line\": 4,\n",
    "    \"function\": \"my_function\",\n",
    "    \"tags\": [],\n",
    "    \"context\": list(list(e) for e in zip(range(1, 7 + 1), \"\"\"\\\n",
    "from inspect import getfile\n",
    "\n",
    "def my_function():  # noqa\n",
    "    getfile(\"asdf\")  # Will raise a TypeError\n",
    "\n",
    "frames = []  # noqa\n",
    "try:\\\n",
    "\"\"\".split(\"\\n\")))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The exception-driven result: errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Error(Result):\n",
    "    \"\"\"\n",
    "    Non-passing test result due to an exception being raised.\n",
    "\n",
    "    It is passed a set of common functions: the presence of these functions in the\n",
    "    traceback of the exception are expected and normal, making their eventual\n",
    "    reporting redundant and sort of trivial. The frames corresponding to these functions\n",
    "    in the traceback summary kept by this object will be tagged as such.\n",
    "    \"\"\"\n",
    "    TAG_COMMON = \"common\"\n",
    "\n",
    "    def __init__(self, fns_common: Iterable[Callable]) -> None:\n",
    "        super().__init__()\n",
    "        self._type_exc: type\n",
    "        self._value_exc: Any\n",
    "        self._type_exc, self._value_exc, tb = sys.exc_info()\n",
    "        if tb is None:\n",
    "            raise RuntimeError(\"Can only instantiate this class when an exception has been raised.\")\n",
    "\n",
    "        codes_common = {unwrap(fn).__code__ for fn in fns_common}\n",
    "        self._traceback: List[Frame] = []\n",
    "        for frame_raw, num_line in walk_tb(tb):\n",
    "            tags = []\n",
    "            if frame_raw.f_code in codes_common:\n",
    "                tags.append(Error.TAG_COMMON)\n",
    "            self._traceback.append(Frame(getframeinfo(frame_raw), num_line, tags))\n",
    "\n",
    "    def is_success(self) -> bool:\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def type_exc(self) -> type:\n",
    "        \"\"\"Returns the type of the exception associated to this result.\"\"\"\n",
    "        return self._type_exc\n",
    "\n",
    "    @property\n",
    "    def value_exc(self) -> Any:\n",
    "        \"\"\"Returns the exception raised in association to this test result.\"\"\"\n",
    "        return self._value_exc\n",
    "\n",
    "    @property\n",
    "    def traceback(self) -> List[Frame]:\n",
    "        \"\"\"\n",
    "        Returns a summary of the stack trace associated to the exception that brought this test result.\n",
    "        \"\"\"\n",
    "        return self._traceback\n",
    "\n",
    "    def as_dict(self, context_before: int = 3, context_after: int = 3) -> Dict:\n",
    "        d = super().as_dict()\n",
    "        d.update(\n",
    "            {\n",
    "                \"type_exc\": self.type_exc.__name__,\n",
    "                \"value_exc\": str(self.value_exc),\n",
    "                \"traceback\": [frame.as_dict(context_before, context_after) for frame in self.traceback]\n",
    "            }\n",
    "        )\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import getfile\n",
    "\n",
    "\n",
    "def fn_raise():\n",
    "    raise RuntimeError()\n",
    "\n",
    "\n",
    "def caller():\n",
    "    fn_raise()\n",
    "\n",
    "\n",
    "try:\n",
    "    caller()\n",
    "    assert False\n",
    "except RuntimeError:\n",
    "    err: Error = Error([caller])\n",
    "    assert not err.is_success()\n",
    "    assert not err.is_failure()\n",
    "    assert err.type_exc == RuntimeError\n",
    "    assert isinstance(err.value_exc, RuntimeError)\n",
    "    assert len(err.traceback) == 3\n",
    "    assert [frame.function for frame in err.traceback] == [\"<module>\", \"caller\", \"fn_raise\"]\n",
    "    assert [frame.tags for frame in err.traceback] == [[], [Error.TAG_COMMON], []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This trick gets us a cell's \"file name\", given that the `__file__` constant is not defined\n",
    "# in Jupyter notebooks.\n",
    "import inspect\n",
    "def _asdf():  # noqa\n",
    "    pass\n",
    "filename = inspect.getfile(_asdf)  # noqa\n",
    "\n",
    "try:\n",
    "    raise RuntimeError()\n",
    "    assert False\n",
    "except RuntimeError:\n",
    "    assert {\n",
    "        \"type\": \"Error\",\n",
    "        \"type_exc\": \"RuntimeError\",\n",
    "        \"value_exc\": \"\",\n",
    "        \"traceback\": [\n",
    "            {\n",
    "                \"file\": filename,\n",
    "                \"line\": 9,\n",
    "                \"function\": \"<module>\",\n",
    "                \"tags\": [],\n",
    "                \"context\": [[9, \"    raise RuntimeError()\"]]\n",
    "            }\n",
    "        ]\n",
    "    } == Error([]).as_dict(0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliberate exception: failures\n",
    "\n",
    "For convenience's sake, we model `Failure`s as a subclass of `Error` to gain the exception breakdown functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Failure(Error):\n",
    "    \"\"\"\n",
    "    Test result stemming from a condition check that failed, or a test run marked\n",
    "    as a failure.\n",
    "    \"\"\"\n",
    "    def __init__(self, reason: str, fns_common: Iterable[Callable]):\n",
    "        super().__init__(fns_common)\n",
    "        self._reason = reason\n",
    "\n",
    "    @property\n",
    "    def reason(self) -> str:\n",
    "        \"Reason given by the programmer as to why the test failed.\"\n",
    "        return self._reason\n",
    "\n",
    "    def is_failure(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def as_dict(self, context_before: int = 3, context_after: int = 3) -> Dict:\n",
    "        d = super().as_dict(context_before, context_after)\n",
    "        d[\"reason\"] = self.reason\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert False\n",
    "except AssertionError:\n",
    "    err: Failure = Failure(\"asdf\", [])\n",
    "    assert not err.is_success()\n",
    "    assert err.is_failure()\n",
    "    assert err.type_exc == AssertionError\n",
    "    assert isinstance(err.value_exc, AssertionError)\n",
    "    assert isinstance(err.traceback, list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "def _asdf():  # noqa\n",
    "    pass\n",
    "filename = inspect.getfile(_asdf)  # noqa\n",
    "\n",
    "try:\n",
    "    assert False\n",
    "except AssertionError:\n",
    "    assert {\n",
    "        \"type\": \"Failure\",\n",
    "        \"type_exc\": \"AssertionError\",\n",
    "        \"value_exc\": \"\",\n",
    "        \"traceback\": [\n",
    "            {\n",
    "                \"file\": filename,\n",
    "                \"line\": 7,\n",
    "                \"function\": \"<module>\",\n",
    "                \"tags\": [],\n",
    "                \"context\": [[7, \"    assert False\"]]\n",
    "            }\n",
    "        ],\n",
    "        \"reason\": \"asdf\"\n",
    "    } == Failure(\"asdf\", []).as_dict(0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test suites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Subscriber(ABC):\n",
    "    \"\"\"\n",
    "    Object reacting to test results as they are generated by running tests.\n",
    "    \"\"\"\n",
    "\n",
    "    def on_add_to_suite(self, suite: \"Suite\") -> None:\n",
    "        pass\n",
    "\n",
    "    def on_result(self, name_test: str, result: Result) -> None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "TestFunction = Callable[..., None]\n",
    "\n",
    "\n",
    "class Suite:\n",
    "    \"\"\"\n",
    "    Suite of tests, gathering the result of multiple named test runs. Test code fragments\n",
    "    are named using the `test()` decorator, or leveraging it indirectly by registering\n",
    "    a shortcut cell magic.\n",
    "\n",
    "    Test suites gets added functionality through a publish/subscribe system. Subscriber are\n",
    "    special objects tied to the suite instance through its `|' (bit OR) operator. At the\n",
    "    moment, the only event broadcast to all subscribers is the generation of a new test\n",
    "    result (and its appending to the suite's log). For instance, the `Report` plug-in\n",
    "    supports the suite by giving immediate feedback on a test's results. Thus, to\n",
    "    instantiate a suite with this added feature, one would use code like\n",
    "\n",
    "    suite = Suite() | Report()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name_magic: str = \"test\") -> None:\n",
    "        self._tests: Dict[str, List[Result]] = {}\n",
    "        self._fns_common = [fail, self.test]\n",
    "        self._subscribers: List[Subscriber] = []\n",
    "\n",
    "    def test(\n",
    "        self,\n",
    "        fn: Optional[TestFunction] = None,\n",
    "        name: str = \"\",\n",
    "        args: Sequence[Any] = [],\n",
    "        kwargs: Mapping[str, Any] = {}\n",
    "    ) -> Union[Callable[[TestFunction], TestFunction], TestFunction]:\n",
    "        \"\"\"\n",
    "        Runs a test encoded into a function. Completing the function's execution counts as a\n",
    "        test success; tripped assertions and other exceptions generate some other Result;\n",
    "        and the test result is retained by this Suite instance.\n",
    "\n",
    "        This decorator can be used two ways. Without application, one can decorate a\n",
    "        test function without parameter:\n",
    "\n",
    "            @suite.test\n",
    "            def this_is_my_test():\n",
    "                # Test goes here!\n",
    "\n",
    "        The name of the test corresponds to that of the function. Applying the decorator can\n",
    "        supply arguments to the test function and override the name of the test.\n",
    "\n",
    "            @suite.test(name=\"My test, with spaces\", args=(3, 4))\n",
    "            def fn_test(a, b):\n",
    "                # Test goes here!\n",
    "\n",
    "        To run a test with multiple parameter sets, one may even call this function directly,\n",
    "        not as a decorator:\n",
    "\n",
    "            def fn_test(a, b):\n",
    "                # Test test test...\n",
    "\n",
    "            for a, b in [(2, 8), (3, 4)]:\n",
    "                suite.test(fn_test, name=f\"Test with {a}, {b}\", args=(a, b))\n",
    "\n",
    "        fn\n",
    "            Function that embodies the test code.\n",
    "        name\n",
    "            Name of the test; by default, the name of the function is used.\n",
    "        args\n",
    "            Positional arguments to pass to the function to run the test.\n",
    "        kwargs\n",
    "            Named arguments to pass to the function to run the test.\n",
    "        \"\"\"\n",
    "        if fn is None:\n",
    "            return lambda fn: self.test(fn, name=name, args=args, kwargs=kwargs)\n",
    "\n",
    "        try:\n",
    "            fn(*args, **kwargs)\n",
    "            result = Success()\n",
    "        except TestFailed as err:\n",
    "            result = Failure(err.reason or \"Test marked as failed.\", self._fns_common)\n",
    "        except AssertionError as err:\n",
    "            result = Failure(str(err) or \"Assertion failed.\", self._fns_common)\n",
    "        except BaseException:\n",
    "            result = Error(self._fns_common)\n",
    "\n",
    "        name_test = name\n",
    "        if not name_test:\n",
    "            name_test = fn.__name__\n",
    "            if args or kwargs:\n",
    "                str_args = \", \".join(\n",
    "                    [repr(str(a)) for a in args] +\n",
    "                    [f\"{k}={repr(str(v))}\" for k, v in kwargs.items()]\n",
    "                )\n",
    "                name_test += f\"({str_args})\"\n",
    "        self._tests.setdefault(name_test, []).append(result)\n",
    "        for subscriber in self._subscribers:\n",
    "            subscriber.on_result(name_test, result)\n",
    "\n",
    "        return fn\n",
    "\n",
    "    @property\n",
    "    def results(self) -> Iterator[Tuple[str, Iterator[Result]]]:\n",
    "        \"\"\"\n",
    "        Iterates through the gathered test results. For each named test, yields a tuple of\n",
    "        the name of the test and an iterator over each result gathered as the test has\n",
    "        been run.\n",
    "        \"\"\"\n",
    "        for name, test_results in self._tests.items():\n",
    "            yield name, iter(test_results)\n",
    "\n",
    "    def as_dict(self) -> Dict[str, List[Dict]]:\n",
    "        \"Provides a structured data representation suitable for data serialization and exportation.\"\n",
    "        return {name: [r.as_dict() for r in rez] for name, rez in self.results}\n",
    "\n",
    "    def __or__(self, subscriber: Subscriber) -> \"Suite\":\n",
    "        \"\"\"\n",
    "        Generates a clone of this suite instance, but with this subscriber subscribed to it.\n",
    "\n",
    "        The new suite will not share member data structures with `self`, but if `self` carries\n",
    "        test results already, the new suite will reference the same result objects -- we\n",
    "        assume that Result objects are immutable.\n",
    "        \"\"\"\n",
    "        suite_with_subscriber = Suite()\n",
    "        suite_with_subscriber._tests = copy(self._tests)  # Under assumption of results immutability.\n",
    "        suite_with_subscriber._subscribers = copy(self._subscribers)\n",
    "        suite_with_subscriber._subscribers.append(subscriber)\n",
    "        subscriber.on_add_to_suite(suite_with_subscriber)\n",
    "        return suite_with_subscriber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(Suite()._tests, dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = Suite()\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def succeeding():\n",
    "    assert True\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def failing_by_assert_terse():\n",
    "    assert False\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def failing_by_assert_reason():\n",
    "    assert False, \"assert reason\"\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def failing_manually_terse():\n",
    "    fail()\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def failing_manually_reason():\n",
    "    fail(\"fail reason\")\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def error():\n",
    "    raise RuntimeError(\"doh\")\n",
    "\n",
    "\n",
    "assert [\n",
    "    (\"succeeding\", [(Success, \"\")]),\n",
    "    (\"failing_by_assert_terse\", [(Failure, \"Assertion failed.\")]),\n",
    "    (\"failing_by_assert_reason\", [(Failure, \"assert reason\")]),\n",
    "    (\"failing_manually_terse\", [(Failure, \"Test marked as failed.\")]),\n",
    "    (\"failing_manually_reason\", [(Failure, \"fail reason\")]),\n",
    "    (\"error\", [(Error, \"\")])\n",
    "] == [(name, [(type(r), r.reason if hasattr(r, \"reason\") else \"\") for r in rez]) for name, rez in suite.results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check each non-success for tagging of the first frame.\n",
    "num_non_success = 0\n",
    "for name, rez in suite.results:\n",
    "    for r in rez:\n",
    "        if not r.is_success():\n",
    "            num_non_success += 1\n",
    "            assert Error.TAG_COMMON in r.traceback[0].tags\n",
    "\n",
    "assert num_non_success == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check failures for tagging of the fail call.\n",
    "num_failures = 0\n",
    "for name, rez in suite.results:\n",
    "    if \"failing_manually\" in name:\n",
    "        for r in rez:\n",
    "            num_failures += 1\n",
    "            assert Error.TAG_COMMON in r.traceback[-1].tags\n",
    "\n",
    "assert num_failures == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = Suite()\n",
    "\n",
    "\n",
    "@suite.test(name=\"trial\")\n",
    "def t1():\n",
    "    fail()\n",
    "\n",
    "\n",
    "@suite.test(name=\"trial\")\n",
    "def t2():\n",
    "    raise RuntimeError()\n",
    "\n",
    "\n",
    "@suite.test(name=\"trial\")\n",
    "def t3():\n",
    "    pass  # Literally!\n",
    "\n",
    "\n",
    "assert [(\"trial\", [Failure, Error, Success])] == [(name, [type(r) for r in rez]) for name, rez in suite.results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = Suite()\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def first():\n",
    "    fail()\n",
    "\n",
    "\n",
    "@suite.test(name=\"first\")\n",
    "def first2():\n",
    "    pass\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def second():\n",
    "    raise RuntimeError()\n",
    "\n",
    "\n",
    "assert {name: [r[\"type\"] for r in rez] for name, rez in suite.as_dict().items()} == {\n",
    "    \"first\": [\"Failure\", \"Success\"],\n",
    "    \"second\": [\"Error\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing environment protection during test execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"x\" not in globals()\n",
    "mylist = [1, 2, 3]\n",
    "\n",
    "\n",
    "suite = Suite()\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def trial():\n",
    "    x = 5\n",
    "    assert x == 5\n",
    "    mylist.append(4)\n",
    "\n",
    "\n",
    "assert \"x\" not in globals()\n",
    "assert [(\"trial\", [Success])] == [(name, [type(r) for r in rez]) for name, rez in suite.results]\n",
    "# Accidental globals are obviously not protected.\n",
    "assert [1, 2, 3, 4] == mylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ns in [globals(), locals()]:\n",
    "    assert \"C\" not in ns\n",
    "\n",
    "\n",
    "suite = Suite()\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def check_something_about_a_new_class():\n",
    "    class C:\n",
    "        def f(self):\n",
    "            return 5\n",
    "\n",
    "    assert C().f() == 5\n",
    "\n",
    "\n",
    "for ns in [globals(), locals()]:\n",
    "    assert \"C\" not in ns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing publish/subscribe of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9:1: W293 blank line contains whitespace\n"
     ]
    }
   ],
   "source": [
    "class TestSubscriber(Subscriber):\n",
    "\n",
    "    def __init__(self, lr: List[Tuple[str, Result]]):\n",
    "        self.suite = None\n",
    "        self._results: List[Tuple[str, Result]] = lr\n",
    "\n",
    "    def on_add_to_suite(self, suite: Suite) -> None:\n",
    "        self.suite = suite\n",
    "        \n",
    "    def on_result(self, name_test: str, result: Result) -> None:\n",
    "        self._results.append((name_test, result))\n",
    "\n",
    "\n",
    "results: List[Tuple[str, Result]] = []\n",
    "sub = TestSubscriber(results)\n",
    "assert sub.suite is None\n",
    "suite = Suite() | sub\n",
    "assert sub.suite is suite\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def passing():\n",
    "    pass\n",
    "\n",
    "\n",
    "assert len(results) == 1\n",
    "name_last, result_last = results[-1]\n",
    "assert name_last == \"passing\"\n",
    "assert isinstance(result_last, Success)\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def error():\n",
    "    raise RuntimeError(\"asdf\")\n",
    "\n",
    "\n",
    "assert len(results) == 2\n",
    "name_last, result_last = results[-1]\n",
    "assert name_last == \"error\"\n",
    "assert isinstance(result_last, Error)\n",
    "assert str(result_last.value_exc) == \"asdf\"\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def failing():\n",
    "    fail(\"There is no why\")\n",
    "\n",
    "\n",
    "assert len(results) == 3\n",
    "name_last, result_last = results[-1]\n",
    "assert name_last == \"failing\"\n",
    "assert isinstance(result_last, Failure)\n",
    "assert result_last.reason == \"There is no why\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating test run reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report colorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Emphasis(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, s: str) -> str:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Color(Emphasis):\n",
    "\n",
    "    def __init__(self, fg=None, bg=None, style=None) -> None:\n",
    "        super().__init__()\n",
    "        self._fg = fg\n",
    "        self._bg = bg\n",
    "        self._style = style\n",
    "\n",
    "    def __call__(self, s: str) -> str:\n",
    "        return colors.color(s, fg=self._fg, bg=self._bg, style=self._style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert Color(fg=\"red\", bg=\"blue\", style=\"bold\")(\"asdf\") == '\\x1b[31;44;1masdf\\x1b[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Plain(Emphasis):\n",
    "\n",
    "    def __call__(self, s: str) -> str:\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert Plain()(\"asdf\") == \"asdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Colorizer:\n",
    "\n",
    "    def __init__(self, important: Emphasis, trivial: Emphasis, failure: Emphasis, error: Emphasis) -> None:\n",
    "        self.important: Emphasis = important\n",
    "        self.trivial: Emphasis = trivial\n",
    "        self.failure: Emphasis = failure\n",
    "        self.error: Emphasis = error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def plain() -> Colorizer:\n",
    "    return Colorizer(Plain(), Plain(), Plain(), Plain())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = plain()\n",
    "assert c.important(\"asdf\") == \"asdf\"\n",
    "assert c.trivial(\"asdf\") == \"asdf\"\n",
    "assert c.failure(\"asdf\") == \"asdf\"\n",
    "assert c.error(\"asdf\") == \"asdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def color(  # noqa\n",
    "    important: Optional[Emphasis] = None,\n",
    "    trivial: Optional[Emphasis] = None,\n",
    "    failure: Optional[Emphasis] = None,\n",
    "    error: Optional[Emphasis] = None\n",
    ") -> Colorizer:\n",
    "    return Colorizer(\n",
    "        important or Color(style=\"bold\"),\n",
    "        trivial or Color(fg=\"white\"),\n",
    "        failure or Color(fg=\"yellow\"),\n",
    "        error or Color(fg=\"red\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = color(important=Color(fg=\"white\"), trivial=Color(fg=\"black\"), failure=Color(bg=\"blue\"), error=Color(fg=\"green\"))\n",
    "assert c.important(\"asdf\") == Color(fg=\"white\")(\"asdf\")\n",
    "assert c.trivial(\"asdf\") == Color(fg=\"black\")(\"asdf\")\n",
    "assert c.failure(\"asdf\") == Color(bg=\"blue\")(\"asdf\")\n",
    "assert c.error(\"asdf\") == Color(fg=\"green\")(\"asdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raising an exception when reporting failures and errors\n",
    "\n",
    "This can be useful when running all tests after a code modification, especially if running from within a CI system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class ProblemsEncountered(Exception):\n",
    "    \"\"\"Raised (optionally) when a reporting routine must report failures and errors.\"\"\"\n",
    "\n",
    "    def __init__(self, num_failures, num_errors):\n",
    "        plural_failure = \"s\" if num_failures > 1 else \"\"\n",
    "        plural_errors = \"s\" if num_errors > 1 else \"\"\n",
    "        super().__init__(\n",
    "            f\"Problems encountered during testing: {num_failures} failure{plural_failure}, \"\n",
    "            f\"{num_errors} error{plural_errors}\"\n",
    "        )\n",
    "        self.num_failures = num_failures\n",
    "        self.num_errors = num_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "try:\n",
    "    raise ProblemsEncountered(2, 2)\n",
    "except ProblemsEncountered as err:\n",
    "    assert re.search(\"2 failures, 2 errors\", str(err))\n",
    "\n",
    "try:\n",
    "    raise ProblemsEncountered(1, 0)\n",
    "except ProblemsEncountered as err:\n",
    "    assert re.search(\"1 failure, 0 error\", str(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def raise_on_error(suite: Suite) -> None:\n",
    "    num_failures = 0\n",
    "    num_errors = 0\n",
    "    for _, rez in suite.results:\n",
    "        for r in rez:\n",
    "            if not r.is_success():\n",
    "                if r.is_failure():\n",
    "                    num_failures += 1\n",
    "                else:\n",
    "                    num_errors += 1\n",
    "    if num_failures > 0 or num_errors > 0:\n",
    "        raise ProblemsEncountered(num_failures, num_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_suite_with_two_tests():\n",
    "    suite = Suite()\n",
    "    for name in [\"one\", \"two\"]:\n",
    "        @suite.test(name=f\"passing-{name}\", args=(name,))\n",
    "        def passing(a):\n",
    "            pass\n",
    "\n",
    "    return suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = make_suite_with_two_tests()\n",
    "\n",
    "try:\n",
    "    raise_on_error(suite)\n",
    "except ProblemsEncountered:\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = make_suite_with_two_tests()\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def failure():\n",
    "    fail()\n",
    "\n",
    "\n",
    "try:\n",
    "    raise_on_error(suite)\n",
    "    assert False\n",
    "except ProblemsEncountered as err:\n",
    "    assert err.num_failures == 1\n",
    "    assert err.num_errors == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = make_suite_with_two_tests()\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def error():\n",
    "    raise RuntimeError()\n",
    "\n",
    "\n",
    "try:\n",
    "    raise_on_error(suite)\n",
    "    assert False\n",
    "except ProblemsEncountered as err:\n",
    "    assert err.num_failures == 0\n",
    "    assert err.num_errors == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exhaustive report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "TestNameFormatter = Callable[[str, int], str]\n",
    "\n",
    "\n",
    "def name_all(name_test: str, num_result: int) -> str:\n",
    "    \"\"\"\n",
    "    Test name formatter that puts out the name of a test even when it has\n",
    "    been run multiple times.\n",
    "    \"\"\"\n",
    "    return name_test\n",
    "\n",
    "\n",
    "def ladder(name_test: str, num_result: int) -> str:\n",
    "    \"\"\"\n",
    "    Test name formatter that puts out the name of a test only once, even\n",
    "    if it has been run multiple times.\n",
    "    \"\"\"\n",
    "    if num_result == 0:\n",
    "        return name_test\n",
    "    return \" \" * len(name_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def quoter(formatter: TestNameFormatter) -> TestNameFormatter:\n",
    "    \"\"\"\n",
    "    Test name formatter that surrounds the name between double quotes. Not\n",
    "    meant to be used directly by users of this module.\n",
    "    \"\"\"\n",
    "    def quoter_format(name_test: str, num_result: int) -> str:\n",
    "        return f\"\\\"{formatter(name_test, num_result)}\\\"\"\n",
    "\n",
    "    return quoter_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class PolicyReportingProblems:\n",
    "    \"\"\"What to do when reporting test results that involve problems (failures and errors).\"\"\"\n",
    "    def __init__(self, label: str) -> None:\n",
    "        self.label = label\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.label\n",
    "\n",
    "    def __eq__(self, other: Any) -> bool:\n",
    "        if not isinstance(other, PolicyReportingProblems):\n",
    "            return False\n",
    "        return self.label == other.label\n",
    "\n",
    "\n",
    "IGNORE = PolicyReportingProblems(\"ignore\")\n",
    "RAISE = PolicyReportingProblems(\"raise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def report_results(  # noqa\n",
    "    suite: Suite,\n",
    "    file: TextIOBase = sys.stdout,\n",
    "    colorizer: Colorizer = color(),\n",
    "    format_name_test: TestNameFormatter = ladder,\n",
    "    sep_name_result: str = \"\\t\",\n",
    "    quote_names: bool = False,\n",
    "    labels_result_custom: Mapping[type, str] = {},\n",
    "    on_error: PolicyReportingProblems = IGNORE\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Reports the name and result for each attempt at running a test, without details\n",
    "    as to issues encountered (failures and errors).\n",
    "\n",
    "    suite\n",
    "        Suite of test to write report from.\n",
    "    file\n",
    "        File-like object to write report to. Default is standard output.\n",
    "    colorizer\n",
    "        Policy for emphasizing the written report.\n",
    "    format_name_test\n",
    "        Some tests are run more than once (for instance, for iterative problem solving).\n",
    "        In a report written for human reading, the repeated naming of a test run more\n",
    "        than once can feel redundant; it is eliminated by setting this to `ladder`.\n",
    "        All tests will be named if the formatter used instead is `name_all`.\n",
    "    sep_name_result\n",
    "        Separating character used between test name and result label. Default is \"\\t\".\n",
    "    quote_names\n",
    "        If True, the test names will be surrounded with double quotes in the output.\n",
    "    labels_result_custom\n",
    "        Dictionary of labels to use with different result types, when the default\n",
    "        labels (*ok* for success, *failed* for failure, *ERROR* for error) should be\n",
    "        changed. The emphasis for each label is derived from the colorizer.\n",
    "    on_error\n",
    "        What to do when reporting results that include problems such as failures and\n",
    "        errors. If set to RAISE, it will raise a ProblemsEncountered exception, which\n",
    "        is useful when running the notebook as part of a CI/CD pipeline; otherwise,\n",
    "        or if set to IGNORE, nothing more is done than writing the report.\n",
    "        Default is IGNORE.\n",
    "    \"\"\"\n",
    "    len_all_names = [len(name) for name, _ in suite.results]\n",
    "    if len(len_all_names) == 0:\n",
    "        return\n",
    "    len_name_largest = max(len_all_names)\n",
    "\n",
    "    labels_result = {\n",
    "        type_result: colorize(labels_result_custom.get(type_result, label_default))\n",
    "        for type_result, colorize, label_default in [\n",
    "            (Success, Plain(), \"ok\"),\n",
    "            (Failure, colorizer.failure, \"failed\"),\n",
    "            (Error, colorizer.error, \"ERROR\")\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    if quote_names:\n",
    "        format_name_test = quoter(format_name_test)\n",
    "\n",
    "    for name, rez in suite.results:\n",
    "        p_name = f\"{name:{len_name_largest}s}\"\n",
    "        for num, r in enumerate(rez):\n",
    "            print(format_name_test(p_name, num), labels_result[type(r)], sep=sep_name_result, file=file)\n",
    "\n",
    "    if on_error is RAISE:\n",
    "        raise_on_error(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = Suite()\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def first():\n",
    "    raise RuntimeError()\n",
    "\n",
    "\n",
    "@suite.test(name=\"first\")\n",
    "def first2():\n",
    "    pass\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def second():\n",
    "    fail()\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def third():\n",
    "    pass\n",
    "\n",
    "\n",
    "@suite.test\n",
    "def fourth():\n",
    "    assert False\n",
    "\n",
    "\n",
    "@suite.test(name=\"fourth\")\n",
    "def fourth2():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: tests `first` and `fourth` are run twice; `second` and `third` only once. The name of each test is written only once. The results are either `ok`, `failed` or `ERROR` and are emphasized as normal for success, yellow for failure and red for error. The results are also lined up cleanly into a second column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first \t\u001b[31mERROR\u001b[0m\n",
      "      \tok\n",
      "second\t\u001b[33mfailed\u001b[0m\n",
      "third \tok\n",
      "fourth\t\u001b[33mfailed\u001b[0m\n",
      "      \tok\n"
     ]
    }
   ],
   "source": [
    "report_results(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as above, but with a `RAISE` policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first \t\u001b[31mERROR\u001b[0m\n",
      "      \tok\n",
      "second\t\u001b[33mfailed\u001b[0m\n",
      "third \tok\n",
      "fourth\t\u001b[33mfailed\u001b[0m\n",
      "      \tok\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    report_results(suite, on_error=RAISE)\n",
    "    assert False\n",
    "except ProblemsEncountered as err:\n",
    "    assert err.num_failures == 2\n",
    "    assert err.num_errors == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: like previous, except that the test names are written on each line a result is reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first \t\u001b[31mERROR\u001b[0m\n",
      "first \tok\n",
      "second\t\u001b[33mfailed\u001b[0m\n",
      "third \tok\n",
      "fourth\t\u001b[33mfailed\u001b[0m\n",
      "fourth\tok\n"
     ]
    }
   ],
   "source": [
    "report_results(suite, format_name_test=name_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: like previous, but CSV-like, with test names quoted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"first \",ERROR\n",
      "\"first \",ok\n",
      "\"second\",failed\n",
      "\"third \",ok\n",
      "\"fourth\",failed\n",
      "\"fourth\",ok\n"
     ]
    }
   ],
   "source": [
    "report_results(suite, format_name_test=name_all, quote_names=True, sep_name_result=\",\", colorizer=plain())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite_all_passed = Suite()\n",
    "\n",
    "\n",
    "@suite_all_passed.test\n",
    "def first():\n",
    "    pass\n",
    "\n",
    "\n",
    "@suite_all_passed.test\n",
    "def second():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: two results, both `ok`. No exception raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first \tok\n",
      "second\tok\n"
     ]
    }
   ],
   "source": [
    "report_results(suite_all_passed, on_error=RAISE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite_empty = Suite()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: nothing written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_results(suite_empty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test result summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def summarize_results(  # noqa\n",
    "    suite: Suite,\n",
    "    file: Optional[TextIOBase] = sys.stdout,\n",
    "    colorizer: Colorizer = color(),\n",
    "    sep: str = \", \",\n",
    "    on_error: PolicyReportingProblems = IGNORE\n",
    ") -> Dict[type, int]:\n",
    "    \"\"\"\n",
    "    Writes a very short summary of a test run, counting the number of each result obtained.\n",
    "\n",
    "    suite\n",
    "        Suite of test to write report from.\n",
    "    file\n",
    "        File-like object to write report to. Default is standard output.\n",
    "    colorizer\n",
    "        Policy for emphasizing the written report.\n",
    "    sep\n",
    "        Separation string between the labeled numbers of results. Default is \", \"\n",
    "    on_error\n",
    "        What to do when reporting results that include problems such as failures and\n",
    "        errors. If set to RAISE, it will raise a ProblemsEncountered exception, which\n",
    "        is useful when running the notebook as part of a CI/CD pipeline; otherwise,\n",
    "        or if set to IGNORE, nothing more is done than writing the report.\n",
    "        Default is IGNORE.\n",
    "    \"\"\"\n",
    "    summary = {t: 0 for t in [Success, Failure, Error]}\n",
    "    for _, rez in suite.results:\n",
    "        for r in rez:\n",
    "            summary[type(r)] += 1\n",
    "\n",
    "    if file is not None:\n",
    "        print(\n",
    "            f\"{summary[Success]} passed\",\n",
    "            (colorizer.failure if summary[Failure] > 0 else colorizer.trivial)(f\"{summary[Failure]} failed\"),\n",
    "            (colorizer.error if summary[Error] > 0 else colorizer.trivial)(f\"{summary[Error]} raised an error\"),\n",
    "            file=file,\n",
    "            sep=sep\n",
    "        )\n",
    "\n",
    "    if on_error == RAISE:\n",
    "        raise_on_error(suite)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: should show 3 tests passed, 2 failures (in yellow), 1 error (in red)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 passed, \u001b[33m2 failed\u001b[0m, \u001b[31m1 raised an error\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{__main__.Success: 3, __main__.Failure: 2, __main__.Error: 1}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_results(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same, but raising an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 passed, \u001b[33m2 failed\u001b[0m, \u001b[31m1 raised an error\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    summarize_results(suite, on_error=RAISE)\n",
    "    assert False\n",
    "except ProblemsEncountered as err:\n",
    "    assert err.num_failures == 2\n",
    "    assert err.num_errors == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check visually that nothing is written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert summarize_results(suite, file=None) == {\n",
    "    Success: 3,\n",
    "    Failure: 2,\n",
    "    Error: 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite_all_passed = Suite()\n",
    "\n",
    "\n",
    "@suite_all_passed.test\n",
    "def first():\n",
    "    pass\n",
    "\n",
    "\n",
    "@suite_all_passed.test\n",
    "def second():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check visually that report is fine. Should show 2 tests passed, and failed and errors labeled in a subdued color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 passed, \u001b[37m0 failed\u001b[0m, \u001b[37m0 raised an error\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{__main__.Success: 2, __main__.Failure: 0, __main__.Error: 0}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_results(suite_all_passed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check visually that report is fine. Should show 0 for each type of test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 passed, \u001b[37m0 failed\u001b[0m, \u001b[37m0 raised an error\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{__main__.Success: 0, __main__.Failure: 0, __main__.Error: 0}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_results(suite_empty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed report of issues encountered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing one frame for a result's associated traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def print_frame(  # noqa\n",
    "    frame: Frame,\n",
    "    file: TextIOBase = sys.stdout,\n",
    "    colorizer: Colorizer = color(),\n",
    "    lines_context: int = 3\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Writes up a single stack frame report.\n",
    "\n",
    "    frame\n",
    "        Stack frame to report on.\n",
    "    file\n",
    "        File-like object to write report to. Default is standard output.\n",
    "    colorizer\n",
    "        Policy for emphasizing the written report.\n",
    "    lines_context\n",
    "        Number of lines of code to fetch and write up before and after the\n",
    "        line associated to the stack frame.\n",
    "    \"\"\"\n",
    "    header = (\n",
    "        colorizer.trivial\n",
    "        if Error.TAG_COMMON in frame.tags\n",
    "        else Plain()\n",
    "    )(\n",
    "        \" | \".join([\n",
    "            \"Code cell\" if \"cell\" == frame.name_file else f\"File {frame.name_file}\",\n",
    "            f\"Line {frame.num_line}\",\n",
    "            f\"Function {frame.function}\"\n",
    "        ])\n",
    "    )\n",
    "    print(header, file=file)\n",
    "    if Error.TAG_COMMON not in frame.tags:\n",
    "        context: List[Tuple[int, str]] = frame.context(before=lines_context, after=lines_context)\n",
    "        if len(context) > 0:\n",
    "            max_len_num_line = len(str(context[-1][0]))\n",
    "            for i, line in zip(\n",
    "                [i for i, _ in context],\n",
    "                highlight(\n",
    "                    \"\\n\".join(ln for _, ln in context),\n",
    "                    lexer=Python3Lexer(),\n",
    "                    formatter=TerminalFormatter()\n",
    "                ).split(\"\\n\")\n",
    "            ):\n",
    "                print(\n",
    "                    colorizer.trivial(f\"{i:{max_len_num_line}d}\"),\n",
    "                    colorizer.trivial(\"|\"),\n",
    "                    line,\n",
    "                    sep=\" \",\n",
    "                    file=file\n",
    "                )\n",
    "    print(file=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: show show 3 frame reports. First and third must have appropriate code context (check against the line number), three lines of context around target line max. Second frame report should have subdued color and no code context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File <ipython-input-72-f4f6026f3037> | Line 10 | Function <module>\n",
      "\u001b[37m 9\u001b[0m \u001b[37m|\u001b[0m \u001b[34mtry\u001b[39;49;00m:\n",
      "\u001b[37m10\u001b[0m \u001b[37m|\u001b[0m     caller()\n",
      "\u001b[37m11\u001b[0m \u001b[37m|\u001b[0m     \u001b[34massert\u001b[39;49;00m \u001b[34mFalse\u001b[39;49;00m\n",
      "\u001b[37m12\u001b[0m \u001b[37m|\u001b[0m \u001b[34mexcept\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m:\n",
      "\u001b[37m13\u001b[0m \u001b[37m|\u001b[0m     _, _, tb = sys.exc_info()\n",
      "\n",
      "\u001b[37mFile <ipython-input-72-f4f6026f3037> | Line 6 | Function caller\u001b[0m\n",
      "\n",
      "File <ipython-input-72-f4f6026f3037> | Line 2 | Function raiser\n",
      "\u001b[37m1\u001b[0m \u001b[37m|\u001b[0m \u001b[34mdef\u001b[39;49;00m \u001b[32mraiser\u001b[39;49;00m():\n",
      "\u001b[37m2\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m()\n",
      "\u001b[37m3\u001b[0m \u001b[37m|\u001b[0m \n",
      "\u001b[37m4\u001b[0m \u001b[37m|\u001b[0m \n",
      "\u001b[37m5\u001b[0m \u001b[37m|\u001b[0m \u001b[34mdef\u001b[39;49;00m \u001b[32mcaller\u001b[39;49;00m():\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def raiser():\n",
    "    raise RuntimeError()\n",
    "\n",
    "\n",
    "def caller():\n",
    "    raiser()\n",
    "\n",
    "\n",
    "try:\n",
    "    caller()\n",
    "    assert False\n",
    "except RuntimeError:\n",
    "    _, _, tb = sys.exc_info()\n",
    "    for frame, lineno in walk_tb(tb):\n",
    "        print_frame(\n",
    "            Frame(\n",
    "                getframeinfo(frame),\n",
    "                lineno,\n",
    "                [Error.TAG_COMMON] if frame.f_code is caller.__code__ else [])\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailing one result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def detail_result(  # noqa\n",
    "    name_test: str,\n",
    "    result: Error,\n",
    "    prefix_header: str,\n",
    "    file: TextIOBase = sys.stdout,\n",
    "    colorizer: Colorizer = color(),\n",
    "    lines_context: int = 3\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Writes up a report regarding a single test result.\n",
    "\n",
    "    name_test\n",
    "        Name of the test the result was gotten for.\n",
    "    result\n",
    "        Error-type result to report on.\n",
    "    prefix_header\n",
    "        String prepended to the header of the result report.\n",
    "    file\n",
    "        File-like object to write report to. Default is standard output.\n",
    "    colorizer\n",
    "        Policy for emphasizing the written report.\n",
    "    lines_context\n",
    "        Number of lines of code to fetch and write up before and after the\n",
    "        line associated to the stack frame.\n",
    "    \"\"\"\n",
    "    header = \" ** \".join([\n",
    "        prefix_header,\n",
    "        f\"Test {colorizer.important(name_test)}\",\n",
    "        {Failure: colorizer.failure, Error: colorizer.error}[type(result)](type(result).__name__)\n",
    "    ])\n",
    "    print(\"-\" * len(colors.strip_color(header)), file=file)\n",
    "    print(header, file=file)\n",
    "    if result.is_failure():\n",
    "        print(result.reason, file=file)\n",
    "    else:\n",
    "        print(f\"{result.type_exc.__name__}:\", str(result.value_exc) or \"<no detail provided>\", file=file)\n",
    "    print(file=file)\n",
    "\n",
    "    for frame in result.traceback:  # First frame is always Suite.test, which is irrelevant.\n",
    "        print_frame(frame, file=file, colorizer=colorizer, lines_context=lines_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: header should start with first word **HEY**, label the test as **second** and map it as a *Failure*. Its frame report shows two subdued frames without code context, sandwiching a frame showing the call to the `fail()` function (line 16) that tripped the failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "HEY ** Test \u001b[1msecond\u001b[0m ** \u001b[33mFailure\u001b[0m\n",
      "Test marked as failed.\n",
      "\n",
      "\u001b[37mFile <ipython-input-24-4027f2cde817> | Line 73 | Function test\u001b[0m\n",
      "\n",
      "File <ipython-input-55-56d79be7ffb4> | Line 16 | Function second\n",
      "\u001b[37m14\u001b[0m \u001b[37m|\u001b[0m \u001b[90m@suite\u001b[39;49;00m.test\n",
      "\u001b[37m15\u001b[0m \u001b[37m|\u001b[0m \u001b[34mdef\u001b[39;49;00m \u001b[32msecond\u001b[39;49;00m():\n",
      "\u001b[37m16\u001b[0m \u001b[37m|\u001b[0m     fail()\n",
      "\u001b[37m17\u001b[0m \u001b[37m|\u001b[0m \n",
      "\u001b[37m18\u001b[0m \u001b[37m|\u001b[0m \n",
      "\u001b[37m19\u001b[0m \u001b[37m|\u001b[0m \u001b[90m@suite\u001b[39;49;00m.test\n",
      "\n",
      "\u001b[37mFile <ipython-input-6-f0d8945e4a67> | Line 3 | Function fail\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name, rez = list(suite.results)[1]\n",
    "failure = list(rez)[0]\n",
    "assert isinstance(failure, Failure)\n",
    "detail_result(name, failure, \"HEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def detail_issues(  # noqa\n",
    "    suite: Suite,\n",
    "    file: TextIOBase = sys.stdout,\n",
    "    colorizer: Colorizer = color(),\n",
    "    lines_context: int = 3,\n",
    "    max_report: int = sys.maxsize,\n",
    "    on_error: PolicyReportingProblems = IGNORE\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Writes up a report detailing the issues encountered while running the test suite.\n",
    "\n",
    "    suite\n",
    "        The test suite.\n",
    "    file\n",
    "        The file-like object to write the report to. Default is standard output.\n",
    "    colorizer\n",
    "        Color scheme used for emphasizing the various bits of the report.\n",
    "    lines_context\n",
    "        Number of lines of context to provide around each line of code involved\n",
    "        in a reported problem.\n",
    "    max_report\n",
    "        Maximum number of problems to report on.\n",
    "    on_error\n",
    "        What to do when reporting results that include problems such as failures and\n",
    "        errors. If set to RAISE, it will raise a ProblemsEncountered exception, which\n",
    "        is useful when running the notebook as part of a CI/CD pipeline; otherwise,\n",
    "        or if set to IGNORE, nothing more is done than writing the report.\n",
    "        Default is IGNORE.\n",
    "    \"\"\"\n",
    "    summary = summarize_results(suite, file=None)\n",
    "    num_problems = summary[Failure] + summary[Error]\n",
    "    num_remaining: int = -1\n",
    "    if num_problems == 0:\n",
    "        if summary[Success] == 0:\n",
    "            print(\"No test run.\", file=file)\n",
    "        else:\n",
    "            print(f\"All {summary[Success]} tests passed. No failure nor error encountered.\", file=file)\n",
    "    else:\n",
    "        index = 1\n",
    "        for name, rez in suite.results:\n",
    "            if num_remaining < 0:\n",
    "                for r in rez:\n",
    "                    if not r.is_success():\n",
    "                        detail_result(name, r, f\"# {index}/{num_problems}\", lines_context=lines_context, file=file)\n",
    "                        print()\n",
    "\n",
    "                        if index >= max_report:\n",
    "                            num_remaining = num_problems - index\n",
    "                            break\n",
    "                        index += 1\n",
    "\n",
    "    if num_remaining > 0:\n",
    "        print(\n",
    "            colorizer.important(\n",
    "                f\"... plus {num_remaining} other issue{'s' if num_remaining > 1 else ''}.\"\n",
    "            ),\n",
    "            file=file\n",
    "        )\n",
    "    if on_error == RAISE:\n",
    "        raise_on_error(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: 3 problem reports expected. First one is for an undetailed error (`RuntimeError`). Second and third are for failures. The first frame report for each (...` | Function test`) is in a subdued color, as is the frame for the call to function `fail`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "# 1/3 ** Test \u001b[1mfirst\u001b[0m ** \u001b[31mError\u001b[0m\n",
      "RuntimeError: <no detail provided>\n",
      "\n",
      "\u001b[37mFile <ipython-input-24-4027f2cde817> | Line 73 | Function test\u001b[0m\n",
      "\n",
      "File <ipython-input-55-56d79be7ffb4> | Line 6 | Function first\n",
      "\u001b[37m4\u001b[0m \u001b[37m|\u001b[0m \u001b[90m@suite\u001b[39;49;00m.test\n",
      "\u001b[37m5\u001b[0m \u001b[37m|\u001b[0m \u001b[34mdef\u001b[39;49;00m \u001b[32mfirst\u001b[39;49;00m():\n",
      "\u001b[37m6\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m()\n",
      "\u001b[37m7\u001b[0m \u001b[37m|\u001b[0m \n",
      "\u001b[37m8\u001b[0m \u001b[37m|\u001b[0m \n",
      "\u001b[37m9\u001b[0m \u001b[37m|\u001b[0m \u001b[90m@suite\u001b[39;49;00m.test(name=\u001b[33m\"\u001b[39;49;00m\u001b[33mfirst\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "-------------------------------\n",
      "# 2/3 ** Test \u001b[1msecond\u001b[0m ** \u001b[33mFailure\u001b[0m\n",
      "Test marked as failed.\n",
      "\n",
      "\u001b[37mFile <ipython-input-24-4027f2cde817> | Line 73 | Function test\u001b[0m\n",
      "\n",
      "File <ipython-input-55-56d79be7ffb4> | Line 16 | Function second\n",
      "\u001b[37m14\u001b[0m \u001b[37m|\u001b[0m \u001b[90m@suite\u001b[39;49;00m.test\n",
      "\u001b[37m15\u001b[0m \u001b[37m|\u001b[0m \u001b[34mdef\u001b[39;49;00m \u001b[32msecond\u001b[39;49;00m():\n",
      "\u001b[37m16\u001b[0m \u001b[37m|\u001b[0m     fail()\n",
      "\u001b[37m17\u001b[0m \u001b[37m|\u001b[0m \n",
      "\u001b[37m18\u001b[0m \u001b[37m|\u001b[0m \n",
      "\u001b[37m19\u001b[0m \u001b[37m|\u001b[0m \u001b[90m@suite\u001b[39;49;00m.test\n",
      "\n",
      "\u001b[37mFile <ipython-input-6-f0d8945e4a67> | Line 3 | Function fail\u001b[0m\n",
      "\n",
      "\n",
      "-------------------------------\n",
      "# 3/3 ** Test \u001b[1mfourth\u001b[0m ** \u001b[33mFailure\u001b[0m\n",
      "Assertion failed.\n",
      "\n",
      "\u001b[37mFile <ipython-input-24-4027f2cde817> | Line 73 | Function test\u001b[0m\n",
      "\n",
      "File <ipython-input-55-56d79be7ffb4> | Line 26 | Function fourth\n",
      "\u001b[37m24\u001b[0m \u001b[37m|\u001b[0m \u001b[90m@suite\u001b[39;49;00m.test\n",
      "\u001b[37m25\u001b[0m \u001b[37m|\u001b[0m \u001b[34mdef\u001b[39;49;00m \u001b[32mfourth\u001b[39;49;00m():\n",
      "\u001b[37m26\u001b[0m \u001b[37m|\u001b[0m     \u001b[34massert\u001b[39;49;00m \u001b[34mFalse\u001b[39;49;00m\n",
      "\u001b[37m27\u001b[0m \u001b[37m|\u001b[0m \n",
      "\u001b[37m28\u001b[0m \u001b[37m|\u001b[0m \n",
      "\u001b[37m29\u001b[0m \u001b[37m|\u001b[0m \u001b[90m@suite\u001b[39;49;00m.test(name=\u001b[33m\"\u001b[39;49;00m\u001b[33mfourth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "detail_issues(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: show only the report for the error, and mention that two more problems have yet to be reported. Also, code context is only one line around the target line, making for a 3-line code blurb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "# 1/3 ** Test \u001b[1mfirst\u001b[0m ** \u001b[31mError\u001b[0m\n",
      "RuntimeError: <no detail provided>\n",
      "\n",
      "\u001b[37mFile <ipython-input-24-4027f2cde817> | Line 73 | Function test\u001b[0m\n",
      "\n",
      "File <ipython-input-55-56d79be7ffb4> | Line 6 | Function first\n",
      "\u001b[37m5\u001b[0m \u001b[37m|\u001b[0m \u001b[34mdef\u001b[39;49;00m \u001b[32mfirst\u001b[39;49;00m():\n",
      "\u001b[37m6\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m()\n",
      "\n",
      "\n",
      "\u001b[1m... plus 2 other issues.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "detail_issues(suite, max_report=1, lines_context=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same, but with exception raising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "# 1/3 ** Test \u001b[1mfirst\u001b[0m ** \u001b[31mError\u001b[0m\n",
      "RuntimeError: <no detail provided>\n",
      "\n",
      "\u001b[37mFile <ipython-input-24-4027f2cde817> | Line 73 | Function test\u001b[0m\n",
      "\n",
      "File <ipython-input-55-56d79be7ffb4> | Line 6 | Function first\n",
      "\u001b[37m5\u001b[0m \u001b[37m|\u001b[0m \u001b[34mdef\u001b[39;49;00m \u001b[32mfirst\u001b[39;49;00m():\n",
      "\u001b[37m6\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m()\n",
      "\n",
      "\n",
      "\u001b[1m... plus 2 other issues.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    detail_issues(suite, max_report=1, lines_context=1, on_error=RAISE)\n",
    "    assert False\n",
    "except ProblemsEncountered as err:\n",
    "    assert err.num_failures == 2\n",
    "    assert err.num_errors == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: now show the error and the first failure, with 0 code context (so only the target line is shown). The final message about remaining issues is singular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "# 1/3 ** Test \u001b[1mfirst\u001b[0m ** \u001b[31mError\u001b[0m\n",
      "RuntimeError: <no detail provided>\n",
      "\n",
      "\u001b[37mFile <ipython-input-24-4027f2cde817> | Line 73 | Function test\u001b[0m\n",
      "\n",
      "File <ipython-input-55-56d79be7ffb4> | Line 6 | Function first\n",
      "\u001b[37m6\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m()\n",
      "\n",
      "\n",
      "-------------------------------\n",
      "# 2/3 ** Test \u001b[1msecond\u001b[0m ** \u001b[33mFailure\u001b[0m\n",
      "Test marked as failed.\n",
      "\n",
      "\u001b[37mFile <ipython-input-24-4027f2cde817> | Line 73 | Function test\u001b[0m\n",
      "\n",
      "File <ipython-input-55-56d79be7ffb4> | Line 16 | Function second\n",
      "\u001b[37m16\u001b[0m \u001b[37m|\u001b[0m     fail()\n",
      "\n",
      "\u001b[37mFile <ipython-input-6-f0d8945e4a67> | Line 3 | Function fail\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m... plus 1 other issue.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "detail_issues(suite, max_report=2, lines_context=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: two tests passed, no failure nor error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 2 tests passed. No failure nor error encountered.\n"
     ]
    }
   ],
   "source": [
    "detail_issues(suite_all_passed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: no test has been run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No test run.\n"
     ]
    }
   ],
   "source": [
    "detail_issues(suite_empty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On-the-fly result reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Report(Subscriber):\n",
    "    \"\"\"\n",
    "    Test suite subscriber that reports on test results on-the-fly. When a test\n",
    "    does not succeed, details on the failure can optionally be provided. See\n",
    "    documentation on class `Suite` to get an example on the usage of this\n",
    "    plug-in.\n",
    "\n",
    "    file\n",
    "        File-like object where the test results are reported. Default is\n",
    "        standard output.\n",
    "    verbose\n",
    "        If True, the feedback on test results contains traceback information\n",
    "        when problems are encountered.\n",
    "    file\n",
    "        File-like object where the feedback is put out.\n",
    "    colorizer\n",
    "        Policy on how to emphasize the feedback output.\n",
    "    lines_context\n",
    "        Number of lines of code to provide as context in traceback frames\n",
    "        around the line of code at the nexus of an issue.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        verbose: bool = True,\n",
    "        file: TextIOBase = sys.stdout,\n",
    "        colorizer: Colorizer = color(),\n",
    "        lines_context: int = 3\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._last: Optional[bool] = None\n",
    "        self._file = file\n",
    "        self._verbose = verbose\n",
    "        self._colorizer = colorizer\n",
    "        self._lines_context = lines_context\n",
    "\n",
    "    def on_result(self, name_test: str, result: Result) -> None:\n",
    "        name_test = self._colorizer.important(name_test)\n",
    "        if result.is_success():\n",
    "            msg = f\"Test {name_test} passed.\"\n",
    "            if self._verbose and self._last is False:\n",
    "                print(\"-\" * len(colors.strip_color(msg)), file=self._file)\n",
    "            self._last = True\n",
    "            print(msg, file=self._file)\n",
    "        else:\n",
    "            if self._last is True:\n",
    "                print(file=self._file)\n",
    "            self._last = False\n",
    "            if self._verbose:\n",
    "                detail_result(\n",
    "                    name_test,\n",
    "                    result,\n",
    "                    \"Issue encountered\",\n",
    "                    self._file,\n",
    "                    self._colorizer,\n",
    "                    self._lines_context\n",
    "                )\n",
    "            else:\n",
    "                index_frame_relevant = -1\n",
    "                if result.is_failure():\n",
    "                    label = self._colorizer.failure(f\"Test {name_test} failed\")\n",
    "                    print(f\"{label}: {result.reason}\", file=self._file)\n",
    "                    if isinstance(result.type_exc, TestFailed):\n",
    "                        index_frame_relevant = -2\n",
    "                else:\n",
    "                    label = self._colorizer.error(f\"Error occured during test {name_test}\")\n",
    "                    value_exc = \"\"\n",
    "                    if str(result.value_exc):\n",
    "                        value_exc = f\" -- {str(result.value_exc)}\"\n",
    "                    print(f\"{label}: {result.type_exc.__name__}{value_exc}\", file=self._file)\n",
    "                frame_relevant = result.traceback[index_frame_relevant]\n",
    "                print_frame(\n",
    "                    frame_relevant,\n",
    "                    file=self._file,\n",
    "                    colorizer=self._colorizer,\n",
    "                    lines_context=self._lines_context\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exercise_suite(reporter):\n",
    "    suite = Suite() | reporter\n",
    "\n",
    "    @suite.test\n",
    "    def first_passing():\n",
    "        pass\n",
    "\n",
    "    @suite.test\n",
    "    def second_passing():\n",
    "        pass\n",
    "\n",
    "    @suite.test\n",
    "    def failing():\n",
    "        assert 1 == 0, \"one vs. zero\"\n",
    "\n",
    "    @suite.test\n",
    "    def error_raising():\n",
    "        raise RuntimeError(\"Some error()\")\n",
    "\n",
    "    @suite.test\n",
    "    def next_to_last():\n",
    "        pass\n",
    "\n",
    "    @suite.test\n",
    "    def last():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: first and second tests pass, third test fails, fourth test raises an error, last two tests pass. Failure and error are tersely described, only the most relevant stack frame is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mfirst_passing\u001b[0m passed.\n",
      "Test \u001b[1msecond_passing\u001b[0m passed.\n",
      "\n",
      "\u001b[33mTest \u001b[1mfailing\u001b[0m failed\u001b[0m: one vs. zero\n",
      "File <ipython-input-83-04424abffded> | Line 14 | Function failing\n",
      "\u001b[37m12\u001b[0m \u001b[37m|\u001b[0m     \u001b[90m@suite\u001b[39;49;00m.test\n",
      "\u001b[37m13\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mdef\u001b[39;49;00m \u001b[32mfailing\u001b[39;49;00m():\n",
      "\u001b[37m14\u001b[0m \u001b[37m|\u001b[0m         \u001b[34massert\u001b[39;49;00m \u001b[34m1\u001b[39;49;00m == \u001b[34m0\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mone vs. zero\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[37m15\u001b[0m \u001b[37m|\u001b[0m \n",
      "\u001b[37m16\u001b[0m \u001b[37m|\u001b[0m     \u001b[90m@suite\u001b[39;49;00m.test\n",
      "\u001b[37m17\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mdef\u001b[39;49;00m \u001b[32merror_raising\u001b[39;49;00m():\n",
      "\n",
      "\u001b[31mError occured during test \u001b[1merror_raising\u001b[0m\u001b[0m: RuntimeError -- Some error()\n",
      "File <ipython-input-83-04424abffded> | Line 18 | Function error_raising\n",
      "\u001b[37m16\u001b[0m \u001b[37m|\u001b[0m     \u001b[90m@suite\u001b[39;49;00m.test\n",
      "\u001b[37m17\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mdef\u001b[39;49;00m \u001b[32merror_raising\u001b[39;49;00m():\n",
      "\u001b[37m18\u001b[0m \u001b[37m|\u001b[0m         \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mSome error()\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\u001b[37m19\u001b[0m \u001b[37m|\u001b[0m \n",
      "\u001b[37m20\u001b[0m \u001b[37m|\u001b[0m     \u001b[90m@suite\u001b[39;49;00m.test\n",
      "\u001b[37m21\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mdef\u001b[39;49;00m \u001b[32mnext_to_last\u001b[39;49;00m():\n",
      "\n",
      "Test \u001b[1mnext_to_last\u001b[0m passed.\n",
      "Test \u001b[1mlast\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "exercise_suite(Report(verbose=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: like previous, but with full detail of each non-success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mfirst_passing\u001b[0m passed.\n",
      "Test \u001b[1msecond_passing\u001b[0m passed.\n",
      "\n",
      "--------------------------------------------\n",
      "Issue encountered ** Test \u001b[1m\u001b[1mfailing\u001b[0m\u001b[0m ** \u001b[33mFailure\u001b[0m\n",
      "one vs. zero\n",
      "\n",
      "\u001b[37mFile <ipython-input-24-4027f2cde817> | Line 73 | Function test\u001b[0m\n",
      "\n",
      "File <ipython-input-83-04424abffded> | Line 14 | Function failing\n",
      "\u001b[37m12\u001b[0m \u001b[37m|\u001b[0m     \u001b[90m@suite\u001b[39;49;00m.test\n",
      "\u001b[37m13\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mdef\u001b[39;49;00m \u001b[32mfailing\u001b[39;49;00m():\n",
      "\u001b[37m14\u001b[0m \u001b[37m|\u001b[0m         \u001b[34massert\u001b[39;49;00m \u001b[34m1\u001b[39;49;00m == \u001b[34m0\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mone vs. zero\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[37m15\u001b[0m \u001b[37m|\u001b[0m \n",
      "\u001b[37m16\u001b[0m \u001b[37m|\u001b[0m     \u001b[90m@suite\u001b[39;49;00m.test\n",
      "\u001b[37m17\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mdef\u001b[39;49;00m \u001b[32merror_raising\u001b[39;49;00m():\n",
      "\n",
      "------------------------------------------------\n",
      "Issue encountered ** Test \u001b[1m\u001b[1merror_raising\u001b[0m\u001b[0m ** \u001b[31mError\u001b[0m\n",
      "RuntimeError: Some error()\n",
      "\n",
      "\u001b[37mFile <ipython-input-24-4027f2cde817> | Line 73 | Function test\u001b[0m\n",
      "\n",
      "File <ipython-input-83-04424abffded> | Line 18 | Function error_raising\n",
      "\u001b[37m16\u001b[0m \u001b[37m|\u001b[0m     \u001b[90m@suite\u001b[39;49;00m.test\n",
      "\u001b[37m17\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mdef\u001b[39;49;00m \u001b[32merror_raising\u001b[39;49;00m():\n",
      "\u001b[37m18\u001b[0m \u001b[37m|\u001b[0m         \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mSome error()\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\u001b[37m19\u001b[0m \u001b[37m|\u001b[0m \n",
      "\u001b[37m20\u001b[0m \u001b[37m|\u001b[0m     \u001b[90m@suite\u001b[39;49;00m.test\n",
      "\u001b[37m21\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mdef\u001b[39;49;00m \u001b[32mnext_to_last\u001b[39;49;00m():\n",
      "\n",
      "-------------------------\n",
      "Test \u001b[1mnext_to_last\u001b[0m passed.\n",
      "Test \u001b[1mlast\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "exercise_suite(Report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a cell magic to write tests\n",
    "\n",
    "The current decorator approach involves little boilerplate, but it can be reduced further nonetheless. Let's use a cell magic, in which all code written is wrapped into an ad hoc test function. Let's use a subscriber to register this cell magic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Magic(Subscriber):\n",
    "\n",
    "    def __init__(self, name_magic: str = \"test\") -> None:\n",
    "        self._name_magic = name_magic\n",
    "\n",
    "    def on_add_to_suite(self, suite):\n",
    "        suite._fns_common.append(run_test_from_cell)\n",
    "        ipython = get_ipython()\n",
    "        if ipython and self._name_magic:\n",
    "            register_cell_magic(self._name_magic)(lambda line, cell: test_cell(suite, line, cell))\n",
    "\n",
    "\n",
    "def test_cell(suite: Suite, line: str, cell: Optional[str]) -> None:\n",
    "    \"\"\"\n",
    "    Runs a test written using a cell magic.\n",
    "    \"\"\"\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        raise ValueError(\"Please provide a title for the test (right after the cell magic invocation).\")\n",
    "    cell = (cell or \"\").strip()\n",
    "    if not cell:\n",
    "        raise ValueError(\"There is no test to execute! Please write some code in there.\")\n",
    "\n",
    "    suite.test(run_test_from_cell, name=line, args=(cell,))\n",
    "\n",
    "\n",
    "def run_test_from_cell(cell: str) -> None:\n",
    "    \"\"\"\n",
    "    Executes the body of a cell, in context of the execution of a test.\n",
    "    \"\"\"\n",
    "    code_source = \"\\n\" + cell\n",
    "    ipython = get_ipython()\n",
    "    name_cell = ipython.compile.cache(code_source)\n",
    "    code = compile(code_source, name_cell, \"exec\")\n",
    "    exec(code, ipython.user_global_ns, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple usage of the magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = Suite() | Magic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%test First test, passing\n",
    "assert True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%test Second test, failing\n",
    "fail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%test Third test, raising an error\n",
    "raise RuntimeError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_function(x, y):\n",
    "    return x * y + x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%test function in my notebook\n",
    "assert my_function(4, 5) == 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "assert {name: [r[\"type\"] for r in rez] for name, rez in suite.as_dict().items()} == {\n",
    "    \"First test, passing\": [\"Success\"],\n",
    "    \"Second test, failing\": [\"Failure\"],\n",
    "    \"Third test, raising an error\": [\"Error\"],\n",
    "    \"function in my notebook\": [\"Success\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure even with the cell magic environment is not polluted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in [\"x\", \"f\", \"C\"]:\n",
    "    for ns in [globals(), locals(), get_ipython().user_ns]:\n",
    "        assert name not in ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%test Add x, f and C\n",
    "x = 5\n",
    "\n",
    "def f():\n",
    "    return 5\n",
    "\n",
    "\n",
    "class C():\n",
    "    \n",
    "    def f(self):\n",
    "        return 5\n",
    "    \n",
    "    \n",
    "assert f() == x\n",
    "assert C().f() == f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in [\"x\", \"f\", \"C\"]:\n",
    "    for ns in [globals(), locals(), get_ipython().user_ns]:\n",
    "        try:\n",
    "            assert name not in ns\n",
    "        except AssertionError:\n",
    "            print(f\"Name {name} still defined.\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailing one result, obtained by testing with cell magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite_magic = Suite() | Magic(\"test_magic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%test_magic the label\n",
    "x = 5\n",
    "assert x == 5\n",
    "fail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: header should start with first word **Head**, label the test as **the label** and map it as a *Failure*. Its frame report shows three subdued frames without code context, sandwiching a frame showing the call to the `fail()` function (line 16) that tripped the failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Head ** Test \u001b[1mthe label\u001b[0m ** \u001b[33mFailure\u001b[0m\n",
      "Test marked as failed.\n",
      "\n",
      "\u001b[37mFile <ipython-input-24-4027f2cde817> | Line 73 | Function test\u001b[0m\n",
      "\n",
      "\u001b[37mFile <ipython-input-86-b19f42f5c076> | Line 35 | Function run_test_from_cell\u001b[0m\n",
      "\n",
      "File <ipython-input-0-576533794e6c> | Line 4 | Function <module>\n",
      "\u001b[37m2\u001b[0m \u001b[37m|\u001b[0m x = \u001b[34m5\u001b[39;49;00m\n",
      "\u001b[37m3\u001b[0m \u001b[37m|\u001b[0m \u001b[34massert\u001b[39;49;00m x == \u001b[34m5\u001b[39;49;00m\n",
      "\u001b[37m4\u001b[0m \u001b[37m|\u001b[0m fail()\n",
      "\n",
      "\u001b[37mFile <ipython-input-6-f0d8945e4a67> | Line 3 | Function fail\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name, rez = list(suite_magic.results)[0]\n",
    "failure = list(rez)[0]\n",
    "assert isinstance(failure, Failure)\n",
    "detail_result(name, failure, \"Head\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation index\n",
    "\n",
    "Let's compose a docstring for the `jupytest` module (which we will export out of bits of this notebook). This docstring will act as a reference documentation index."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": [
     "header"
    ]
   },
   "source": [
    "Unit and integration testing in a notebook\n",
    "\n",
    "*** Building and running suites of tests ***\n",
    "\n",
    "class Suite\n",
    "    Method test (context manager)\n",
    "Function fail\n",
    "\n",
    "*** Reporting test results ***\n",
    "\n",
    "Function report_results\n",
    "Function summarize_results\n",
    "Function detail_issues\n",
    "Class Report (used as a subscriber plug-in to class Suite)\n",
    "\n",
    "*** Delving deeper into test results (going beyond the tools described above) ***\n",
    "\n",
    "Class Suite\n",
    "    Property results\n",
    "Class Result\n",
    "    Sub-class Success\n",
    "    Sub-class Error\n",
    "        Sub-class Failure\n",
    "    Method is_success\n",
    "    Method is_failure\n",
    "Class Frame\n",
    "\n",
    "*** Customizing result reporting ***\n",
    "\n",
    "Class Colorizer\n",
    "    Function plain\n",
    "    Function color\n",
    "Class Emphasis\n",
    "    Sub-class Plain\n",
    "    Sub-class Color\n",
    "Type TestNameFormatter\n",
    "    Function ladder\n",
    "    Function name_all\n",
    "    Function quoter\n",
    "Function detail_results\n",
    "Function print_frame\n",
    "\n",
    "*** Checking out the intricacies of test isolation ***\n",
    "\n",
    "Function protect_environment\n",
    "\n",
    "Happy jupytesting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exportation of testing tools to `jupytest.py` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean up previous\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "import shutil\n",
    "\n",
    "if os.path.isdir(\"jupytest\"):\n",
    "    print(\"Clean up previous\")\n",
    "    shutil.rmtree(\"jupytest\")\n",
    "os.makedirs(\"jupytest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "with open(\"jupytest.ipynb\", \"r\", encoding=\"utf-8\") as file_notebook:\n",
    "    nb = nbformat.read(file_notebook, nbformat.NO_CONVERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"jupytest/__init__.py\", \"w\", encoding=\"utf-8\") as file_module:\n",
    "    print(\n",
    "        \"\\n\\n\".join(\n",
    "            [\n",
    "                \"\\n\\n\".join(\n",
    "                    f'\"\"\"\\n{cell.source}\\n\"\"\"'\n",
    "                    for cell in nb.cells\n",
    "                    if cell.cell_type == 'raw' and \"header\" in cell.metadata.get(\"tags\", [])\n",
    "                )\n",
    "            ] +\n",
    "            [\n",
    "                cell.source\n",
    "                for cell in nb.cells\n",
    "                if cell.cell_type == 'code' and \"module\" in cell.metadata.get(\"tags\", [])\n",
    "            ]\n",
    "        ),\n",
    "        file=file_module,\n",
    "        end=\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "# Force the reload of the new jupytest module. If running from a fresh\n",
    "# kernel, the reload is spurious but innocuous.\n",
    "import jupytest\n",
    "importlib.reload(jupytest)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the exported module has all the tools we defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "for construct in [\n",
    "    \"Result\",\n",
    "    \"Success\",\n",
    "    \"Frame\",\n",
    "    \"Error\",\n",
    "    \"Failure\",\n",
    "    \"TestFailed\",\n",
    "    \"fail\",\n",
    "    \"Subscriber\",\n",
    "    \"Suite\",\n",
    "    \"Emphasis\",\n",
    "    \"Color\",\n",
    "    \"Plain\",\n",
    "    \"Colorizer\",\n",
    "    \"color\",\n",
    "    \"plain\",\n",
    "    \"TestNameFormatter\",\n",
    "    \"ladder\",\n",
    "    \"name_all\",\n",
    "    \"quoter\",\n",
    "    \"ProblemsEncountered\",\n",
    "    \"raise_on_error\",\n",
    "    \"PolicyReportingProblems\",\n",
    "    \"IGNORE\",\n",
    "    \"RAISE\",\n",
    "    \"report_results\",\n",
    "    \"summarize_results\",\n",
    "    \"print_frame\",\n",
    "    \"detail_result\",\n",
    "    \"detail_issues\",\n",
    "    \"Report\"\n",
    "]:\n",
    "    assert hasattr(jupytest, construct), f\"Have not got construct {construct}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: ensure the documentation for the `jupytest` module itself corresponds to the `header` cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package jupytest:\n",
      "\n",
      "NAME\n",
      "    jupytest - Unit and integration testing in a notebook\n",
      "\n",
      "DESCRIPTION\n",
      "    *** Building and running suites of tests ***\n",
      "    \n",
      "    class Suite\n",
      "        Method test (context manager)\n",
      "    Function fail\n",
      "    \n",
      "    *** Reporting test results ***\n",
      "    \n",
      "    Function report_results\n",
      "    Function summarize_results\n",
      "    Function detail_issues\n",
      "    Class Report (used as a subscriber plug-in to class Suite)\n",
      "    \n",
      "    *** Delving deeper into test results (going beyond the tools described above) ***\n",
      "    \n",
      "    Class Suite\n",
      "        Property results\n",
      "    Class Result\n",
      "        Sub-class Success\n",
      "        Sub-class Error\n",
      "            Sub-class Failure\n",
      "        Method is_success\n",
      "        Method is_failure\n",
      "    Class Frame\n",
      "    \n",
      "    *** Customizing result reporting ***\n",
      "    \n",
      "    Class Colorizer\n",
      "        Function plain\n",
      "        Function color\n",
      "    Class Emphasis\n",
      "        Sub-class Plain\n",
      "        Sub-class Color\n",
      "    Type TestNameFormatter\n",
      "        Function ladder\n",
      "        Function name_all\n",
      "        Function quoter\n",
      "    Function detail_results\n",
      "    Function print_frame\n",
      "    \n",
      "    *** Checking out the intricacies of test isolation ***\n",
      "    \n",
      "    Function protect_environment\n",
      "    \n",
      "    Happy jupytesting!\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "\n",
      "\n",
      "CLASSES\n",
      "    abc.ABC(builtins.object)\n",
      "        Emphasis\n",
      "            Color\n",
      "            Plain\n",
      "        Result\n",
      "            Error\n",
      "                Failure\n",
      "            Success\n",
      "        Subscriber\n",
      "            Magic\n",
      "            Report\n",
      "    builtins.Exception(builtins.BaseException)\n",
      "        ProblemsEncountered\n",
      "        TestFailed\n",
      "    builtins.object\n",
      "        Colorizer\n",
      "        Frame\n",
      "        PolicyReportingProblems\n",
      "        Suite\n",
      "    \n",
      "    class Color(Emphasis)\n",
      "     |  Color(fg=None, bg=None, style=None) -> None\n",
      "     |  \n",
      "     |  Helper class that provides a standard way to create an ABC using\n",
      "     |  inheritance.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Color\n",
      "     |      Emphasis\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, s: str) -> str\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __init__(self, fg=None, bg=None, style=None) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Emphasis:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Colorizer(builtins.object)\n",
      "     |  Colorizer(important: jupytest.Emphasis, trivial: jupytest.Emphasis, failure: jupytest.Emphasis, error: jupytest.Emphasis) -> None\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, important: jupytest.Emphasis, trivial: jupytest.Emphasis, failure: jupytest.Emphasis, error: jupytest.Emphasis) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Emphasis(abc.ABC)\n",
      "     |  Helper class that provides a standard way to create an ABC using\n",
      "     |  inheritance.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Emphasis\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, s: str) -> str\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'__call__'})\n",
      "    \n",
      "    class Error(Result)\n",
      "     |  Error(fns_common: Iterable[Callable]) -> None\n",
      "     |  \n",
      "     |  Non-passing test result due to an exception being raised.\n",
      "     |  \n",
      "     |  It is passed a set of common functions: the presence of these functions in the\n",
      "     |  traceback of the exception are expected and normal, making their eventual\n",
      "     |  reporting redundant and sort of trivial. The frames corresponding to these functions\n",
      "     |  in the traceback summary kept by this object will be tagged as such.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Error\n",
      "     |      Result\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, fns_common: Iterable[Callable]) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  as_dict(self, context_before: int = 3, context_after: int = 3) -> Dict\n",
      "     |      Expresses this result as a dictionary suitable to structured data serialization.\n",
      "     |  \n",
      "     |  is_success(self) -> bool\n",
      "     |      True when an associated test run has passed.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  traceback\n",
      "     |      Returns a summary of the stack trace associated to the exception that brought this test result.\n",
      "     |  \n",
      "     |  type_exc\n",
      "     |      Returns the type of the exception associated to this result.\n",
      "     |  \n",
      "     |  value_exc\n",
      "     |      Returns the exception raised in association to this test result.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  TAG_COMMON = 'common'\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Result:\n",
      "     |  \n",
      "     |  is_failure(self) -> bool\n",
      "     |      True when an associated has not passed because a designed failure condition was met.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Result:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Failure(Error)\n",
      "     |  Failure(reason: str, fns_common: Iterable[Callable])\n",
      "     |  \n",
      "     |  Test result stemming from a condition check that failed, or a test run marked\n",
      "     |  as a failure.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Failure\n",
      "     |      Error\n",
      "     |      Result\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, reason: str, fns_common: Iterable[Callable])\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  as_dict(self, context_before: int = 3, context_after: int = 3) -> Dict\n",
      "     |      Expresses this result as a dictionary suitable to structured data serialization.\n",
      "     |  \n",
      "     |  is_failure(self) -> bool\n",
      "     |      True when an associated has not passed because a designed failure condition was met.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  reason\n",
      "     |      Reason given by the programmer as to why the test failed.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Error:\n",
      "     |  \n",
      "     |  is_success(self) -> bool\n",
      "     |      True when an associated test run has passed.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Error:\n",
      "     |  \n",
      "     |  traceback\n",
      "     |      Returns a summary of the stack trace associated to the exception that brought this test result.\n",
      "     |  \n",
      "     |  type_exc\n",
      "     |      Returns the type of the exception associated to this result.\n",
      "     |  \n",
      "     |  value_exc\n",
      "     |      Returns the exception raised in association to this test result.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Error:\n",
      "     |  \n",
      "     |  TAG_COMMON = 'common'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Result:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Frame(builtins.object)\n",
      "     |  Frame(tb: inspect.Traceback, num_line: int, tags: Union[List[str], NoneType] = None) -> None\n",
      "     |  \n",
      "     |  Information regarding a frame of a traceback. Provides more than the very limited\n",
      "     |  code context that comes from standard library introspection tools.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, tb: inspect.Traceback, num_line: int, tags: Union[List[str], NoneType] = None) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __str__(self) -> str\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  as_dict(self, context_before: int = 3, context_after: int = 3) -> Dict\n",
      "     |  \n",
      "     |  context(self, before: int = 3, after: int = 3) -> List[Tuple[int, str]]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Magic(Subscriber)\n",
      "     |  Magic(name_magic: str = 'test') -> None\n",
      "     |  \n",
      "     |  Object reacting to test results as they are generated by running tests.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Magic\n",
      "     |      Subscriber\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, name_magic: str = 'test') -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  on_add_to_suite(self, suite)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Subscriber:\n",
      "     |  \n",
      "     |  on_result(self, name_test: str, result: jupytest.Result) -> None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Subscriber:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Plain(Emphasis)\n",
      "     |  Helper class that provides a standard way to create an ABC using\n",
      "     |  inheritance.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Plain\n",
      "     |      Emphasis\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, s: str) -> str\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Emphasis:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class PolicyReportingProblems(builtins.object)\n",
      "     |  PolicyReportingProblems(label: str) -> None\n",
      "     |  \n",
      "     |  What to do when reporting test results that involve problems (failures and errors).\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other: Any) -> bool\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __init__(self, label: str) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __str__(self) -> str\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "    \n",
      "    class ProblemsEncountered(builtins.Exception)\n",
      "     |  ProblemsEncountered(num_failures, num_errors)\n",
      "     |  \n",
      "     |  Raised (optionally) when a reporting routine must report failures and errors.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ProblemsEncountered\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, num_failures, num_errors)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.Exception:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "    \n",
      "    class Report(Subscriber)\n",
      "     |  Report(verbose: bool = True, file: io.TextIOBase = <ipykernel.iostream.OutStream object at 0x7f3e06fdc4f0>, colorizer: jupytest.Colorizer = <jupytest.Colorizer object at 0x7f3e04040070>, lines_context: int = 3) -> None\n",
      "     |  \n",
      "     |  Test suite subscriber that reports on test results on-the-fly. When a test\n",
      "     |  does not succeed, details on the failure can optionally be provided. See\n",
      "     |  documentation on class `Suite` to get an example on the usage of this\n",
      "     |  plug-in.\n",
      "     |  \n",
      "     |  file\n",
      "     |      File-like object where the test results are reported. Default is\n",
      "     |      standard output.\n",
      "     |  verbose\n",
      "     |      If True, the feedback on test results contains traceback information\n",
      "     |      when problems are encountered.\n",
      "     |  file\n",
      "     |      File-like object where the feedback is put out.\n",
      "     |  colorizer\n",
      "     |      Policy on how to emphasize the feedback output.\n",
      "     |  lines_context\n",
      "     |      Number of lines of code to provide as context in traceback frames\n",
      "     |      around the line of code at the nexus of an issue.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Report\n",
      "     |      Subscriber\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, verbose: bool = True, file: io.TextIOBase = <ipykernel.iostream.OutStream object at 0x7f3e06fdc4f0>, colorizer: jupytest.Colorizer = <jupytest.Colorizer object at 0x7f3e04040070>, lines_context: int = 3) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  on_result(self, name_test: str, result: jupytest.Result) -> None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Subscriber:\n",
      "     |  \n",
      "     |  on_add_to_suite(self, suite: 'Suite') -> None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Subscriber:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Result(abc.ABC)\n",
      "     |  Result of a test. Indicates whether the test passed (was a success), and if it did not,\n",
      "     |  whether it was a failure (as opposed to any other kind of issue).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Result\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  as_dict(self) -> Dict\n",
      "     |      Expresses this result as a dictionary suitable to structured data serialization.\n",
      "     |  \n",
      "     |  is_failure(self) -> bool\n",
      "     |      True when an associated has not passed because a designed failure condition was met.\n",
      "     |  \n",
      "     |  is_success(self) -> bool\n",
      "     |      True when an associated test run has passed.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'is_success'})\n",
      "    \n",
      "    class Subscriber(abc.ABC)\n",
      "     |  Object reacting to test results as they are generated by running tests.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Subscriber\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  on_add_to_suite(self, suite: 'Suite') -> None\n",
      "     |  \n",
      "     |  on_result(self, name_test: str, result: jupytest.Result) -> None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "    \n",
      "    class Success(Result)\n",
      "     |  Result for a test that passed.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Success\n",
      "     |      Result\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  is_success(self) -> bool\n",
      "     |      True when an associated test run has passed.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Result:\n",
      "     |  \n",
      "     |  as_dict(self) -> Dict\n",
      "     |      Expresses this result as a dictionary suitable to structured data serialization.\n",
      "     |  \n",
      "     |  is_failure(self) -> bool\n",
      "     |      True when an associated has not passed because a designed failure condition was met.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Result:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Suite(builtins.object)\n",
      "     |  Suite(name_magic: str = 'test') -> None\n",
      "     |  \n",
      "     |  Suite of tests, gathering the result of multiple named test runs. Test code fragments\n",
      "     |  are named using the `test()` decorator, or leveraging it indirectly by registering\n",
      "     |  a shortcut cell magic.\n",
      "     |  \n",
      "     |  Test suites gets added functionality through a publish/subscribe system. Subscriber are\n",
      "     |  special objects tied to the suite instance through its `|' (bit OR) operator. At the\n",
      "     |  moment, the only event broadcast to all subscribers is the generation of a new test\n",
      "     |  result (and its appending to the suite's log). For instance, the `Report` plug-in\n",
      "     |  supports the suite by giving immediate feedback on a test's results. Thus, to\n",
      "     |  instantiate a suite with this added feature, one would use code like\n",
      "     |  \n",
      "     |  suite = Suite() | Report()\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, name_magic: str = 'test') -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __or__(self, subscriber: jupytest.Subscriber) -> 'Suite'\n",
      "     |      Generates a clone of this suite instance, but with this subscriber subscribed to it.\n",
      "     |      \n",
      "     |      The new suite will not share member data structures with `self`, but if `self` carries\n",
      "     |      test results already, the new suite will reference the same result objects -- we\n",
      "     |      assume that Result objects are immutable.\n",
      "     |  \n",
      "     |  as_dict(self) -> Dict[str, List[Dict]]\n",
      "     |      Provides a structured data representation suitable for data serialization and exportation.\n",
      "     |  \n",
      "     |  test(self, fn: Union[Callable[..., NoneType], NoneType] = None, name: str = '', args: Sequence[Any] = [], kwargs: Mapping[str, Any] = {}) -> Union[Callable[[Callable[..., NoneType]], Callable[..., NoneType]], Callable[..., NoneType]]\n",
      "     |      Runs a test encoded into a function. Completing the function's execution counts as a\n",
      "     |      test success; tripped assertions and other exceptions generate some other Result;\n",
      "     |      and the test result is retained by this Suite instance.\n",
      "     |      \n",
      "     |      This decorator can be used two ways. Without application, one can decorate a\n",
      "     |      test function without parameter:\n",
      "     |      \n",
      "     |          @suite.test\n",
      "     |          def this_is_my_test():\n",
      "     |              # Test goes here!\n",
      "     |      \n",
      "     |      The name of the test corresponds to that of the function. Applying the decorator can\n",
      "     |      supply arguments to the test function and override the name of the test.\n",
      "     |      \n",
      "     |          @suite.test(name=\"My test, with spaces\", args=(3, 4))\n",
      "     |          def fn_test(a, b):\n",
      "     |              # Test goes here!\n",
      "     |      \n",
      "     |      To run a test with multiple parameter sets, one may even call this function directly,\n",
      "     |      not as a decorator:\n",
      "     |      \n",
      "     |          def fn_test(a, b):\n",
      "     |              # Test test test...\n",
      "     |      \n",
      "     |          for a, b in [(2, 8), (3, 4)]:\n",
      "     |              suite.test(fn_test, name=f\"Test with {a}, {b}\", args=(a, b))\n",
      "     |      \n",
      "     |      fn\n",
      "     |          Function that embodies the test code.\n",
      "     |      name\n",
      "     |          Name of the test; by default, the name of the function is used.\n",
      "     |      args\n",
      "     |          Positional arguments to pass to the function to run the test.\n",
      "     |      kwargs\n",
      "     |          Named arguments to pass to the function to run the test.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  results\n",
      "     |      Iterates through the gathered test results. For each named test, yields a tuple of\n",
      "     |      the name of the test and an iterator over each result gathered as the test has\n",
      "     |      been run.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class TestFailed(builtins.Exception)\n",
      "     |  TestFailed(reason: str) -> None\n",
      "     |  \n",
      "     |  Exception raised by this framework in order to mark a test run as a Failure.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TestFailed\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, reason: str) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.Exception:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "\n",
      "FUNCTIONS\n",
      "    color(important: Union[jupytest.Emphasis, NoneType] = None, trivial: Union[jupytest.Emphasis, NoneType] = None, failure: Union[jupytest.Emphasis, NoneType] = None, error: Union[jupytest.Emphasis, NoneType] = None) -> jupytest.Colorizer\n",
      "    \n",
      "    detail_issues(suite: jupytest.Suite, file: io.TextIOBase = <ipykernel.iostream.OutStream object at 0x7f3e06fdc4f0>, colorizer: jupytest.Colorizer = <jupytest.Colorizer object at 0x7f3e04043e20>, lines_context: int = 3, max_report: int = 9223372036854775807, on_error: jupytest.PolicyReportingProblems = <jupytest.PolicyReportingProblems object at 0x7f3e04038880>) -> None\n",
      "        Writes up a report detailing the issues encountered while running the test suite.\n",
      "        \n",
      "        suite\n",
      "            The test suite.\n",
      "        file\n",
      "            The file-like object to write the report to. Default is standard output.\n",
      "        colorizer\n",
      "            Color scheme used for emphasizing the various bits of the report.\n",
      "        lines_context\n",
      "            Number of lines of context to provide around each line of code involved\n",
      "            in a reported problem.\n",
      "        max_report\n",
      "            Maximum number of problems to report on.\n",
      "        on_error\n",
      "            What to do when reporting results that include problems such as failures and\n",
      "            errors. If set to RAISE, it will raise a ProblemsEncountered exception, which\n",
      "            is useful when running the notebook as part of a CI/CD pipeline; otherwise,\n",
      "            or if set to IGNORE, nothing more is done than writing the report.\n",
      "            Default is IGNORE.\n",
      "    \n",
      "    detail_result(name_test: str, result: jupytest.Error, prefix_header: str, file: io.TextIOBase = <ipykernel.iostream.OutStream object at 0x7f3e06fdc4f0>, colorizer: jupytest.Colorizer = <jupytest.Colorizer object at 0x7f3e04043c40>, lines_context: int = 3) -> None\n",
      "        Writes up a report regarding a single test result.\n",
      "        \n",
      "        name_test\n",
      "            Name of the test the result was gotten for.\n",
      "        result\n",
      "            Error-type result to report on.\n",
      "        prefix_header\n",
      "            String prepended to the header of the result report.\n",
      "        file\n",
      "            File-like object to write report to. Default is standard output.\n",
      "        colorizer\n",
      "            Policy for emphasizing the written report.\n",
      "        lines_context\n",
      "            Number of lines of code to fetch and write up before and after the\n",
      "            line associated to the stack frame.\n",
      "    \n",
      "    fail(reason: str = '')\n",
      "        Marks some ongoing test as failed, with an optional reason for failure.\n",
      "    \n",
      "    ladder(name_test: str, num_result: int) -> str\n",
      "        Test name formatter that puts out the name of a test only once, even\n",
      "        if it has been run multiple times.\n",
      "    \n",
      "    name_all(name_test: str, num_result: int) -> str\n",
      "        Test name formatter that puts out the name of a test even when it has\n",
      "        been run multiple times.\n",
      "    \n",
      "    plain() -> jupytest.Colorizer\n",
      "    \n",
      "    print_frame(frame: jupytest.Frame, file: io.TextIOBase = <ipykernel.iostream.OutStream object at 0x7f3e06fdc4f0>, colorizer: jupytest.Colorizer = <jupytest.Colorizer object at 0x7f3e04043a60>, lines_context: int = 3) -> None\n",
      "        Writes up a single stack frame report.\n",
      "        \n",
      "        frame\n",
      "            Stack frame to report on.\n",
      "        file\n",
      "            File-like object to write report to. Default is standard output.\n",
      "        colorizer\n",
      "            Policy for emphasizing the written report.\n",
      "        lines_context\n",
      "            Number of lines of code to fetch and write up before and after the\n",
      "            line associated to the stack frame.\n",
      "    \n",
      "    quoter(formatter: Callable[[str, int], str]) -> Callable[[str, int], str]\n",
      "        Test name formatter that surrounds the name between double quotes. Not\n",
      "        meant to be used directly by users of this module.\n",
      "    \n",
      "    raise_on_error(suite: jupytest.Suite) -> None\n",
      "    \n",
      "    report_results(suite: jupytest.Suite, file: io.TextIOBase = <ipykernel.iostream.OutStream object at 0x7f3e06fdc4f0>, colorizer: jupytest.Colorizer = <jupytest.Colorizer object at 0x7f3e040487c0>, format_name_test: Callable[[str, int], str] = <function ladder at 0x7f3e0403e700>, sep_name_result: str = '\\t', quote_names: bool = False, labels_result_custom: Mapping[type, str] = {}, on_error: jupytest.PolicyReportingProblems = <jupytest.PolicyReportingProblems object at 0x7f3e04038880>) -> None\n",
      "        Reports the name and result for each attempt at running a test, without details\n",
      "        as to issues encountered (failures and errors).\n",
      "        \n",
      "        suite\n",
      "            Suite of test to write report from.\n",
      "        file\n",
      "            File-like object to write report to. Default is standard output.\n",
      "        colorizer\n",
      "            Policy for emphasizing the written report.\n",
      "        format_name_test\n",
      "            Some tests are run more than once (for instance, for iterative problem solving).\n",
      "            In a report written for human reading, the repeated naming of a test run more\n",
      "            than once can feel redundant; it is eliminated by setting this to `ladder`.\n",
      "            All tests will be named if the formatter used instead is `name_all`.\n",
      "        sep_name_result\n",
      "            Separating character used between test name and result label. Default is \"      \".\n",
      "        quote_names\n",
      "            If True, the test names will be surrounded with double quotes in the output.\n",
      "        labels_result_custom\n",
      "            Dictionary of labels to use with different result types, when the default\n",
      "            labels (*ok* for success, *failed* for failure, *ERROR* for error) should be\n",
      "            changed. The emphasis for each label is derived from the colorizer.\n",
      "        on_error\n",
      "            What to do when reporting results that include problems such as failures and\n",
      "            errors. If set to RAISE, it will raise a ProblemsEncountered exception, which\n",
      "            is useful when running the notebook as part of a CI/CD pipeline; otherwise,\n",
      "            or if set to IGNORE, nothing more is done than writing the report.\n",
      "            Default is IGNORE.\n",
      "    \n",
      "    run_test_from_cell(cell: str) -> None\n",
      "        Executes the body of a cell, in context of the execution of a test.\n",
      "    \n",
      "    summarize_results(suite: jupytest.Suite, file: Union[io.TextIOBase, NoneType] = <ipykernel.iostream.OutStream object at 0x7f3e06fdc4f0>, colorizer: jupytest.Colorizer = <jupytest.Colorizer object at 0x7f3e04043880>, sep: str = ', ', on_error: jupytest.PolicyReportingProblems = <jupytest.PolicyReportingProblems object at 0x7f3e04038880>) -> Dict[type, int]\n",
      "        Writes a very short summary of a test run, counting the number of each result obtained.\n",
      "        \n",
      "        suite\n",
      "            Suite of test to write report from.\n",
      "        file\n",
      "            File-like object to write report to. Default is standard output.\n",
      "        colorizer\n",
      "            Policy for emphasizing the written report.\n",
      "        sep\n",
      "            Separation string between the labeled numbers of results. Default is \", \"\n",
      "        on_error\n",
      "            What to do when reporting results that include problems such as failures and\n",
      "            errors. If set to RAISE, it will raise a ProblemsEncountered exception, which\n",
      "            is useful when running the notebook as part of a CI/CD pipeline; otherwise,\n",
      "            or if set to IGNORE, nothing more is done than writing the report.\n",
      "            Default is IGNORE.\n",
      "    \n",
      "    test_cell(suite: jupytest.Suite, line: str, cell: Union[str, NoneType]) -> None\n",
      "        Runs a test written using a cell magic.\n",
      "\n",
      "DATA\n",
      "    Any = typing.Any\n",
      "    Callable = typing.Callable\n",
      "    Dict = typing.Dict\n",
      "    IGNORE = <jupytest.PolicyReportingProblems object>\n",
      "    Iterable = typing.Iterable\n",
      "    Iterator = typing.Iterator\n",
      "    List = typing.List\n",
      "    Mapping = typing.Mapping\n",
      "    Optional = typing.Optional\n",
      "    RAISE = <jupytest.PolicyReportingProblems object>\n",
      "    Sequence = typing.Sequence\n",
      "    TestFunction = typing.Callable[..., NoneType]\n",
      "    TestNameFormatter = typing.Callable[[str, int], str]\n",
      "    Tuple = typing.Tuple\n",
      "    Union = typing.Union\n",
      "\n",
      "FILE\n",
      "    /data/code/jupytest/jupytest/__init__.py\n",
      "\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(jupytest))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
