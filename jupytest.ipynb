{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The following uses a kind of literate programming approach to build a library of tools useful for writing unit and integration tests directly into a notebook. The library is to be articulated as a Python package built as the concatenation of a subset of the code cells of this notebook, using an ad hoc script. To help with identifying which code cells are parts of the final package and which are inline testing code, we use *tags*, which make up cell metadata in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from contextlib import contextmanager, ExitStack\n",
    "from copy import copy, deepcopy\n",
    "from inspect import getframeinfo, Traceback, unwrap\n",
    "from io import TextIOBase\n",
    "import itertools\n",
    "from linecache import getline\n",
    "import sys\n",
    "from traceback import walk_tb\n",
    "from typing import ContextManager, Dict, List, Tuple, Iterator, Union, Iterable, Optional, Any, Callable, Mapping\n",
    "\n",
    "import colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Result(ABC):\n",
    "    \"\"\"\n",
    "    Result of a test. Indicates whether the test passed (was a success), and if it did not,\n",
    "    whether it was a failure (as opposed to any other kind of issue).\n",
    "    \"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def is_success(self) -> bool:\n",
    "        \"\"\"True when an associated test run has passed.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def is_failure(self) -> bool:\n",
    "        \"\"\"True when an associated has not passed because a designed failure condition was met.\"\"\"\n",
    "        return False\n",
    "    \n",
    "    def as_dict(self) -> Dict:\n",
    "        \"\"\"Expresses this result as a dictionary suitable to structured data serialization.\"\"\"\n",
    "        return {\"type\": type(self).__name__}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual test failure\n",
    "\n",
    "Tests can be made to fail deliberately by raising a special exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestFailed(Exception):\n",
    "    \"\"\"\n",
    "    Exception raised by this framework in order to mark a test run as a Failure.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reason: str) -> None:\n",
    "        super().__init__(reason)\n",
    "        self.reason = reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    raise TestFailed(\"asdf\")\n",
    "except TestFailed as err:\n",
    "    assert str(err) == \"asdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fail(reason: str = \"\"):\n",
    "    \"Marks some ongoing test as failed, with an optional reason for failure.\"\n",
    "    raise TestFailed(reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupytest": {
     "test_code": true
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    fail(\"asdf\")\n",
    "    assert False\n",
    "except TestFailed as err:\n",
    "    assert err.reason == \"asdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test result: success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Success(Result):\n",
    "    \"\"\"\n",
    "    Result for a test that passed.\n",
    "    \"\"\"\n",
    "    def is_success(self) -> bool:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupytest": {
     "test_code": true
    },
    "tags": [
     "Test"
    ]
   },
   "outputs": [],
   "source": [
    "assert Success().is_success()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert Success().as_dict() == {\"type\": \"Success\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test result following the test code raising an exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traceback frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Frame:\n",
    "    \"\"\"\n",
    "    Information regarding a frame of a traceback. Provides more than the very limited\n",
    "    code context that comes from standard library introspection tools.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tb: Traceback, num_line: int, tags: Optional[List[str]] = None) -> None:\n",
    "        self.num_line = num_line\n",
    "        self.name_file = tb.filename\n",
    "        self.function = tb.function\n",
    "        self.tags = tags or []\n",
    "        \n",
    "    def context(self, before: int = 3, after: int = 3) -> List[Tuple[int, str]]:\n",
    "        ctx = [(self.num_line, getline(self.name_file, self.num_line).rstrip())]\n",
    "        for delta in range(1, before + 1):\n",
    "            ctx.insert(0, (self.num_line - delta, getline(self.name_file, self.num_line - delta).rstrip()))\n",
    "        for delta in range(1, after + 1):\n",
    "            ctx.append((self.num_line + delta, getline(self.name_file, self.num_line + delta).rstrip()))\n",
    "\n",
    "        # Clean up context: remove line-ending blanks and blank lines top and bottom\n",
    "        # of the context blob.\n",
    "        while len(ctx) > 0:\n",
    "            for i in [0, -1]:\n",
    "                if len(ctx[i][1]) == 0:\n",
    "                    del ctx[i]\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        return ctx\n",
    "    \n",
    "    def as_dict(self, context_before: int = 3, context_after: int = 3) -> Dict:\n",
    "        return {\n",
    "            \"file\": self.name_file,\n",
    "            \"line\": self.num_line,\n",
    "            \"function\": self.function,\n",
    "            \"context\": [[i, line] for i, line in self.context(context_before, context_after)],\n",
    "            \"tags\": self.tags\n",
    "        }\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return f\"File {self.name_file}, Line {self.num_line}, Function {self.function}\"\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return str(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import getfile\n",
    "\n",
    "def my_function():\n",
    "    getfile(\"asdf\")  # Will raise a TypeError\n",
    "\n",
    "frames = []\n",
    "try:\n",
    "    my_function()\n",
    "    assert False\n",
    "except TypeError:\n",
    "    _, _, tb = sys.exc_info()\n",
    "    for frame_raw, lineno in walk_tb(tb):\n",
    "        frame = Frame(getframeinfo(frame_raw), lineno)\n",
    "        assert frame.name_file == getfile(frame_raw)\n",
    "        assert frame.num_line == lineno\n",
    "        assert frame.function == frame_raw.f_code.co_name\n",
    "        assert frame.tags == []\n",
    "        frames.append(frame)\n",
    "\n",
    "assert len(frames) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = frames[1]\n",
    "assert frame.context(0, 0) == [(4, \"    getfile(\\\"asdf\\\")  # Will raise a TypeError\")]\n",
    "assert frame.context(1, 1) == [(3, \"def my_function():\"), (4, \"    getfile(\\\"asdf\\\")  # Will raise a TypeError\")]\n",
    "assert frame.context(3, 3) == [\n",
    "    (1, \"from inspect import getfile\"),\n",
    "    (2, \"\"),\n",
    "    (3, \"def my_function():\"),\n",
    "    (4, \"    getfile(\\\"asdf\\\")  # Will raise a TypeError\"),\n",
    "    (5, \"\"),\n",
    "    (6, \"frames = []\"),\n",
    "    (7, \"try:\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert frame.context(45, 0) == list(zip(range(1, 4 + 1), [\n",
    "    \"from inspect import getfile\",\n",
    "    \"\",\n",
    "    \"def my_function():\",\n",
    "    \"    getfile(\\\"asdf\\\")  # Will raise a TypeError\"\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert frame.context(0, 9000) == list(zip(range(4, 20 + 1), \"\"\"\\\n",
    "    getfile(\"asdf\")  # Will raise a TypeError\n",
    "\n",
    "frames = []\n",
    "try:\n",
    "    my_function()\n",
    "    assert False\n",
    "except TypeError:\n",
    "    _, _, tb = sys.exc_info()\n",
    "    for frame_raw, lineno in walk_tb(tb):\n",
    "        frame = Frame(getframeinfo(frame_raw), lineno)\n",
    "        assert frame.name_file == getfile(frame_raw)\n",
    "        assert frame.num_line == lineno\n",
    "        assert frame.function == frame_raw.f_code.co_name\n",
    "        assert frame.tags == []\n",
    "        frames.append(frame)\n",
    "\n",
    "assert len(frames) == 3\\\n",
    "\"\"\".split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert frame.as_dict() == {\n",
    "    \"file\": getfile(my_function),\n",
    "    \"line\": 4,\n",
    "    \"function\": \"my_function\",\n",
    "    \"tags\": [],\n",
    "    \"context\": list(list(e) for e in zip(range(1, 7 + 1), \"\"\"\\\n",
    "from inspect import getfile\n",
    "\n",
    "def my_function():\n",
    "    getfile(\"asdf\")  # Will raise a TypeError\n",
    "\n",
    "frames = []\n",
    "try:\\\n",
    "\"\"\".split(\"\\n\")))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The exception-driven result: errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Error(Result):\n",
    "    \"\"\"\n",
    "    Non-passing test result due to an exception being raised.\n",
    "    \n",
    "    It is passed a set of common functions: the presence of these functions in the\n",
    "    traceback of the exception are expected and normal, making their eventual\n",
    "    reporting redundant and sort of trivial. The frames corresponding to these functions\n",
    "    in the traceback summary kept by this object will be tagged as such.\n",
    "    \"\"\"\n",
    "    TAG_COMMON = \"common\"\n",
    "    \n",
    "    def __init__(self, fns_common: Iterable[Callable]) -> None:\n",
    "        super().__init__()\n",
    "        self._type_exc: type\n",
    "        self._value_exc: Any\n",
    "        self._type_exc, self._value_exc, tb = sys.exc_info()\n",
    "        if tb is None:\n",
    "            raise RuntimeError(\"Can only instantiate this class when an exception has been raised.\")\n",
    "            \n",
    "        codes_common = {unwrap(fn).__code__ for fn in fns_common}\n",
    "        self._traceback: List[Frame] = []\n",
    "        for frame_raw, num_line in walk_tb(tb):\n",
    "            tags = []\n",
    "            if frame_raw.f_code in codes_common:\n",
    "                tags.append(Error.TAG_COMMON)\n",
    "            self._traceback.append(Frame(getframeinfo(frame_raw), num_line, tags))\n",
    "        \n",
    "    def is_success(self) -> bool:\n",
    "        return False\n",
    "    \n",
    "    @property\n",
    "    def type_exc(self) -> type:\n",
    "        \"\"\"Returns the type of the exception associated to this result.\"\"\"\n",
    "        return self._type_exc\n",
    "    \n",
    "    @property\n",
    "    def value_exc(self) -> Any:\n",
    "        \"\"\"Returns the exception raised in association to this test result.\"\"\"\n",
    "        return self._value_exc\n",
    "    \n",
    "    @property\n",
    "    def traceback(self) -> List[Frame]:\n",
    "        \"\"\"\n",
    "        Returns a summary of the stack trace associated to the exception that brought this test result.\n",
    "        \"\"\"\n",
    "        return self._traceback\n",
    "    \n",
    "    def as_dict(self, context_before: int = 3, context_after: int = 3) -> Dict:\n",
    "        d = super().as_dict()\n",
    "        d.update(\n",
    "            {\n",
    "                \"type_exc\": self.type_exc.__name__,\n",
    "                \"value_exc\": str(self.value_exc),\n",
    "                \"traceback\": [frame.as_dict(context_before, context_after) for frame in self.traceback]\n",
    "            }\n",
    "        )\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupytest": {
     "test_code": true
    }
   },
   "outputs": [],
   "source": [
    "from inspect import getfile\n",
    "\n",
    "def fn_raise():\n",
    "    raise RuntimeError()\n",
    "    \n",
    "def caller():\n",
    "    fn_raise()\n",
    "\n",
    "try:\n",
    "    caller()\n",
    "    assert False\n",
    "except RuntimeError:\n",
    "    err: Error = Error([caller])\n",
    "    assert not err.is_success()\n",
    "    assert not err.is_failure()\n",
    "    assert err.type_exc == RuntimeError\n",
    "    assert isinstance(err.value_exc, RuntimeError)\n",
    "    assert len(err.traceback) == 3\n",
    "    assert [frame.function for frame in err.traceback] == [\"<module>\", \"caller\", \"fn_raise\"]\n",
    "    assert [frame.tags for frame in err.traceback] == [[], [Error.TAG_COMMON], []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This trick gets us a cell's \"file name\", given that the `__file__` constant is not defined\n",
    "# in Jupyter notebooks.\n",
    "import inspect\n",
    "def _asdf():\n",
    "    pass\n",
    "filename = inspect.getfile(_asdf)\n",
    "\n",
    "try:\n",
    "    raise RuntimeError()\n",
    "    assert False\n",
    "except RuntimeError:\n",
    "    assert {\n",
    "        \"type\": \"Error\",\n",
    "        \"type_exc\": \"RuntimeError\",\n",
    "        \"value_exc\": \"\",\n",
    "        \"traceback\": [\n",
    "            {\n",
    "                \"file\": filename,\n",
    "                \"line\": 9,\n",
    "                \"function\": \"<module>\",\n",
    "                \"tags\": [],\n",
    "                \"context\": [[9, \"    raise RuntimeError()\"]]\n",
    "            }\n",
    "        ]\n",
    "    } == Error([]).as_dict(0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliberate exception: failures\n",
    "\n",
    "For convenience's sake, we model `Failure`s as a subclass of `Error` to gain the exception breakdown functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Failure(Error):\n",
    "    \"\"\"\n",
    "    Test result stemming from a condition check that failed, or a test run marked\n",
    "    as a failure.\n",
    "    \"\"\"\n",
    "    def __init__(self, reason: str, fns_common: Iterable[Callable]):\n",
    "        super().__init__(fns_common)\n",
    "        self._reason = reason\n",
    "        \n",
    "    @property\n",
    "    def reason(self) -> str:\n",
    "        \"Reason given by the programmer as to why the test failed.\"\n",
    "        return self._reason\n",
    "    \n",
    "    def is_failure(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def as_dict(self, context_before: int = 3, context_after: int = 3) -> Dict:\n",
    "        d = super().as_dict(context_before, context_after)\n",
    "        d[\"reason\"] = self.reason\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupytest": {
     "test_code": true
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    assert False\n",
    "except:\n",
    "    err: Failure = Failure(\"asdf\", [])\n",
    "    assert not err.is_success()\n",
    "    assert err.is_failure()\n",
    "    assert err.type_exc == AssertionError\n",
    "    assert isinstance(err.value_exc, AssertionError)\n",
    "    assert isinstance(err.traceback, list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "def _asdf():\n",
    "    pass\n",
    "filename = inspect.getfile(_asdf)\n",
    "\n",
    "try:\n",
    "    assert False\n",
    "except:\n",
    "    assert {\n",
    "        \"type\": \"Failure\",\n",
    "        \"type_exc\": \"AssertionError\",\n",
    "        \"value_exc\": \"\",\n",
    "        \"traceback\": [\n",
    "            {\n",
    "                \"file\": filename,\n",
    "                \"line\": 7,\n",
    "                \"function\": \"<module>\",\n",
    "                \"tags\": [],\n",
    "                \"context\": [[7, \"    assert False\"]]\n",
    "            }\n",
    "        ],\n",
    "        \"reason\": \"asdf\"\n",
    "    } == Failure(\"asdf\", []).as_dict(0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment protection\n",
    "\n",
    "This is an important feature for test isolation: when running some test code, object definitions and redefinitions should be specific to the scope of the test (as if they ran from a function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def protect_environment(*names: str) -> ContextManager:\n",
    "    \"\"\"\n",
    "    Isolates the notebook's environment (variables) from redefinition and the definition\n",
    "    of new symbols during execution of the context. In addition, any variable named in\n",
    "    parameter is protected from any state change during execution of the context.\n",
    "    \"\"\"\n",
    "    assert get_ipython().ns_table[\"user_local\"] is get_ipython().ns_table[\"user_global\"]\n",
    "\n",
    "    namespace_orig = copy(globals())\n",
    "    for name in names:\n",
    "        if name in namespace_orig:\n",
    "            namespace_orig[name] = deepcopy(namespace_orig[name])\n",
    "    \n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        G = globals()\n",
    "        G.clear()\n",
    "        G.update(namespace_orig)\n",
    "        for field in [\"user_global\", \"user_local\"]:\n",
    "            get_ipython().ns_table[field] = namespace_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "jupytest": {
     "test_code": true
    }
   },
   "outputs": [],
   "source": [
    "mylist = [1, 2, 3, 4, 5]\n",
    "assert \"otherlist\" not in get_ipython().ns_table[\"user_local\"]\n",
    "\n",
    "with protect_environment():\n",
    "    otherlist = [10, 11, 12]\n",
    "    assert len(otherlist) == 3\n",
    "    mylist.pop()\n",
    "    mylist = [90]\n",
    "    \n",
    "assert \"otherlist\" not in get_ipython().ns_table[\"user_local\"]\n",
    "assert mylist == [1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "jupytest": {
     "test_code": true
    }
   },
   "outputs": [],
   "source": [
    "mylist = [1, 2, 3, 4, 5]\n",
    "assert \"otherlist\" not in get_ipython().ns_table[\"user_local\"]\n",
    "\n",
    "with protect_environment(\"mylist\"):\n",
    "    otherlist = [10, 11, 12]\n",
    "    assert len(otherlist) == 3\n",
    "    mylist.pop()\n",
    "    \n",
    "assert \"otherlist\" not in get_ipython().ns_table[\"user_local\"]\n",
    "assert mylist == [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test suites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Suite:\n",
    "    \"\"\"\n",
    "    Suite of tests, gathering the result of multiple named test runs. Test code fragments\n",
    "    are named using the `test()` context manager.\n",
    "    \"\"\"    \n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self._tests: Dict[str, List[Result]] = {}\n",
    "        self._fns_common = [fail, self.test]\n",
    "\n",
    "    @contextmanager\n",
    "    def test(self, name: str, protect_env: Union[bool, Iterable[str]] = True) -> ContextManager[None]:\n",
    "        \"\"\"\n",
    "        Starts a named testing code fragment. The fragment is run right away, which produces\n",
    "        a certain test Result that is retained by the Suite instance.\n",
    "        \n",
    "        name        - Name of the test\n",
    "        protect_env - If set to True, any symbol defined or redefined by the code in context\n",
    "                      of this manager is undone when popping out of the context. This facilitates\n",
    "                      test isolation. If, instead of True, an iterable sequence of names is passed\n",
    "                      as value to this parameter, the objects corresponding to these names in the\n",
    "                      user's namespace are saved by deep copy, thereby protecting these objects\n",
    "                      from any state change as well. If False is given as parameter value, the\n",
    "                      user's environment is not isolated from the test code, making any any definition\n",
    "                      or state change definitive (which is the usual behaviour when computing with\n",
    "                      notebooks).\n",
    "        \"\"\"\n",
    "        with ExitStack() as stack:\n",
    "            if protect_env is not False:\n",
    "                stack.enter_context(\n",
    "                    protect_environment(*(protect_env if hasattr(protect_env, \"__iter__\") else []))\n",
    "                )\n",
    "            append_result = self._tests.setdefault(name, []).append\n",
    "            try:\n",
    "                yield\n",
    "                result_args = ()\n",
    "                append_result(Success())\n",
    "            except TestFailed as err:\n",
    "                append_result(Failure(err.reason or \"Test marked as failed.\", self._fns_common))\n",
    "            except AssertionError as err:\n",
    "                append_result(Failure(str(err) or \"Assertion failed.\", self._fns_common))\n",
    "            except BaseException:\n",
    "                append_result(Error(self._fns_common))\n",
    "            \n",
    "    @property\n",
    "    def results(self) -> Iterator[Tuple[str, Iterator[Result]]]:\n",
    "        \"\"\"\n",
    "        Iterates through the gathered test results. For each named test, yields a tuple of\n",
    "        the name of the test and an iterator over each result gathered as the test has\n",
    "        been run.\n",
    "        \"\"\"\n",
    "        for name, test_results in self._tests.items():\n",
    "            yield name, iter(test_results)\n",
    "            \n",
    "    def as_dict(self) -> Dict[str, List[Dict]]:\n",
    "        \"Provides a structured data representation suitable for data serialization and exportation.\"\n",
    "        return {name: [r.as_dict() for r in rez] for name, rez in self.results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "jupytest": {
     "test_code": true
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(Suite()._tests, dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "jupytest": {
     "test_code": true
    }
   },
   "outputs": [],
   "source": [
    "with Suite().test(\"sanity-check\") as x:\n",
    "    assert x is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "jupytest": {
     "test_code": true
    }
   },
   "outputs": [],
   "source": [
    "suite = Suite()\n",
    "\n",
    "with suite.test(\"succeeding\"):\n",
    "    assert True\n",
    "    \n",
    "with suite.test(\"failing-by-assert-terse\"):\n",
    "    assert False\n",
    "    \n",
    "with suite.test(\"failing-by-assert-reason\"):\n",
    "    assert False, \"assert reason\"\n",
    "    \n",
    "with suite.test(\"failing-manually-terse\"):\n",
    "    fail()\n",
    "    \n",
    "with suite.test(\"failing-manually-reason\"):\n",
    "    fail(\"fail reason\")\n",
    "    \n",
    "with suite.test(\"error\"):\n",
    "    raise RuntimeError(\"doh\")\n",
    "\n",
    "assert [\n",
    "    (\"succeeding\", [(Success, \"\")]),\n",
    "    (\"failing-by-assert-terse\", [(Failure, \"Assertion failed.\")]),\n",
    "    (\"failing-by-assert-reason\", [(Failure, \"assert reason\")]),\n",
    "    (\"failing-manually-terse\", [(Failure, \"Test marked as failed.\")]),\n",
    "    (\"failing-manually-reason\", [(Failure, \"fail reason\")]),\n",
    "    (\"error\", [(Error, \"\")])\n",
    "] == [(name, [(type(r), r.reason if hasattr(r, \"reason\") else \"\") for r in rez]) for name, rez in suite.results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check each non-success for tagging of the first frame.\n",
    "num_non_success = 0\n",
    "for name, rez in suite.results:\n",
    "    for r in rez:\n",
    "        if not r.is_success():\n",
    "            num_non_success += 1\n",
    "            assert Error.TAG_COMMON in r.traceback[0].tags\n",
    "\n",
    "assert num_non_success == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check failures for tagging of the fail call.\n",
    "num_failures = 0\n",
    "for name, rez in suite.results:\n",
    "    if \"failing-manually\" in name:\n",
    "        for r in rez:\n",
    "            num_failures += 1\n",
    "            assert Error.TAG_COMMON in r.traceback[-1].tags\n",
    "\n",
    "assert num_failures == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "jupytest": {
     "test_code": true
    }
   },
   "outputs": [],
   "source": [
    "suite = Suite()\n",
    "\n",
    "with suite.test(\"trial\"):\n",
    "    fail()\n",
    "    \n",
    "with suite.test(\"trial\"):\n",
    "    raise RuntimeError()\n",
    "    \n",
    "with suite.test(\"trial\"):\n",
    "    pass  # Literally!\n",
    "\n",
    "assert [(\"trial\", [Failure, Error, Success])] == [(name, [type(r) for r in rez]) for name, rez in suite.results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "def _asdf():\n",
    "    pass\n",
    "filename = inspect.getfile(_asdf)\n",
    "\n",
    "\n",
    "suite = Suite()\n",
    "\n",
    "with suite.test(\"first\"):\n",
    "    fail()\n",
    "\n",
    "with suite.test(\"first\"):\n",
    "    pass\n",
    "\n",
    "with suite.test(\"second\"):\n",
    "    raise RuntimeError()\n",
    "\n",
    "assert {name: [r[\"type\"] for r in rez] for name, rez in suite.as_dict().items()} == {\n",
    "    \"first\": [\"Failure\", \"Success\"],\n",
    "    \"second\": [\"Error\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing environment protection during test execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"x\" not in globals()\n",
    "\n",
    "suite = Suite()\n",
    "with suite.test(\"trial\"):\n",
    "    x = 5\n",
    "    assert x == 5\n",
    "    \n",
    "assert \"x\" not in globals()\n",
    "assert [(\"trial\", [Success])] == [(name, [type(r) for r in rez]) for name, rez in suite.results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"x\" not in globals()\n",
    "\n",
    "suite = Suite()\n",
    "with suite.test(\"trial\", protect_env=[]):  # Test this as [] has False boolean value.\n",
    "    x = 5\n",
    "    assert x == 5\n",
    "    \n",
    "assert \"x\" not in globals()\n",
    "assert [(\"trial\", [Success])] == [(name, [type(r) for r in rez]) for name, rez in suite.results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"x\" not in globals()\n",
    "mylist = [1, 2, 3]\n",
    "\n",
    "suite = Suite()\n",
    "with suite.test(\"trial\", protect_env=[\"mylist\"]):\n",
    "    x = 5\n",
    "    assert x == 5\n",
    "    mylist.append(4)\n",
    "    \n",
    "assert \"x\" not in globals()\n",
    "assert [(\"trial\", [Success])] == [(name, [type(r) for r in rez]) for name, rez in suite.results]\n",
    "assert [1, 2, 3] == mylist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating test run reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report colorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Emphasis(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, s: str) -> str:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Color(Emphasis):\n",
    "    \n",
    "    def __init__(self, fg=None, bg=None, style=None) -> None:\n",
    "        super().__init__()\n",
    "        self._fg = fg\n",
    "        self._bg = bg\n",
    "        self._style = style\n",
    "        \n",
    "    def __call__(self, s: str) -> str:\n",
    "        return colors.color(s, fg=self._fg, bg=self._bg, style=self._style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert Color(fg=\"red\", bg=\"blue\", style=\"bold\")(\"asdf\") == '\\x1b[31;44;1masdf\\x1b[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plain(Emphasis):\n",
    "    \n",
    "    def __call__(self, s: str) -> str:\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert Plain()(\"asdf\") == \"asdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Colorizer:\n",
    "    \n",
    "    def __init__(self, important: Emphasis, failure: Emphasis, error: Emphasis) -> None:\n",
    "        self.important: Emphasis = important\n",
    "        self.failure: Emphasis = failure\n",
    "        self.error: Emphasis = error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plain() -> Colorizer:\n",
    "    return Colorizer(Plain(), Plain(), Plain())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = plain()\n",
    "assert c.important(\"asdf\") == \"asdf\"\n",
    "assert c.failure(\"asdf\") == \"asdf\"\n",
    "assert c.error(\"asdf\") == \"asdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color(\n",
    "    important: Optional[Emphasis] = None,\n",
    "    failure: Optional[Emphasis] = None,\n",
    "    error: Optional[Emphasis] = None\n",
    ") -> Colorizer:\n",
    "    return Colorizer(\n",
    "        important or Color(style=\"negative\"),\n",
    "        failure or Color(fg=\"yellow\"),\n",
    "        error or Color(fg=\"red\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = color(important=Color(fg=\"white\"), failure=Color(bg=\"blue\"), error=Color(fg=\"green\"))\n",
    "assert c.important(\"asdf\") == Color(fg=\"white\")(\"asdf\")\n",
    "assert c.failure(\"asdf\") == Color(bg=\"blue\")(\"asdf\")\n",
    "assert c.error(\"asdf\") == Color(fg=\"green\")(\"asdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exhaustive report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestNameFormatter = Callable[[str, int], str]\n",
    "\n",
    "def name_all(name_test: str, num_result: int) -> str:\n",
    "    return name_test\n",
    "\n",
    "def ladder(name_test: str, num_result: int) -> str:\n",
    "    if num_result == 0:\n",
    "        return name_test\n",
    "    return \" \" * len(name_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_results(\n",
    "    suite: Suite,\n",
    "    file: TextIOBase = sys.stdout,\n",
    "    colorizer: Colorizer = color(),\n",
    "    format_name_test: TestNameFormatter = ladder,\n",
    "    sep_name_result: str = \"\\t\",\n",
    "    labels_result_custom: Mapping[type, str] = {}\n",
    ") -> None:\n",
    "    len_all_names = [len(name) for name, _ in suite.results]\n",
    "    if len(len_all_names) == 0:\n",
    "        return\n",
    "    len_name_largest = max(len_all_names)\n",
    "\n",
    "    labels_result = {\n",
    "        type_result: colorize(labels_result_custom.get(type_result, label_default))\n",
    "        for type_result, colorize, label_default in [\n",
    "            (Success, Plain(), \"ok\"),\n",
    "            (Failure, colorizer.failure, \"failed\"),\n",
    "            (Error, colorizer.error, \"ERROR\")\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for name, rez in suite.results:\n",
    "        p_name = f\"{name:{len_name_largest}s}\"\n",
    "        for num, r in enumerate(rez):\n",
    "            print(format_name_test(p_name, num), labels_result[type(r)], sep=sep_name_result, file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = Suite()\n",
    "\n",
    "with suite.test(\"first\"):\n",
    "    raise RuntimeError()\n",
    "with suite.test(\"first\"):\n",
    "    pass\n",
    "with suite.test(\"second\"):\n",
    "    fail()\n",
    "with suite.test(\"third\"):\n",
    "    pass\n",
    "with suite.test(\"fourth\"):\n",
    "    assert False\n",
    "with suite.test(\"fourth\"):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first \t\u001b[31mERROR\u001b[0m\n",
      "      \tok\n",
      "second\t\u001b[33mfailed\u001b[0m\n",
      "third \tok\n",
      "fourth\t\u001b[33mfailed\u001b[0m\n",
      "      \tok\n"
     ]
    }
   ],
   "source": [
    "report_results(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first \t\u001b[31mERROR\u001b[0m\n",
      "first \tok\n",
      "second\t\u001b[33mfailed\u001b[0m\n",
      "third \tok\n",
      "fourth\t\u001b[33mfailed\u001b[0m\n",
      "fourth\tok\n"
     ]
    }
   ],
   "source": [
    "report_results(suite, format_name_test=name_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite_all_passed = Suite()\n",
    "\n",
    "with suite_all_passed.test(\"first\"):\n",
    "    pass\n",
    "with suite_all_passed.test(\"second\"):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first \tok\n",
      "second\tok\n"
     ]
    }
   ],
   "source": [
    "report_results(suite_all_passed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite_empty = Suite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_results(suite_empty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test result summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_results(\n",
    "    suite: Suite,\n",
    "    file: Optional[TextIOBase] = sys.stdout,\n",
    "    colorizer: Colorizer = color(),\n",
    "    sep: str = \", \"\n",
    ") -> Dict[type, int]:\n",
    "    summary = {t: 0 for t in [Success, Failure, Error]}\n",
    "    for _, rez in suite.results:\n",
    "        for r in rez:\n",
    "            summary[type(r)] += 1\n",
    "\n",
    "    if file is not None:\n",
    "        print(\n",
    "            f\"{summary[Success]} passed\",\n",
    "            (colorizer.failure if summary[Failure] > 0 else Plain())(f\"{summary[Failure]} failed\"),\n",
    "            (colorizer.error if summary[Error] > 0 else Plain())(f\"{summary[Error]} errors\"),\n",
    "            file=file,\n",
    "            sep=sep\n",
    "        )\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 passed, \u001b[33m2 failed\u001b[0m, \u001b[31m1 errors\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{__main__.Success: 3, __main__.Failure: 2, __main__.Error: 1}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_results(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{__main__.Success: 3, __main__.Failure: 2, __main__.Error: 1}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_results(suite, file=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite_all_passed = Suite()\n",
    "\n",
    "with suite_all_passed.test(\"first\"):\n",
    "    pass\n",
    "with suite_all_passed.test(\"second\"):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 passed, 0 failed, 0 errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{__main__.Success: 2, __main__.Failure: 0, __main__.Error: 0}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_results(suite_all_passed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 passed, 0 failed, 0 errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{__main__.Success: 0, __main__.Failure: 0, __main__.Error: 0}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_results(suite_empty)"
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
