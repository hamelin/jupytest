{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The following uses a kind of literate programming approach to build a library of tools useful for writing unit and integration tests directly into a notebook. The library is to be articulated as a Python package built as the concatenation of a subset of the code cells of this notebook, using an ad hoc script. To help with identifying which code cells are parts of the final package and which are inline testing code, we use *tags*, which make up cell metadata in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from contextlib import contextmanager, ExitStack\n",
    "from copy import copy, deepcopy\n",
    "from io import RawIOBase\n",
    "import sys\n",
    "from traceback import extract_tb, StackSummary\n",
    "from typing import ContextManager, Dict, List, Tuple, Iterator, Union, Iterable, Optional, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Result(ABC):\n",
    "    \"\"\"\n",
    "    Result of a test. Indicates whether the test passed (was a success), and if it did not,\n",
    "    whether it was a failure (as opposed to any other kind of issue).\n",
    "    \"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def is_success(self) -> bool:\n",
    "        \"\"\"True when an associated test run has passed.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def is_failure(self) -> bool:\n",
    "        \"\"\"True when an associated has not passed because a designed failure condition was met.\"\"\"\n",
    "        return False\n",
    "    \n",
    "    def as_dict(self) -> Dict:\n",
    "        \"\"\"Expresses this result as a dictionary suitable to structured data serialization.\"\"\"\n",
    "        return {\"type\": type(self).__name__}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Success(Result):\n",
    "    \"\"\"\n",
    "    Result for a test that passed.\n",
    "    \"\"\"\n",
    "    def is_success(self) -> bool:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "jupytest": {
     "test_code": true
    },
    "tags": [
     "Test"
    ]
   },
   "outputs": [],
   "source": [
    "assert Success().is_success()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert Success().as_dict() == {\"type\": \"Success\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Error(Result):\n",
    "    \"\"\"Non-passing test result due to an exception being raised.\"\"\"\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self._type_exc: type\n",
    "        self._value_exc: Any\n",
    "        self._type_exc, self._value_exc, tb = sys.exc_info()\n",
    "        if tb is None:\n",
    "            raise RuntimeError(\"Can only instantiate this class when an exception has been raised.\")\n",
    "        self._traceback: StackSummary = extract_tb(tb)\n",
    "        \n",
    "    def is_success(self) -> bool:\n",
    "        return False\n",
    "    \n",
    "    @property\n",
    "    def type_exc(self) -> type:\n",
    "        \"\"\"Returns the type of the exception associated to this result.\"\"\"\n",
    "        return self._type_exc\n",
    "    \n",
    "    @property\n",
    "    def value_exc(self) -> Any:\n",
    "        \"\"\"Returns the exception raised in association to this test result.\"\"\"\n",
    "        return self._value_exc\n",
    "    \n",
    "    @property\n",
    "    def traceback(self) -> StackSummary:\n",
    "        \"\"\"\n",
    "        Returns a summary of the stack trace associated to the exception that brought this test result.\n",
    "        \"\"\"\n",
    "        return self._traceback\n",
    "    \n",
    "    def as_dict(self) -> Dict:\n",
    "        d = super().as_dict()\n",
    "        d.update(\n",
    "            {\n",
    "                \"type_exc\": self.type_exc.__name__,\n",
    "                \"value_exc\": str(self.value_exc),\n",
    "                \"traceback\": [\n",
    "                    {\n",
    "                        \"filename\": filename,\n",
    "                        \"lineno\": lineno,\n",
    "                        \"context\": context,\n",
    "                        \"line_code\": line_code\n",
    "                    }\n",
    "                    for filename, lineno, context, line_code in [tuple(frame) for frame in self.traceback]\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "jupytest": {
     "test_code": true
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    raise RuntimeError()\n",
    "except:\n",
    "    err: Error = Error()\n",
    "    assert not err.is_success()\n",
    "    assert not err.is_failure()\n",
    "    assert err.type_exc == RuntimeError\n",
    "    assert isinstance(err.value_exc, RuntimeError)\n",
    "    assert isinstance(err.traceback, StackSummary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This trick gets us a cell's \"file name\", given that the `__file__` constant is not defined\n",
    "# in Jupyter notebooks.\n",
    "import inspect\n",
    "def _asdf():\n",
    "    pass\n",
    "filename = inspect.getfile(_asdf)\n",
    "\n",
    "try:\n",
    "    raise RuntimeError()\n",
    "except:\n",
    "    assert {\n",
    "        \"type\": \"Error\",\n",
    "        \"type_exc\": \"RuntimeError\",\n",
    "        \"value_exc\": \"\",\n",
    "        \"traceback\": [\n",
    "            {\n",
    "                \"filename\": filename,\n",
    "                \"lineno\": 9,\n",
    "                \"context\": \"<module>\",\n",
    "                \"line_code\": \"raise RuntimeError()\"\n",
    "            }\n",
    "        ]\n",
    "    } == Error().as_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failure\n",
    "\n",
    "For convenience's sake, we model `Failure`s as a subclass of `Error` to gain the exception breakdown functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Failure(Error):\n",
    "    \"\"\"\n",
    "    Test result stemming from a condition check that failed, or a test run marked\n",
    "    as a failure.\n",
    "    \"\"\"\n",
    "    def __init__(self, reason: str):\n",
    "        super().__init__()\n",
    "        self._reason = reason\n",
    "        \n",
    "    @property\n",
    "    def reason(self) -> str:\n",
    "        \"Reason given by the programmer as to why the test failed.\"\n",
    "        return self._reason\n",
    "    \n",
    "    def is_failure(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def as_dict(self) -> Dict:\n",
    "        d = super().as_dict()\n",
    "        d[\"reason\"] = self.reason\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "jupytest": {
     "test_code": true
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    assert False\n",
    "except:\n",
    "    err: Failure = Failure(\"asdf\")\n",
    "    assert not err.is_success()\n",
    "    assert err.is_failure()\n",
    "    assert err.type_exc == AssertionError\n",
    "    assert isinstance(err.value_exc, AssertionError)\n",
    "    assert isinstance(err.traceback, StackSummary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "def _asdf():\n",
    "    pass\n",
    "filename = inspect.getfile(_asdf)\n",
    "\n",
    "try:\n",
    "    assert False\n",
    "except:\n",
    "    assert {\n",
    "        \"type\": \"Failure\",\n",
    "        \"type_exc\": \"AssertionError\",\n",
    "        \"value_exc\": \"\",\n",
    "        \"traceback\": [\n",
    "            {\n",
    "                \"filename\": filename,\n",
    "                \"lineno\": 7,\n",
    "                \"context\": \"<module>\",\n",
    "                \"line_code\": \"assert False\"\n",
    "            }\n",
    "        ],\n",
    "        \"reason\": \"asdf\"\n",
    "    } == Failure(\"asdf\").as_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestFailed(Exception):\n",
    "    \"\"\"\n",
    "    Exception raised by this framework in order to mark a test run as a Failure.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reason: str) -> None:\n",
    "        super().__init__(reason)\n",
    "        self.reason = reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    raise TestFailed(\"asdf\")\n",
    "except TestFailed as err:\n",
    "    assert str(err) == \"asdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test:\n",
    "    \"\"\"\n",
    "    Object passed to a test code fragment, so it can communication test run outcomes\n",
    "    to the test framework.\n",
    "    \"\"\"\n",
    "    def fail(self, reason: str = \"\"):\n",
    "        \"Marks the ongoing test as failed.\"\n",
    "        raise TestFailed(reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupytest": {
     "test_code": true
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    Test().fail(\"asdf\")\n",
    "    assert False\n",
    "except TestFailed as err:\n",
    "    assert err.reason == \"asdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment protection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def protect_environment(*names: str) -> ContextManager:\n",
    "    \"\"\"\n",
    "    Isolates the notebook's environment (variables) from redefinition and the definition\n",
    "    of new symbols during execution of the context. In addition, any variable named in\n",
    "    parameter is protected from any state change during execution of the context.\n",
    "    \"\"\"\n",
    "    assert get_ipython().ns_table[\"user_local\"] is get_ipython().ns_table[\"user_global\"]\n",
    "\n",
    "    namespace_orig = copy(globals())\n",
    "    for name in names:\n",
    "        if name in namespace_orig:\n",
    "            namespace_orig[name] = deepcopy(namespace_orig[name])\n",
    "    \n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        G = globals()\n",
    "        G.clear()\n",
    "        G.update(namespace_orig)\n",
    "        for field in [\"user_global\", \"user_local\"]:\n",
    "            get_ipython().ns_table[field] = namespace_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupytest": {
     "test_code": true
    }
   },
   "outputs": [],
   "source": [
    "mylist = [1, 2, 3, 4, 5]\n",
    "assert \"otherlist\" not in get_ipython().ns_table[\"user_local\"]\n",
    "\n",
    "with protect_environment():\n",
    "    otherlist = [10, 11, 12]\n",
    "    assert len(otherlist) == 3\n",
    "    mylist.pop()\n",
    "    mylist = [90]\n",
    "    \n",
    "assert \"otherlist\" not in get_ipython().ns_table[\"user_local\"]\n",
    "assert mylist == [1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupytest": {
     "test_code": true
    }
   },
   "outputs": [],
   "source": [
    "mylist = [1, 2, 3, 4, 5]\n",
    "assert \"otherlist\" not in get_ipython().ns_table[\"user_local\"]\n",
    "\n",
    "with protect_environment(\"mylist\"):\n",
    "    otherlist = [10, 11, 12]\n",
    "    assert len(otherlist) == 3\n",
    "    mylist.pop()\n",
    "    \n",
    "assert \"otherlist\" not in get_ipython().ns_table[\"user_local\"]\n",
    "assert mylist == [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test suites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Suite:\n",
    "    \"\"\"\n",
    "    Suite of tests, gathering the result of multiple named test runs. Test code fragments\n",
    "    are named using the `test()` context manager.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self._tests: Dict[str, List[Result]] = {}\n",
    "\n",
    "    @contextmanager\n",
    "    def test(self, name: str, protect_env: Union[bool, Iterable[str]] = True) -> ContextManager[Test]:\n",
    "        \"\"\"\n",
    "        Starts a named testing code fragment. The fragment is run right away, which produces\n",
    "        a certain test Result that is retained by the Suite instance.\n",
    "        \n",
    "        name        - Name of the test\n",
    "        protect_env - If set to True, any symbol defined or redefined by the code in context\n",
    "                      of this manager is undone when popping out of the context. This facilitates\n",
    "                      test isolation. If, instead of True, an iterable sequence of names is passed\n",
    "                      as value to this parameter, the objects corresponding to these names in the\n",
    "                      user's namespace are saved by deep copy, thereby protecting these objects\n",
    "                      from any state change as well. If False is given as parameter value, the\n",
    "                      user's environment is not isolated from the test code, making any any definition\n",
    "                      or state change definitive (which is the usual behaviour when computing with\n",
    "                      notebooks).\n",
    "        \"\"\"\n",
    "        with ExitStack() as stack:\n",
    "            if protect_env is not False:\n",
    "                stack.enter_context(\n",
    "                    protect_environment(*(protect_env if hasattr(protect_env, \"__iter__\") else []))\n",
    "                )\n",
    "            append_result = self._tests.setdefault(name, []).append\n",
    "            try:\n",
    "                yield Test()\n",
    "                result_args = ()\n",
    "                append_result(Success())\n",
    "            except TestFailed as err:\n",
    "                append_result(Failure(err.reason or \"test marked as failed\"))\n",
    "            except AssertionError as err:\n",
    "                append_result(Failure(str(err) or \"assertion failed\"))\n",
    "            except BaseException:\n",
    "                append_result(Error())\n",
    "            \n",
    "    @property\n",
    "    def results(self) -> Iterator[Tuple[str, Iterator[Result]]]:\n",
    "        \"\"\"\n",
    "        Iterates through the gathered test results. For each named test, yields a tuple of\n",
    "        the name of the test and an iterator over each result gathered as the test has\n",
    "        been run.\n",
    "        \"\"\"\n",
    "        for name, test_results in self._tests.items():\n",
    "            yield name, iter(test_results)\n",
    "            \n",
    "    def as_dict(self) -> Dict[str, List[Dict]]:\n",
    "        \"Provides a structured data representation suitable for data serialization and exportation.\"\n",
    "        return {name: [r.as_dict() for r in rez] for name, rez in self.results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupytest": {
     "test_code": true
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(Suite()._tests, dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupytest": {
     "test_code": true
    }
   },
   "outputs": [],
   "source": [
    "with Suite().test(\"sanity-check\") as test:\n",
    "    assert test is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupytest": {
     "test_code": true
    }
   },
   "outputs": [],
   "source": [
    "suite = Suite()\n",
    "\n",
    "with suite.test(\"succeeding\"):\n",
    "    assert True\n",
    "    \n",
    "with suite.test(\"failing-by-assert-terse\"):\n",
    "    assert False\n",
    "    \n",
    "with suite.test(\"failing-by-assert-reason\"):\n",
    "    assert False, \"assert reason\"\n",
    "    \n",
    "with suite.test(\"failing-manually-terse\") as test:\n",
    "    test.fail()\n",
    "    \n",
    "with suite.test(\"failing-manually-reason\") as test:\n",
    "    test.fail(\"fail reason\")\n",
    "    \n",
    "with suite.test(\"error\"):\n",
    "    raise RuntimeError(\"doh\")\n",
    "\n",
    "assert [\n",
    "    (\"succeeding\", [(Success, \"\")]),\n",
    "    (\"failing-by-assert-terse\", [(Failure, \"assertion failed\")]),\n",
    "    (\"failing-by-assert-reason\", [(Failure, \"assert reason\")]),\n",
    "    (\"failing-manually-terse\", [(Failure, \"test marked as failed\")]),\n",
    "    (\"failing-manually-reason\", [(Failure, \"fail reason\")]),\n",
    "    (\"error\", [(Error, \"\")])\n",
    "] == [(name, [(type(r), r.reason if hasattr(r, \"reason\") else \"\") for r in rez]) for name, rez in suite.results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupytest": {
     "test_code": true
    }
   },
   "outputs": [],
   "source": [
    "suite = Suite()\n",
    "\n",
    "with suite.test(\"trial\") as test:\n",
    "    test.fail()\n",
    "    \n",
    "with suite.test(\"trial\") as test:\n",
    "    raise RuntimeError()\n",
    "    \n",
    "with suite.test(\"trial\") as test:\n",
    "    pass  # Literally!\n",
    "\n",
    "assert [(\"trial\", [Failure, Error, Success])] == [(name, [type(r) for r in rez]) for name, rez in suite.results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "def _asdf():\n",
    "    pass\n",
    "filename = inspect.getfile(_asdf)\n",
    "\n",
    "\n",
    "suite = Suite()\n",
    "\n",
    "with suite.test(\"first\") as test:\n",
    "    test.fail()\n",
    "\n",
    "with suite.test(\"first\") as test:\n",
    "    pass\n",
    "\n",
    "with suite.test(\"second\") as test:\n",
    "    raise RuntimeError()\n",
    "\n",
    "assert {name: [r[\"type\"] for r in rez] for name, rez in suite.as_dict().items()} == {\n",
    "    \"first\": [\"Failure\", \"Success\"],\n",
    "    \"second\": [\"Error\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing environment protection during test execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"x\" not in globals()\n",
    "\n",
    "suite = Suite()\n",
    "with suite.test(\"trial\"):\n",
    "    x = 5\n",
    "    assert x == 5\n",
    "    \n",
    "assert \"x\" not in globals()\n",
    "assert [(\"trial\", [Success])] == [(name, [type(r) for r in rez]) for name, rez in suite.results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"x\" not in globals()\n",
    "\n",
    "suite = Suite()\n",
    "with suite.test(\"trial\", protect_env=[]):  # Test this as [] has False boolean value.\n",
    "    x = 5\n",
    "    assert x == 5\n",
    "    \n",
    "assert \"x\" not in globals()\n",
    "assert [(\"trial\", [Success])] == [(name, [type(r) for r in rez]) for name, rez in suite.results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"x\" not in globals()\n",
    "mylist = [1, 2, 3]\n",
    "\n",
    "suite = Suite()\n",
    "with suite.test(\"trial\", protect_env=[\"mylist\"]):\n",
    "    x = 5\n",
    "    assert x == 5\n",
    "    mylist.append(4)\n",
    "    \n",
    "assert \"x\" not in globals()\n",
    "assert [(\"trial\", [Success])] == [(name, [type(r) for r in rez]) for name, rez in suite.results]\n",
    "assert [1, 2, 3] == mylist"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
