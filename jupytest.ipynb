{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The following uses a kind of literate programming approach to build a library of tools useful for writing unit and integration tests directly into a notebook. The library is to be articulated as a Python package built as the concatenation of a subset of the code cells of this notebook, using an ad hoc script. To help with identifying which code cells are parts of the final package and which are inline testing code, we use *tags*, which make up cell metadata in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from contextlib import contextmanager, ExitStack\n",
    "from copy import copy, deepcopy\n",
    "from inspect import getframeinfo, Traceback, unwrap\n",
    "from io import TextIOBase\n",
    "import itertools\n",
    "from linecache import getline\n",
    "import sys\n",
    "from traceback import walk_tb\n",
    "from typing import ContextManager, Dict, List, Tuple, Iterator, Union, Iterable, Optional, Any, Callable, Mapping\n",
    "\n",
    "import colors\n",
    "from pygments import highlight\n",
    "from pygments.lexers import Python3Lexer\n",
    "from pygments.formatters import TerminalFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Result(ABC):\n",
    "    \"\"\"\n",
    "    Result of a test. Indicates whether the test passed (was a success), and if it did not,\n",
    "    whether it was a failure (as opposed to any other kind of issue).\n",
    "    \"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def is_success(self) -> bool:\n",
    "        \"\"\"True when an associated test run has passed.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def is_failure(self) -> bool:\n",
    "        \"\"\"True when an associated has not passed because a designed failure condition was met.\"\"\"\n",
    "        return False\n",
    "    \n",
    "    def as_dict(self) -> Dict:\n",
    "        \"\"\"Expresses this result as a dictionary suitable to structured data serialization.\"\"\"\n",
    "        return {\"type\": type(self).__name__}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual test failure\n",
    "\n",
    "Tests can be made to fail deliberately by raising a special exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class TestFailed(Exception):\n",
    "    \"\"\"\n",
    "    Exception raised by this framework in order to mark a test run as a Failure.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reason: str) -> None:\n",
    "        super().__init__(reason)\n",
    "        self.reason = reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    raise TestFailed(\"asdf\")\n",
    "except TestFailed as err:\n",
    "    assert str(err) == \"asdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def fail(reason: str = \"\"):\n",
    "    \"Marks some ongoing test as failed, with an optional reason for failure.\"\n",
    "    raise TestFailed(reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    fail(\"asdf\")\n",
    "    assert False\n",
    "except TestFailed as err:\n",
    "    assert err.reason == \"asdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test result: success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Success(Result):\n",
    "    \"\"\"\n",
    "    Result for a test that passed.\n",
    "    \"\"\"\n",
    "    def is_success(self) -> bool:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert Success().is_success()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert Success().as_dict() == {\"type\": \"Success\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test result following the test code raising an exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traceback frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Frame:\n",
    "    \"\"\"\n",
    "    Information regarding a frame of a traceback. Provides more than the very limited\n",
    "    code context that comes from standard library introspection tools.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tb: Traceback, num_line: int, tags: Optional[List[str]] = None) -> None:\n",
    "        self.num_line = num_line\n",
    "        self.name_file = tb.filename\n",
    "        self.function = tb.function\n",
    "        self.tags = tags or []\n",
    "        \n",
    "    def context(self, before: int = 3, after: int = 3) -> List[Tuple[int, str]]:\n",
    "        ctx = [(self.num_line, getline(self.name_file, self.num_line).rstrip())]\n",
    "        for delta in range(1, before + 1):\n",
    "            ctx.insert(0, (self.num_line - delta, getline(self.name_file, self.num_line - delta).rstrip()))\n",
    "        for delta in range(1, after + 1):\n",
    "            ctx.append((self.num_line + delta, getline(self.name_file, self.num_line + delta).rstrip()))\n",
    "\n",
    "        # Clean up context: remove line-ending blanks and blank lines top and bottom\n",
    "        # of the context blob.\n",
    "        while len(ctx) > 0:\n",
    "            for i in [0, -1]:\n",
    "                if len(ctx[i][1]) == 0:\n",
    "                    del ctx[i]\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        return ctx\n",
    "    \n",
    "    def as_dict(self, context_before: int = 3, context_after: int = 3) -> Dict:\n",
    "        return {\n",
    "            \"file\": self.name_file,\n",
    "            \"line\": self.num_line,\n",
    "            \"function\": self.function,\n",
    "            \"context\": [[i, line] for i, line in self.context(context_before, context_after)],\n",
    "            \"tags\": self.tags\n",
    "        }\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return f\"File {self.name_file}, Line {self.num_line}, Function {self.function}\"\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return str(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import getfile\n",
    "\n",
    "def my_function():\n",
    "    getfile(\"asdf\")  # Will raise a TypeError\n",
    "\n",
    "frames = []\n",
    "try:\n",
    "    my_function()\n",
    "    assert False\n",
    "except TypeError:\n",
    "    _, _, tb = sys.exc_info()\n",
    "    for frame_raw, lineno in walk_tb(tb):\n",
    "        frame = Frame(getframeinfo(frame_raw), lineno)\n",
    "        assert frame.name_file == getfile(frame_raw)\n",
    "        assert frame.num_line == lineno\n",
    "        assert frame.function == frame_raw.f_code.co_name\n",
    "        assert frame.tags == []\n",
    "        frames.append(frame)\n",
    "\n",
    "assert len(frames) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = frames[1]\n",
    "assert frame.context(0, 0) == [(4, \"    getfile(\\\"asdf\\\")  # Will raise a TypeError\")]\n",
    "assert frame.context(1, 1) == [(3, \"def my_function():\"), (4, \"    getfile(\\\"asdf\\\")  # Will raise a TypeError\")]\n",
    "assert frame.context(3, 3) == [\n",
    "    (1, \"from inspect import getfile\"),\n",
    "    (2, \"\"),\n",
    "    (3, \"def my_function():\"),\n",
    "    (4, \"    getfile(\\\"asdf\\\")  # Will raise a TypeError\"),\n",
    "    (5, \"\"),\n",
    "    (6, \"frames = []\"),\n",
    "    (7, \"try:\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert frame.context(45, 0) == list(zip(range(1, 4 + 1), [\n",
    "    \"from inspect import getfile\",\n",
    "    \"\",\n",
    "    \"def my_function():\",\n",
    "    \"    getfile(\\\"asdf\\\")  # Will raise a TypeError\"\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert frame.context(0, 9000) == list(zip(range(4, 20 + 1), \"\"\"\\\n",
    "    getfile(\"asdf\")  # Will raise a TypeError\n",
    "\n",
    "frames = []\n",
    "try:\n",
    "    my_function()\n",
    "    assert False\n",
    "except TypeError:\n",
    "    _, _, tb = sys.exc_info()\n",
    "    for frame_raw, lineno in walk_tb(tb):\n",
    "        frame = Frame(getframeinfo(frame_raw), lineno)\n",
    "        assert frame.name_file == getfile(frame_raw)\n",
    "        assert frame.num_line == lineno\n",
    "        assert frame.function == frame_raw.f_code.co_name\n",
    "        assert frame.tags == []\n",
    "        frames.append(frame)\n",
    "\n",
    "assert len(frames) == 3\\\n",
    "\"\"\".split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert frame.as_dict() == {\n",
    "    \"file\": getfile(my_function),\n",
    "    \"line\": 4,\n",
    "    \"function\": \"my_function\",\n",
    "    \"tags\": [],\n",
    "    \"context\": list(list(e) for e in zip(range(1, 7 + 1), \"\"\"\\\n",
    "from inspect import getfile\n",
    "\n",
    "def my_function():\n",
    "    getfile(\"asdf\")  # Will raise a TypeError\n",
    "\n",
    "frames = []\n",
    "try:\\\n",
    "\"\"\".split(\"\\n\")))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The exception-driven result: errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Error(Result):\n",
    "    \"\"\"\n",
    "    Non-passing test result due to an exception being raised.\n",
    "    \n",
    "    It is passed a set of common functions: the presence of these functions in the\n",
    "    traceback of the exception are expected and normal, making their eventual\n",
    "    reporting redundant and sort of trivial. The frames corresponding to these functions\n",
    "    in the traceback summary kept by this object will be tagged as such.\n",
    "    \"\"\"\n",
    "    TAG_COMMON = \"common\"\n",
    "    \n",
    "    def __init__(self, fns_common: Iterable[Callable]) -> None:\n",
    "        super().__init__()\n",
    "        self._type_exc: type\n",
    "        self._value_exc: Any\n",
    "        self._type_exc, self._value_exc, tb = sys.exc_info()\n",
    "        if tb is None:\n",
    "            raise RuntimeError(\"Can only instantiate this class when an exception has been raised.\")\n",
    "            \n",
    "        codes_common = {unwrap(fn).__code__ for fn in fns_common}\n",
    "        self._traceback: List[Frame] = []\n",
    "        for frame_raw, num_line in walk_tb(tb):\n",
    "            tags = []\n",
    "            if frame_raw.f_code in codes_common:\n",
    "                tags.append(Error.TAG_COMMON)\n",
    "            self._traceback.append(Frame(getframeinfo(frame_raw), num_line, tags))\n",
    "        \n",
    "    def is_success(self) -> bool:\n",
    "        return False\n",
    "    \n",
    "    @property\n",
    "    def type_exc(self) -> type:\n",
    "        \"\"\"Returns the type of the exception associated to this result.\"\"\"\n",
    "        return self._type_exc\n",
    "    \n",
    "    @property\n",
    "    def value_exc(self) -> Any:\n",
    "        \"\"\"Returns the exception raised in association to this test result.\"\"\"\n",
    "        return self._value_exc\n",
    "    \n",
    "    @property\n",
    "    def traceback(self) -> List[Frame]:\n",
    "        \"\"\"\n",
    "        Returns a summary of the stack trace associated to the exception that brought this test result.\n",
    "        \"\"\"\n",
    "        return self._traceback\n",
    "    \n",
    "    def as_dict(self, context_before: int = 3, context_after: int = 3) -> Dict:\n",
    "        d = super().as_dict()\n",
    "        d.update(\n",
    "            {\n",
    "                \"type_exc\": self.type_exc.__name__,\n",
    "                \"value_exc\": str(self.value_exc),\n",
    "                \"traceback\": [frame.as_dict(context_before, context_after) for frame in self.traceback]\n",
    "            }\n",
    "        )\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import getfile\n",
    "\n",
    "def fn_raise():\n",
    "    raise RuntimeError()\n",
    "    \n",
    "def caller():\n",
    "    fn_raise()\n",
    "\n",
    "try:\n",
    "    caller()\n",
    "    assert False\n",
    "except RuntimeError:\n",
    "    err: Error = Error([caller])\n",
    "    assert not err.is_success()\n",
    "    assert not err.is_failure()\n",
    "    assert err.type_exc == RuntimeError\n",
    "    assert isinstance(err.value_exc, RuntimeError)\n",
    "    assert len(err.traceback) == 3\n",
    "    assert [frame.function for frame in err.traceback] == [\"<module>\", \"caller\", \"fn_raise\"]\n",
    "    assert [frame.tags for frame in err.traceback] == [[], [Error.TAG_COMMON], []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This trick gets us a cell's \"file name\", given that the `__file__` constant is not defined\n",
    "# in Jupyter notebooks.\n",
    "import inspect\n",
    "def _asdf():\n",
    "    pass\n",
    "filename = inspect.getfile(_asdf)\n",
    "\n",
    "try:\n",
    "    raise RuntimeError()\n",
    "    assert False\n",
    "except RuntimeError:\n",
    "    assert {\n",
    "        \"type\": \"Error\",\n",
    "        \"type_exc\": \"RuntimeError\",\n",
    "        \"value_exc\": \"\",\n",
    "        \"traceback\": [\n",
    "            {\n",
    "                \"file\": filename,\n",
    "                \"line\": 9,\n",
    "                \"function\": \"<module>\",\n",
    "                \"tags\": [],\n",
    "                \"context\": [[9, \"    raise RuntimeError()\"]]\n",
    "            }\n",
    "        ]\n",
    "    } == Error([]).as_dict(0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliberate exception: failures\n",
    "\n",
    "For convenience's sake, we model `Failure`s as a subclass of `Error` to gain the exception breakdown functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Failure(Error):\n",
    "    \"\"\"\n",
    "    Test result stemming from a condition check that failed, or a test run marked\n",
    "    as a failure.\n",
    "    \"\"\"\n",
    "    def __init__(self, reason: str, fns_common: Iterable[Callable]):\n",
    "        super().__init__(fns_common)\n",
    "        self._reason = reason\n",
    "        \n",
    "    @property\n",
    "    def reason(self) -> str:\n",
    "        \"Reason given by the programmer as to why the test failed.\"\n",
    "        return self._reason\n",
    "    \n",
    "    def is_failure(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def as_dict(self, context_before: int = 3, context_after: int = 3) -> Dict:\n",
    "        d = super().as_dict(context_before, context_after)\n",
    "        d[\"reason\"] = self.reason\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert False\n",
    "except:\n",
    "    err: Failure = Failure(\"asdf\", [])\n",
    "    assert not err.is_success()\n",
    "    assert err.is_failure()\n",
    "    assert err.type_exc == AssertionError\n",
    "    assert isinstance(err.value_exc, AssertionError)\n",
    "    assert isinstance(err.traceback, list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "def _asdf():\n",
    "    pass\n",
    "filename = inspect.getfile(_asdf)\n",
    "\n",
    "try:\n",
    "    assert False\n",
    "except:\n",
    "    assert {\n",
    "        \"type\": \"Failure\",\n",
    "        \"type_exc\": \"AssertionError\",\n",
    "        \"value_exc\": \"\",\n",
    "        \"traceback\": [\n",
    "            {\n",
    "                \"file\": filename,\n",
    "                \"line\": 7,\n",
    "                \"function\": \"<module>\",\n",
    "                \"tags\": [],\n",
    "                \"context\": [[7, \"    assert False\"]]\n",
    "            }\n",
    "        ],\n",
    "        \"reason\": \"asdf\"\n",
    "    } == Failure(\"asdf\", []).as_dict(0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment protection\n",
    "\n",
    "This is an important feature for test isolation: when running some test code, object definitions and redefinitions should be specific to the scope of the test (as if they ran from a function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def protect_environment(*names: str) -> ContextManager:\n",
    "    \"\"\"\n",
    "    Isolates the notebook's environment (variables) from redefinition and the definition\n",
    "    of new symbols during execution of the context. In addition, any variable named in\n",
    "    parameter is protected from any state change during execution of the context.\n",
    "    \"\"\"\n",
    "    assert get_ipython().ns_table[\"user_local\"] is get_ipython().ns_table[\"user_global\"]\n",
    "\n",
    "    namespace_orig = copy(globals())\n",
    "    for name in names:\n",
    "        if name in namespace_orig:\n",
    "            namespace_orig[name] = deepcopy(namespace_orig[name])\n",
    "    \n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        G = globals()\n",
    "        G.clear()\n",
    "        G.update(namespace_orig)\n",
    "        for field in [\"user_global\", \"user_local\"]:\n",
    "            get_ipython().ns_table[field] = namespace_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist = [1, 2, 3, 4, 5]\n",
    "assert \"otherlist\" not in get_ipython().ns_table[\"user_local\"]\n",
    "\n",
    "with protect_environment():\n",
    "    otherlist = [10, 11, 12]\n",
    "    assert len(otherlist) == 3\n",
    "    mylist.pop()\n",
    "    mylist = [90]\n",
    "    \n",
    "assert \"otherlist\" not in get_ipython().ns_table[\"user_local\"]\n",
    "assert mylist == [1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist = [1, 2, 3, 4, 5]\n",
    "assert \"otherlist\" not in get_ipython().ns_table[\"user_local\"]\n",
    "\n",
    "with protect_environment(\"mylist\"):\n",
    "    otherlist = [10, 11, 12]\n",
    "    assert len(otherlist) == 3\n",
    "    mylist.pop()\n",
    "    \n",
    "assert \"otherlist\" not in get_ipython().ns_table[\"user_local\"]\n",
    "assert mylist == [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test suites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Subscriber(ABC):\n",
    "    \"\"\"\n",
    "    Object reacting to test results as they are generated by running tests.\n",
    "    \"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def on_result(self, name_test: str, result: Result) -> None:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Suite:\n",
    "    \"\"\"\n",
    "    Suite of tests, gathering the result of multiple named test runs. Test code fragments\n",
    "    are named using the `test()` context manager.\n",
    "    \"\"\"    \n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self._tests: Dict[str, List[Result]] = {}\n",
    "        self._fns_common = [fail, self.test]\n",
    "        self._subscribers: List[Subscriber] = []\n",
    "\n",
    "    @contextmanager\n",
    "    def test(self, name: str, protect_env: Union[bool, Iterable[str]] = True) -> ContextManager[None]:\n",
    "        \"\"\"\n",
    "        Starts a named testing code fragment. The fragment is run right away, which produces\n",
    "        a certain test Result that is retained by the Suite instance.\n",
    "        \n",
    "        name        - Name of the test\n",
    "        protect_env - If set to True, any symbol defined or redefined by the code in context\n",
    "                      of this manager is undone when popping out of the context. This facilitates\n",
    "                      test isolation. If, instead of True, an iterable sequence of names is passed\n",
    "                      as value to this parameter, the objects corresponding to these names in the\n",
    "                      user's namespace are saved by deep copy, thereby protecting these objects\n",
    "                      from any state change as well. If False is given as parameter value, the\n",
    "                      user's environment is not isolated from the test code, making any any definition\n",
    "                      or state change definitive (which is the usual behaviour when computing with\n",
    "                      notebooks).\n",
    "        \"\"\"\n",
    "        with ExitStack() as stack:\n",
    "            if protect_env is not False:\n",
    "                stack.enter_context(\n",
    "                    protect_environment(*(protect_env if hasattr(protect_env, \"__iter__\") else []))\n",
    "                )\n",
    "            try:\n",
    "                yield\n",
    "                result = Success()\n",
    "            except TestFailed as err:\n",
    "                result = Failure(err.reason or \"Test marked as failed.\", self._fns_common)\n",
    "            except AssertionError as err:\n",
    "                result = Failure(str(err) or \"Assertion failed.\", self._fns_common)\n",
    "            except BaseException:\n",
    "                result = Error(self._fns_common)\n",
    "                \n",
    "        self._tests.setdefault(name, []).append(result)\n",
    "        for subscriber in self._subscribers:\n",
    "            subscriber.on_result(name, result)\n",
    "            \n",
    "    @property\n",
    "    def results(self) -> Iterator[Tuple[str, Iterator[Result]]]:\n",
    "        \"\"\"\n",
    "        Iterates through the gathered test results. For each named test, yields a tuple of\n",
    "        the name of the test and an iterator over each result gathered as the test has\n",
    "        been run.\n",
    "        \"\"\"\n",
    "        for name, test_results in self._tests.items():\n",
    "            yield name, iter(test_results)\n",
    "            \n",
    "    def as_dict(self) -> Dict[str, List[Dict]]:\n",
    "        \"Provides a structured data representation suitable for data serialization and exportation.\"\n",
    "        return {name: [r.as_dict() for r in rez] for name, rez in self.results}\n",
    "    \n",
    "    def __or__(self, subscriber: Subscriber) -> \"Suite\":\n",
    "        \"\"\"\n",
    "        Generates a clone of this suite instance, but with this subscriber subscribed to it.\n",
    "        \n",
    "        The new suite will not share member data structures with `self`, but if `self` carries\n",
    "        test results already, the new suite will reference the same result objects -- we\n",
    "        assume that Result objects are immutable.\n",
    "        \"\"\"\n",
    "        suite_with_subscriber = Suite()\n",
    "        suite_with_subscriber._tests = copy(self._tests)  # Under assumption of results immutability.\n",
    "        suite_with_subscriber._subscribers = copy(self._subscribers)\n",
    "        suite_with_subscriber._subscribers.append(subscriber)\n",
    "        return suite_with_subscriber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(Suite()._tests, dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Suite().test(\"sanity-check\") as x:\n",
    "    assert x is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = Suite()\n",
    "\n",
    "with suite.test(\"succeeding\"):\n",
    "    assert True\n",
    "    \n",
    "with suite.test(\"failing-by-assert-terse\"):\n",
    "    assert False\n",
    "    \n",
    "with suite.test(\"failing-by-assert-reason\"):\n",
    "    assert False, \"assert reason\"\n",
    "    \n",
    "with suite.test(\"failing-manually-terse\"):\n",
    "    fail()\n",
    "    \n",
    "with suite.test(\"failing-manually-reason\"):\n",
    "    fail(\"fail reason\")\n",
    "    \n",
    "with suite.test(\"error\"):\n",
    "    raise RuntimeError(\"doh\")\n",
    "\n",
    "assert [\n",
    "    (\"succeeding\", [(Success, \"\")]),\n",
    "    (\"failing-by-assert-terse\", [(Failure, \"Assertion failed.\")]),\n",
    "    (\"failing-by-assert-reason\", [(Failure, \"assert reason\")]),\n",
    "    (\"failing-manually-terse\", [(Failure, \"Test marked as failed.\")]),\n",
    "    (\"failing-manually-reason\", [(Failure, \"fail reason\")]),\n",
    "    (\"error\", [(Error, \"\")])\n",
    "] == [(name, [(type(r), r.reason if hasattr(r, \"reason\") else \"\") for r in rez]) for name, rez in suite.results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check each non-success for tagging of the first frame.\n",
    "num_non_success = 0\n",
    "for name, rez in suite.results:\n",
    "    for r in rez:\n",
    "        if not r.is_success():\n",
    "            num_non_success += 1\n",
    "            assert Error.TAG_COMMON in r.traceback[0].tags\n",
    "\n",
    "assert num_non_success == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check failures for tagging of the fail call.\n",
    "num_failures = 0\n",
    "for name, rez in suite.results:\n",
    "    if \"failing-manually\" in name:\n",
    "        for r in rez:\n",
    "            num_failures += 1\n",
    "            assert Error.TAG_COMMON in r.traceback[-1].tags\n",
    "\n",
    "assert num_failures == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = Suite()\n",
    "\n",
    "with suite.test(\"trial\"):\n",
    "    fail()\n",
    "    \n",
    "with suite.test(\"trial\"):\n",
    "    raise RuntimeError()\n",
    "    \n",
    "with suite.test(\"trial\"):\n",
    "    pass  # Literally!\n",
    "\n",
    "assert [(\"trial\", [Failure, Error, Success])] == [(name, [type(r) for r in rez]) for name, rez in suite.results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "def _asdf():\n",
    "    pass\n",
    "filename = inspect.getfile(_asdf)\n",
    "\n",
    "\n",
    "suite = Suite()\n",
    "\n",
    "with suite.test(\"first\"):\n",
    "    fail()\n",
    "\n",
    "with suite.test(\"first\"):\n",
    "    pass\n",
    "\n",
    "with suite.test(\"second\"):\n",
    "    raise RuntimeError()\n",
    "\n",
    "assert {name: [r[\"type\"] for r in rez] for name, rez in suite.as_dict().items()} == {\n",
    "    \"first\": [\"Failure\", \"Success\"],\n",
    "    \"second\": [\"Error\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing environment protection during test execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"x\" not in globals()\n",
    "\n",
    "suite = Suite()\n",
    "with suite.test(\"trial\"):\n",
    "    x = 5\n",
    "    assert x == 5\n",
    "    \n",
    "assert \"x\" not in globals()\n",
    "assert [(\"trial\", [Success])] == [(name, [type(r) for r in rez]) for name, rez in suite.results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"x\" not in globals()\n",
    "\n",
    "suite = Suite()\n",
    "with suite.test(\"trial\", protect_env=[]):  # Test this as [] has False boolean value.\n",
    "    x = 5\n",
    "    assert x == 5\n",
    "    \n",
    "assert \"x\" not in globals()\n",
    "assert [(\"trial\", [Success])] == [(name, [type(r) for r in rez]) for name, rez in suite.results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"x\" not in globals()\n",
    "mylist = [1, 2, 3]\n",
    "\n",
    "suite = Suite()\n",
    "with suite.test(\"trial\", protect_env=[\"mylist\"]):\n",
    "    x = 5\n",
    "    assert x == 5\n",
    "    mylist.append(4)\n",
    "    \n",
    "assert \"x\" not in globals()\n",
    "assert [(\"trial\", [Success])] == [(name, [type(r) for r in rez]) for name, rez in suite.results]\n",
    "assert [1, 2, 3] == mylist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing publish/subscribe of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestSubscriber(Subscriber):\n",
    "    \n",
    "    def __init__(self, lr: List[Tuple[str, Result]]):\n",
    "        self._results: List[Tuple[str, Result]] = lr\n",
    "            \n",
    "    def on_result(self, name_test: str, result: Result) -> None:\n",
    "        self._results.append((name_test, result))\n",
    "\n",
    "\n",
    "results: List[Tuple[str, Result]] = []\n",
    "suite = Suite() | TestSubscriber(results)\n",
    "\n",
    "with suite.test(\"passing\"):\n",
    "    pass\n",
    "assert len(results) == 1\n",
    "name_last, result_last = results[-1]\n",
    "assert name_last == \"passing\"\n",
    "assert isinstance(result_last, Success)\n",
    "\n",
    "with suite.test(\"error\"):\n",
    "    raise RuntimeError(\"asdf\")\n",
    "assert len(results) == 2\n",
    "name_last, result_last = results[-1]\n",
    "assert name_last == \"error\"\n",
    "assert isinstance(result_last, Error)\n",
    "assert str(result_last.value_exc) == \"asdf\"\n",
    "\n",
    "with suite.test(\"failing\"):\n",
    "    fail(\"There is no why\")\n",
    "assert len(results) == 3\n",
    "name_last, result_last = results[-1]\n",
    "assert name_last == \"failing\"\n",
    "assert isinstance(result_last, Failure)\n",
    "assert result_last.reason == \"There is no why\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating test run reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report colorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Emphasis(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, s: str) -> str:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Color(Emphasis):\n",
    "    \n",
    "    def __init__(self, fg=None, bg=None, style=None) -> None:\n",
    "        super().__init__()\n",
    "        self._fg = fg\n",
    "        self._bg = bg\n",
    "        self._style = style\n",
    "        \n",
    "    def __call__(self, s: str) -> str:\n",
    "        return colors.color(s, fg=self._fg, bg=self._bg, style=self._style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert Color(fg=\"red\", bg=\"blue\", style=\"bold\")(\"asdf\") == '\\x1b[31;44;1masdf\\x1b[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Plain(Emphasis):\n",
    "    \n",
    "    def __call__(self, s: str) -> str:\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert Plain()(\"asdf\") == \"asdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Colorizer:\n",
    "    \n",
    "    def __init__(self, important: Emphasis, trivial: Emphasis, failure: Emphasis, error: Emphasis) -> None:\n",
    "        self.important: Emphasis = important\n",
    "        self.trivial: Emphasis = trivial\n",
    "        self.failure: Emphasis = failure\n",
    "        self.error: Emphasis = error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def plain() -> Colorizer:\n",
    "    return Colorizer(Plain(), Plain(), Plain(), Plain())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = plain()\n",
    "assert c.important(\"asdf\") == \"asdf\"\n",
    "assert c.trivial(\"asdf\") == \"asdf\"\n",
    "assert c.failure(\"asdf\") == \"asdf\"\n",
    "assert c.error(\"asdf\") == \"asdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def color(\n",
    "    important: Optional[Emphasis] = None,\n",
    "    trivial: Optional[Emphasis] = None,\n",
    "    failure: Optional[Emphasis] = None,\n",
    "    error: Optional[Emphasis] = None\n",
    ") -> Colorizer:\n",
    "    return Colorizer(\n",
    "        important or Color(style=\"bold\"),\n",
    "        trivial or Color(fg=\"white\"),\n",
    "        failure or Color(fg=\"yellow\"),\n",
    "        error or Color(fg=\"red\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = color(important=Color(fg=\"white\"), trivial=Color(fg=\"black\"), failure=Color(bg=\"blue\"), error=Color(fg=\"green\"))\n",
    "assert c.important(\"asdf\") == Color(fg=\"white\")(\"asdf\")\n",
    "assert c.trivial(\"asdf\") == Color(fg=\"black\")(\"asdf\")\n",
    "assert c.failure(\"asdf\") == Color(bg=\"blue\")(\"asdf\")\n",
    "assert c.error(\"asdf\") == Color(fg=\"green\")(\"asdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exhaustive report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "TestNameFormatter = Callable[[str, int], str]\n",
    "\n",
    "def name_all(name_test: str, num_result: int) -> str:\n",
    "    \"\"\"\n",
    "    Test name formatter that puts out the name of a test even when it has\n",
    "    been run multiple times.\n",
    "    \"\"\"\n",
    "    return name_test\n",
    "\n",
    "def ladder(name_test: str, num_result: int) -> str:\n",
    "    \"\"\"\n",
    "    Test name formatter that puts out the name of a test only once, even\n",
    "    if it has been run multiple times.\n",
    "    \"\"\"\n",
    "    if num_result == 0:\n",
    "        return name_test\n",
    "    return \" \" * len(name_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def quoter(formatter: TestNameFormatter) -> TestNameFormatter:\n",
    "    \"\"\"\n",
    "    Test name formatter that surrounds the name between double quotes. Not\n",
    "    meant to be used directly by users of this module.\n",
    "    \"\"\"\n",
    "    def quoter_format(name_test: str, num_result: int) -> str:\n",
    "        return f\"\\\"{formatter(name_test, num_result)}\\\"\"\n",
    "    \n",
    "    return quoter_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def report_results(\n",
    "    suite: Suite,\n",
    "    file: TextIOBase = sys.stdout,\n",
    "    colorizer: Colorizer = color(),\n",
    "    format_name_test: TestNameFormatter = ladder,\n",
    "    sep_name_result: str = \"\\t\",\n",
    "    quote_names: bool = False,\n",
    "    labels_result_custom: Mapping[type, str] = {}\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Reports the name and result for each attempt at running a test, without details\n",
    "    as to issues encountered (failures and errors).\n",
    "    \n",
    "    suite\n",
    "        Suite of test to write report from.\n",
    "    file\n",
    "        File-like object to write report to. Default is standard output.\n",
    "    colorizer\n",
    "        Policy for emphasizing the written report.\n",
    "    format_name_test\n",
    "        Some tests are run more than once (for instance, for iterative problem solving).\n",
    "        In a report written for human reading, the repeated naming of a test run more\n",
    "        than once can feel redundant; it is eliminated by setting this to `ladder`.\n",
    "        All tests will be named if the formatter used instead is `name_all`.\n",
    "    sep_name_result\n",
    "        Separating character used between test name and result label. Default is \"\\t\".\n",
    "    quote_names\n",
    "        If True, the test names will be surrounded with double quotes in the output.\n",
    "    labels_result_custom\n",
    "        Dictionary of labels to use with different result types, when the default\n",
    "        labels (*ok* for success, *failed* for failure, *ERROR* for error) should be\n",
    "        changed. The emphasis for each label is derived from the colorizer.\n",
    "    \"\"\"\n",
    "    len_all_names = [len(name) for name, _ in suite.results]\n",
    "    if len(len_all_names) == 0:\n",
    "        return\n",
    "    len_name_largest = max(len_all_names)\n",
    "\n",
    "    labels_result = {\n",
    "        type_result: colorize(labels_result_custom.get(type_result, label_default))\n",
    "        for type_result, colorize, label_default in [\n",
    "            (Success, Plain(), \"ok\"),\n",
    "            (Failure, colorizer.failure, \"failed\"),\n",
    "            (Error, colorizer.error, \"ERROR\")\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    if quote_names:\n",
    "        format_name_test = quoter(format_name_test)\n",
    "    \n",
    "    for name, rez in suite.results:\n",
    "        p_name = f\"{name:{len_name_largest}s}\"\n",
    "        for num, r in enumerate(rez):\n",
    "            print(format_name_test(p_name, num), labels_result[type(r)], sep=sep_name_result, file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = Suite()\n",
    "\n",
    "with suite.test(\"first\"):\n",
    "    raise RuntimeError()\n",
    "with suite.test(\"first\"):\n",
    "    pass\n",
    "with suite.test(\"second\"):\n",
    "    fail()\n",
    "with suite.test(\"third\"):\n",
    "    pass\n",
    "with suite.test(\"fourth\"):\n",
    "    assert False\n",
    "with suite.test(\"fourth\"):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: tests `first` and `fourth` are run twice; `second` and `third` only once. The name of each test is written only once. The results are either `ok`, `failed` or `ERROR` and are emphasized as normal for success, yellow for failure and red for error. The results are also lined up cleanly into a second column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first \t\u001b[31mERROR\u001b[0m\n",
      "      \tok\n",
      "second\t\u001b[33mfailed\u001b[0m\n",
      "third \tok\n",
      "fourth\t\u001b[33mfailed\u001b[0m\n",
      "      \tok\n"
     ]
    }
   ],
   "source": [
    "report_results(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: like previous, except that the test names are written on each line a result is reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first \t\u001b[31mERROR\u001b[0m\n",
      "first \tok\n",
      "second\t\u001b[33mfailed\u001b[0m\n",
      "third \tok\n",
      "fourth\t\u001b[33mfailed\u001b[0m\n",
      "fourth\tok\n"
     ]
    }
   ],
   "source": [
    "report_results(suite, format_name_test=name_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: like previous, but CSV-like, with test names quoted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"first \",ERROR\n",
      "\"first \",ok\n",
      "\"second\",failed\n",
      "\"third \",ok\n",
      "\"fourth\",failed\n",
      "\"fourth\",ok\n"
     ]
    }
   ],
   "source": [
    "report_results(suite, format_name_test=name_all, quote_names=True, sep_name_result=\",\", colorizer=plain())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite_all_passed = Suite()\n",
    "\n",
    "with suite_all_passed.test(\"first\"):\n",
    "    pass\n",
    "with suite_all_passed.test(\"second\"):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: two results, both `ok`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first \tok\n",
      "second\tok\n"
     ]
    }
   ],
   "source": [
    "report_results(suite_all_passed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite_empty = Suite()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: nothing written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_results(suite_empty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test result summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def summarize_results(\n",
    "    suite: Suite,\n",
    "    file: Optional[TextIOBase] = sys.stdout,\n",
    "    colorizer: Colorizer = color(),\n",
    "    sep: str = \", \"\n",
    ") -> Dict[type, int]:\n",
    "    \"\"\"\n",
    "    Writes a very short summary of a test run, counting the number of each result obtained.\n",
    "\n",
    "    suite\n",
    "        Suite of test to write report from.\n",
    "    file\n",
    "        File-like object to write report to. Default is standard output.\n",
    "    colorizer\n",
    "        Policy for emphasizing the written report.\n",
    "    sep\n",
    "        Separation string between the labeled numbers of results. Default is \", \"\n",
    "    \"\"\"\n",
    "    summary = {t: 0 for t in [Success, Failure, Error]}\n",
    "    for _, rez in suite.results:\n",
    "        for r in rez:\n",
    "            summary[type(r)] += 1\n",
    "\n",
    "    if file is not None:\n",
    "        print(\n",
    "            f\"{summary[Success]} passed\",\n",
    "            (colorizer.failure if summary[Failure] > 0 else colorizer.trivial)(f\"{summary[Failure]} failed\"),\n",
    "            (colorizer.error if summary[Error] > 0 else colorizer.trivial)(f\"{summary[Error]} raised an error\"),\n",
    "            file=file,\n",
    "            sep=sep\n",
    "        )\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: should show 3 tests passed, 2 failures (in yellow), 1 error (in red)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 passed, \u001b[33m2 failed\u001b[0m, \u001b[31m1 raised an error\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{__main__.Success: 3, __main__.Failure: 2, __main__.Error: 1}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_results(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check visually that nothing is written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert summarize_results(suite, file=None) == {\n",
    "    Success: 3,\n",
    "    Failure: 2,\n",
    "    Error: 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite_all_passed = Suite()\n",
    "\n",
    "with suite_all_passed.test(\"first\"):\n",
    "    pass\n",
    "with suite_all_passed.test(\"second\"):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check visually that report is fine. Should show 2 tests passed, and failed and errors labeled in a subdued color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 passed, \u001b[37m0 failed\u001b[0m, \u001b[37m0 raised an error\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{__main__.Success: 2, __main__.Failure: 0, __main__.Error: 0}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_results(suite_all_passed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check visually that report is fine. Should show 0 for each type of test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 passed, \u001b[37m0 failed\u001b[0m, \u001b[37m0 raised an error\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{__main__.Success: 0, __main__.Failure: 0, __main__.Error: 0}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_results(suite_empty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed report of issues encountered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing one frame for a result's associated traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def print_frame(\n",
    "    frame: Frame,\n",
    "    file: TextIOBase = sys.stdout,\n",
    "    colorizer: Colorizer = color(),\n",
    "    lines_context: int = 3\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Writes up a single stack frame report.\n",
    "\n",
    "    frame\n",
    "        Stack frame to report on.\n",
    "    file\n",
    "        File-like object to write report to. Default is standard output.\n",
    "    colorizer\n",
    "        Policy for emphasizing the written report.\n",
    "    lines_context\n",
    "        Number of lines of code to fetch and write up before and after the\n",
    "        line associated to the stack frame.\n",
    "    \"\"\"\n",
    "    header = (\n",
    "        colorizer.trivial\n",
    "        if Error.TAG_COMMON in frame.tags\n",
    "        else Plain()\n",
    "    )(\" | \".join([f\"File {frame.name_file}\", f\"Line {frame.num_line}\", f\"Function {frame.function}\"]))\n",
    "    print(header, file=file)\n",
    "    if Error.TAG_COMMON not in frame.tags:\n",
    "        context: List[Tuple[int, str]] = frame.context(before=lines_context, after=lines_context)\n",
    "        if len(context) > 0:\n",
    "            max_len_num_line = len(str(context[-1][0]))\n",
    "            for i, line in zip(\n",
    "                [i for i, _ in context],\n",
    "                highlight(\n",
    "                    \"\\n\".join(ln for _, ln in context),\n",
    "                    lexer=Python3Lexer(),\n",
    "                    formatter=TerminalFormatter()\n",
    "                ).split(\"\\n\")\n",
    "            ):\n",
    "                print(\n",
    "                    colorizer.trivial(f\"{i:{max_len_num_line}d}\"),\n",
    "                    colorizer.trivial(\"|\"),\n",
    "                    line,\n",
    "                    sep=\" \",\n",
    "                    file=file\n",
    "                )\n",
    "    print(file=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: show show 3 frame reports. First and third must have appropriate code context (check against the line number), three lines of context around target line max. Second frame report should have subdued color and no code context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File <ipython-input-66-338c4a2e17e5> | Line 8 | Function <module>\n",
      "\u001b[37m 5\u001b[0m \u001b[37m|\u001b[0m     raiser()\n",
      "\u001b[37m 6\u001b[0m \u001b[37m|\u001b[0m \n",
      "\u001b[37m 7\u001b[0m \u001b[37m|\u001b[0m \u001b[34mtry\u001b[39;49;00m:\n",
      "\u001b[37m 8\u001b[0m \u001b[37m|\u001b[0m     caller()\n",
      "\u001b[37m 9\u001b[0m \u001b[37m|\u001b[0m     \u001b[34massert\u001b[39;49;00m \u001b[34mFalse\u001b[39;49;00m\n",
      "\u001b[37m10\u001b[0m \u001b[37m|\u001b[0m \u001b[34mexcept\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m:\n",
      "\u001b[37m11\u001b[0m \u001b[37m|\u001b[0m     _, _, tb = sys.exc_info()\n",
      "\n",
      "\u001b[37mFile <ipython-input-66-338c4a2e17e5> | Line 5 | Function caller\u001b[0m\n",
      "\n",
      "File <ipython-input-66-338c4a2e17e5> | Line 2 | Function raiser\n",
      "\u001b[37m1\u001b[0m \u001b[37m|\u001b[0m \u001b[34mdef\u001b[39;49;00m \u001b[32mraiser\u001b[39;49;00m():\n",
      "\u001b[37m2\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m()\n",
      "\u001b[37m3\u001b[0m \u001b[37m|\u001b[0m \n",
      "\u001b[37m4\u001b[0m \u001b[37m|\u001b[0m \u001b[34mdef\u001b[39;49;00m \u001b[32mcaller\u001b[39;49;00m():\n",
      "\u001b[37m5\u001b[0m \u001b[37m|\u001b[0m     raiser()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def raiser():\n",
    "    raise RuntimeError()\n",
    "    \n",
    "def caller():\n",
    "    raiser()\n",
    "    \n",
    "try:\n",
    "    caller()\n",
    "    assert False\n",
    "except RuntimeError:\n",
    "    _, _, tb = sys.exc_info()\n",
    "    for frame, lineno in walk_tb(tb):\n",
    "        print_frame(\n",
    "            Frame(\n",
    "                getframeinfo(frame), \n",
    "                lineno,\n",
    "                [Error.TAG_COMMON] if frame.f_code is caller.__code__ else [])\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailing one result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def detail_result(\n",
    "    name_test: str,\n",
    "    result: Error,\n",
    "    prefix_header: str,\n",
    "    file: TextIOBase = sys.stdout,\n",
    "    colorizer: Colorizer = color(),\n",
    "    lines_context: int = 3\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Writes up a report regarding a single test result.\n",
    "    \n",
    "    name_test\n",
    "        Name of the test the result was gotten for.\n",
    "    result\n",
    "        Error-type result to report on.\n",
    "    prefix_header\n",
    "        String prepended to the header of the result report.\n",
    "    file\n",
    "        File-like object to write report to. Default is standard output.\n",
    "    colorizer\n",
    "        Policy for emphasizing the written report.\n",
    "    lines_context\n",
    "        Number of lines of code to fetch and write up before and after the\n",
    "        line associated to the stack frame.\n",
    "    \"\"\"\n",
    "    header = \" ** \".join([\n",
    "        prefix_header,\n",
    "        f\"Test {colorizer.important(name_test)}\", \n",
    "        {Failure: colorizer.failure, Error: colorizer.error}[type(result)](type(result).__name__)\n",
    "    ])\n",
    "    print(\"-\" * len(colors.strip_color(header)), file=file)\n",
    "    print(header, file=file)\n",
    "    if result.is_failure():\n",
    "        print(result.reason, file=file)\n",
    "    else:\n",
    "        print(f\"{result.type_exc.__name__}:\", str(result.value_exc) or \"<no detail provided>\", file=file)\n",
    "    print(file=file)\n",
    "    \n",
    "    for frame in result.traceback:  # First frame is always Suite.test, which is irrelevant.\n",
    "        print_frame(frame, file=file, colorizer=colorizer, lines_context=lines_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: header should start with first word **HEY**, label the test as **second** and map it as a *Failure*. Its frame report shows two subdued frames without code context, sandwiching a frame showing the call to the `fail()` function (line 8) that tripped the failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "HEY ** Test \u001b[1msecond\u001b[0m ** \u001b[33mFailure\u001b[0m\n",
      "Test marked as failed.\n",
      "\n",
      "\u001b[37mFile <ipython-input-26-39c764bb9450> | Line 35 | Function test\u001b[0m\n",
      "\n",
      "File <ipython-input-51-11f8f96b7c56> | Line 8 | Function <module>\n",
      "\u001b[37m 5\u001b[0m \u001b[37m|\u001b[0m \u001b[34mwith\u001b[39;49;00m suite.test(\u001b[33m\"\u001b[39;49;00m\u001b[33mfirst\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "\u001b[37m 6\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mpass\u001b[39;49;00m\n",
      "\u001b[37m 7\u001b[0m \u001b[37m|\u001b[0m \u001b[34mwith\u001b[39;49;00m suite.test(\u001b[33m\"\u001b[39;49;00m\u001b[33msecond\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "\u001b[37m 8\u001b[0m \u001b[37m|\u001b[0m     fail()\n",
      "\u001b[37m 9\u001b[0m \u001b[37m|\u001b[0m \u001b[34mwith\u001b[39;49;00m suite.test(\u001b[33m\"\u001b[39;49;00m\u001b[33mthird\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "\u001b[37m10\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mpass\u001b[39;49;00m\n",
      "\u001b[37m11\u001b[0m \u001b[37m|\u001b[0m \u001b[34mwith\u001b[39;49;00m suite.test(\u001b[33m\"\u001b[39;49;00m\u001b[33mfourth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "\n",
      "\u001b[37mFile <ipython-input-5-f0d8945e4a67> | Line 3 | Function fail\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name, rez = list(suite.results)[1]\n",
    "failure = list(rez)[0]\n",
    "assert isinstance(failure, Failure)\n",
    "detail_result(name, failure, \"HEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "def detail_errors(\n",
    "    suite: Suite,\n",
    "    file: TextIOBase = sys.stdout,\n",
    "    colorizer: Colorizer = color(),\n",
    "    lines_context: int = 3,\n",
    "    max_report: int = sys.maxsize\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Writes up a report detailing the issues encountered while running the test suite.\n",
    "    \n",
    "    suite\n",
    "        The test suite.\n",
    "    file\n",
    "        The file-like object to write the report to. Default is standard output.\n",
    "    colorizer\n",
    "        Color scheme used for emphasizing the various bits of the report.\n",
    "    lines_context\n",
    "        Number of lines of context to provide around each line of code involved\n",
    "        in a reported problem.\n",
    "    max_report\n",
    "        Maximum number of problems to report on.\n",
    "    \"\"\"\n",
    "    summary = summarize_results(suite, file=None)\n",
    "    num_problems = summary[Failure] + summary[Error]\n",
    "    if num_problems == 0:\n",
    "        if summary[Success] == 0:\n",
    "            print(\"No test run.\", file=file)\n",
    "        else:\n",
    "            print(f\"All {summary[Success]} tests passed. No failure nor error encountered.\", file=file)\n",
    "    else:\n",
    "        index = 1\n",
    "        for name, rez in suite.results:\n",
    "            for r in rez:\n",
    "                if not r.is_success():\n",
    "                    detail_result(name, r, f\"# {index}/{num_problems}\", lines_context=lines_context, file=file)\n",
    "                    print()\n",
    "\n",
    "                    if index >= max_report:\n",
    "                        num_remaining = num_problems - index\n",
    "                        print(colorizer.important(f\"... plus {num_remaining} other issue{'s' if num_remaining > 1 else ''}.\"))\n",
    "                        return\n",
    "                    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: 3 problem reports expected. First one is for an undetailed error (`RuntimeError`). Second and third are for failures. The first frame report for each (...` | Function test`) is in a subdued color, as is the frame for the call to function `fail`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "# 1/3 ** Test \u001b[1mfirst\u001b[0m ** \u001b[31mError\u001b[0m\n",
      "RuntimeError: <no detail provided>\n",
      "\n",
      "\u001b[37mFile <ipython-input-26-39c764bb9450> | Line 35 | Function test\u001b[0m\n",
      "\n",
      "File <ipython-input-51-11f8f96b7c56> | Line 4 | Function <module>\n",
      "\u001b[37m1\u001b[0m \u001b[37m|\u001b[0m suite = Suite()\n",
      "\u001b[37m2\u001b[0m \u001b[37m|\u001b[0m \n",
      "\u001b[37m3\u001b[0m \u001b[37m|\u001b[0m \u001b[34mwith\u001b[39;49;00m suite.test(\u001b[33m\"\u001b[39;49;00m\u001b[33mfirst\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "\u001b[37m4\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m()\n",
      "\u001b[37m5\u001b[0m \u001b[37m|\u001b[0m \u001b[34mwith\u001b[39;49;00m suite.test(\u001b[33m\"\u001b[39;49;00m\u001b[33mfirst\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "\u001b[37m6\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mpass\u001b[39;49;00m\n",
      "\u001b[37m7\u001b[0m \u001b[37m|\u001b[0m \u001b[34mwith\u001b[39;49;00m suite.test(\u001b[33m\"\u001b[39;49;00m\u001b[33msecond\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "\n",
      "\n",
      "-------------------------------\n",
      "# 2/3 ** Test \u001b[1msecond\u001b[0m ** \u001b[33mFailure\u001b[0m\n",
      "Test marked as failed.\n",
      "\n",
      "\u001b[37mFile <ipython-input-26-39c764bb9450> | Line 35 | Function test\u001b[0m\n",
      "\n",
      "File <ipython-input-51-11f8f96b7c56> | Line 8 | Function <module>\n",
      "\u001b[37m 5\u001b[0m \u001b[37m|\u001b[0m \u001b[34mwith\u001b[39;49;00m suite.test(\u001b[33m\"\u001b[39;49;00m\u001b[33mfirst\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "\u001b[37m 6\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mpass\u001b[39;49;00m\n",
      "\u001b[37m 7\u001b[0m \u001b[37m|\u001b[0m \u001b[34mwith\u001b[39;49;00m suite.test(\u001b[33m\"\u001b[39;49;00m\u001b[33msecond\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "\u001b[37m 8\u001b[0m \u001b[37m|\u001b[0m     fail()\n",
      "\u001b[37m 9\u001b[0m \u001b[37m|\u001b[0m \u001b[34mwith\u001b[39;49;00m suite.test(\u001b[33m\"\u001b[39;49;00m\u001b[33mthird\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "\u001b[37m10\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mpass\u001b[39;49;00m\n",
      "\u001b[37m11\u001b[0m \u001b[37m|\u001b[0m \u001b[34mwith\u001b[39;49;00m suite.test(\u001b[33m\"\u001b[39;49;00m\u001b[33mfourth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "\n",
      "\u001b[37mFile <ipython-input-5-f0d8945e4a67> | Line 3 | Function fail\u001b[0m\n",
      "\n",
      "\n",
      "-------------------------------\n",
      "# 3/3 ** Test \u001b[1mfourth\u001b[0m ** \u001b[33mFailure\u001b[0m\n",
      "Assertion failed.\n",
      "\n",
      "\u001b[37mFile <ipython-input-26-39c764bb9450> | Line 35 | Function test\u001b[0m\n",
      "\n",
      "File <ipython-input-51-11f8f96b7c56> | Line 12 | Function <module>\n",
      "\u001b[37m 9\u001b[0m \u001b[37m|\u001b[0m \u001b[34mwith\u001b[39;49;00m suite.test(\u001b[33m\"\u001b[39;49;00m\u001b[33mthird\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "\u001b[37m10\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mpass\u001b[39;49;00m\n",
      "\u001b[37m11\u001b[0m \u001b[37m|\u001b[0m \u001b[34mwith\u001b[39;49;00m suite.test(\u001b[33m\"\u001b[39;49;00m\u001b[33mfourth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "\u001b[37m12\u001b[0m \u001b[37m|\u001b[0m     \u001b[34massert\u001b[39;49;00m \u001b[34mFalse\u001b[39;49;00m\n",
      "\u001b[37m13\u001b[0m \u001b[37m|\u001b[0m \u001b[34mwith\u001b[39;49;00m suite.test(\u001b[33m\"\u001b[39;49;00m\u001b[33mfourth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "\u001b[37m14\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mpass\u001b[39;49;00m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "detail_errors(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: show only the report for the error, and mention that two more problems have yet to be reported. Also, code context is only one line around the target line, making for a 3-line code blurb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "# 1/3 ** Test \u001b[1mfirst\u001b[0m ** \u001b[31mError\u001b[0m\n",
      "RuntimeError: <no detail provided>\n",
      "\n",
      "\u001b[37mFile <ipython-input-26-39c764bb9450> | Line 35 | Function test\u001b[0m\n",
      "\n",
      "File <ipython-input-51-11f8f96b7c56> | Line 4 | Function <module>\n",
      "\u001b[37m3\u001b[0m \u001b[37m|\u001b[0m \u001b[34mwith\u001b[39;49;00m suite.test(\u001b[33m\"\u001b[39;49;00m\u001b[33mfirst\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "\u001b[37m4\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m()\n",
      "\u001b[37m5\u001b[0m \u001b[37m|\u001b[0m \u001b[34mwith\u001b[39;49;00m suite.test(\u001b[33m\"\u001b[39;49;00m\u001b[33mfirst\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "\n",
      "\n",
      "\u001b[1m... plus 2 other issues.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "detail_errors(suite, max_report=1, lines_context=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: now show the error and the first failure, with 0 code context (so only the target line is shown). The final message about remaining issues is singular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "# 1/3 ** Test \u001b[1mfirst\u001b[0m ** \u001b[31mError\u001b[0m\n",
      "RuntimeError: <no detail provided>\n",
      "\n",
      "\u001b[37mFile <ipython-input-26-39c764bb9450> | Line 35 | Function test\u001b[0m\n",
      "\n",
      "File <ipython-input-51-11f8f96b7c56> | Line 4 | Function <module>\n",
      "\u001b[37m4\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m()\n",
      "\n",
      "\n",
      "-------------------------------\n",
      "# 2/3 ** Test \u001b[1msecond\u001b[0m ** \u001b[33mFailure\u001b[0m\n",
      "Test marked as failed.\n",
      "\n",
      "\u001b[37mFile <ipython-input-26-39c764bb9450> | Line 35 | Function test\u001b[0m\n",
      "\n",
      "File <ipython-input-51-11f8f96b7c56> | Line 8 | Function <module>\n",
      "\u001b[37m8\u001b[0m \u001b[37m|\u001b[0m     fail()\n",
      "\n",
      "\u001b[37mFile <ipython-input-5-f0d8945e4a67> | Line 3 | Function fail\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m... plus 1 other issue.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "detail_errors(suite, max_report=2, lines_context=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: two tests passed, no failure nor error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 2 tests passed. No failure nor error encountered.\n"
     ]
    }
   ],
   "source": [
    "detail_errors(suite_all_passed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: no test has been run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No test run.\n"
     ]
    }
   ],
   "source": [
    "detail_errors(suite_empty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On-the-fly result reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": [
     "module"
    ]
   },
   "outputs": [],
   "source": [
    "class Report(Subscriber):\n",
    "    \"\"\"\n",
    "    Test suite subscriber that reports on test results on-the-fly. When a test\n",
    "    does not succeed, details on the failure can optionally be provided.\n",
    "    \n",
    "    file\n",
    "        File-like object where the test results are reported. Default is\n",
    "        standard output.\n",
    "    in_detail\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        verbose: bool = True,\n",
    "        file: TextIOBase = sys.stdout,\n",
    "        colorizer: Colorizer = color(),\n",
    "        lines_context: int = 3\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._last: Optional[bool] = None\n",
    "        self._file = file\n",
    "        self._verbose = verbose\n",
    "        self._colorizer = colorizer\n",
    "        self._lines_context = lines_context\n",
    "        \n",
    "    def on_result(self, name_test: str, result: Result) -> None:\n",
    "        name_test = self._colorizer.important(name_test)\n",
    "        if result.is_success():\n",
    "            msg = f\"Test {name_test} passed.\"\n",
    "            if self._verbose and self._last is False:\n",
    "                print(\"-\" * len(colors.strip_color(msg)), file=self._file)\n",
    "            self._last = True\n",
    "            print(msg, file=self._file)\n",
    "        else:\n",
    "            if self._last is True:\n",
    "                print(file=self._file)\n",
    "            self._last = False\n",
    "            if self._verbose:\n",
    "                detail_result(\n",
    "                    name_test,\n",
    "                    result,\n",
    "                    \"Issue encountered\",\n",
    "                    self._file,\n",
    "                    self._colorizer,\n",
    "                    self._lines_context\n",
    "                )\n",
    "            else:\n",
    "                index_frame_relevant = -1\n",
    "                if result.is_failure():\n",
    "                    label = self._colorizer.failure(f\"Test {name_test} failed\")\n",
    "                    print(f\"{label}: {result.reason}\", file=self._file)\n",
    "                    if isinstance(result.type_exc, TestFailed):\n",
    "                        index_frame_relevant = -2\n",
    "                else:\n",
    "                    label = self._colorizer.error(f\"Error occured during test {name_test}\")\n",
    "                    value_exc = \"\"\n",
    "                    if str(result.value_exc):\n",
    "                        value_exc = f\" -- {str(result.value_exc)}\"\n",
    "                    print(f\"{label}: {result.type_exc.__name__}{value_exc}\", file=self._file)\n",
    "                frame_relevant = result.traceback[index_frame_relevant]\n",
    "                print_frame(\n",
    "                    frame_relevant,\n",
    "                    file=self._file,\n",
    "                    colorizer=self._colorizer,\n",
    "                    lines_context=self._lines_context\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exercise_suite(reporter):\n",
    "    suite = Suite() | reporter\n",
    "    with suite.test(\"first-passing\"):\n",
    "        pass\n",
    "    with suite.test(\"second-passing\"):\n",
    "        pass\n",
    "    with suite.test(\"failing\"):\n",
    "        assert 1 == 0, \"one vs. zero\"\n",
    "    with suite.test(\"error-raising\"):\n",
    "        raise RuntimeError(\"Some error\")\n",
    "    with suite.test(\"next-to-last\"):\n",
    "        pass\n",
    "    with suite.test(\"last\"):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: first and second tests pass, third test fails, fourth test raises an error, last two tests pass. Failure and error are tersely described, only the most relevant stack frame is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mfirst-passing\u001b[0m passed.\n",
      "Test \u001b[1msecond-passing\u001b[0m passed.\n",
      "\n",
      "\u001b[33mTest \u001b[1mfailing\u001b[0m failed\u001b[0m: one vs. zero\n",
      "File <ipython-input-76-0b9ff122e3c7> | Line 8 | Function exercise_suite\n",
      "\u001b[37m 5\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mwith\u001b[39;49;00m suite.test(\u001b[33m\"\u001b[39;49;00m\u001b[33msecond-passing\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "\u001b[37m 6\u001b[0m \u001b[37m|\u001b[0m         \u001b[34mpass\u001b[39;49;00m\n",
      "\u001b[37m 7\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mwith\u001b[39;49;00m suite.test(\u001b[33m\"\u001b[39;49;00m\u001b[33mfailing\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "\u001b[37m 8\u001b[0m \u001b[37m|\u001b[0m         \u001b[34massert\u001b[39;49;00m \u001b[34m1\u001b[39;49;00m == \u001b[34m0\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mone vs. zero\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[37m 9\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mwith\u001b[39;49;00m suite.test(\u001b[33m\"\u001b[39;49;00m\u001b[33merror-raising\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "\u001b[37m10\u001b[0m \u001b[37m|\u001b[0m         \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mSome error\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\u001b[37m11\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mwith\u001b[39;49;00m suite.test(\u001b[33m\"\u001b[39;49;00m\u001b[33mnext-to-last\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "\n",
      "\u001b[31mError occured during test \u001b[1merror-raising\u001b[0m\u001b[0m: RuntimeError -- Some error\n",
      "File <ipython-input-76-0b9ff122e3c7> | Line 10 | Function exercise_suite\n",
      "\u001b[37m 7\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mwith\u001b[39;49;00m suite.test(\u001b[33m\"\u001b[39;49;00m\u001b[33mfailing\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "\u001b[37m 8\u001b[0m \u001b[37m|\u001b[0m         \u001b[34massert\u001b[39;49;00m \u001b[34m1\u001b[39;49;00m == \u001b[34m0\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mone vs. zero\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[37m 9\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mwith\u001b[39;49;00m suite.test(\u001b[33m\"\u001b[39;49;00m\u001b[33merror-raising\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "\u001b[37m10\u001b[0m \u001b[37m|\u001b[0m         \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mSome error\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\u001b[37m11\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mwith\u001b[39;49;00m suite.test(\u001b[33m\"\u001b[39;49;00m\u001b[33mnext-to-last\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "\u001b[37m12\u001b[0m \u001b[37m|\u001b[0m         \u001b[34mpass\u001b[39;49;00m\n",
      "\u001b[37m13\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mwith\u001b[39;49;00m suite.test(\u001b[33m\"\u001b[39;49;00m\u001b[33mlast\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "\n",
      "Test \u001b[1mnext-to-last\u001b[0m passed.\n",
      "Test \u001b[1mlast\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "exercise_suite(Report(verbose=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual check: like previous, but with full detail of each non-success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mfirst-passing\u001b[0m passed.\n",
      "Test \u001b[1msecond-passing\u001b[0m passed.\n",
      "\n",
      "--------------------------------------------\n",
      "Issue encountered ** Test \u001b[1m\u001b[1mfailing\u001b[0m\u001b[0m ** \u001b[33mFailure\u001b[0m\n",
      "one vs. zero\n",
      "\n",
      "\u001b[37mFile <ipython-input-26-39c764bb9450> | Line 35 | Function test\u001b[0m\n",
      "\n",
      "File <ipython-input-76-0b9ff122e3c7> | Line 8 | Function exercise_suite\n",
      "\u001b[37m 5\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mwith\u001b[39;49;00m suite.test(\u001b[33m\"\u001b[39;49;00m\u001b[33msecond-passing\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "\u001b[37m 6\u001b[0m \u001b[37m|\u001b[0m         \u001b[34mpass\u001b[39;49;00m\n",
      "\u001b[37m 7\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mwith\u001b[39;49;00m suite.test(\u001b[33m\"\u001b[39;49;00m\u001b[33mfailing\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "\u001b[37m 8\u001b[0m \u001b[37m|\u001b[0m         \u001b[34massert\u001b[39;49;00m \u001b[34m1\u001b[39;49;00m == \u001b[34m0\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mone vs. zero\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[37m 9\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mwith\u001b[39;49;00m suite.test(\u001b[33m\"\u001b[39;49;00m\u001b[33merror-raising\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "\u001b[37m10\u001b[0m \u001b[37m|\u001b[0m         \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mSome error\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\u001b[37m11\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mwith\u001b[39;49;00m suite.test(\u001b[33m\"\u001b[39;49;00m\u001b[33mnext-to-last\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "\n",
      "------------------------------------------------\n",
      "Issue encountered ** Test \u001b[1m\u001b[1merror-raising\u001b[0m\u001b[0m ** \u001b[31mError\u001b[0m\n",
      "RuntimeError: Some error\n",
      "\n",
      "\u001b[37mFile <ipython-input-26-39c764bb9450> | Line 35 | Function test\u001b[0m\n",
      "\n",
      "File <ipython-input-76-0b9ff122e3c7> | Line 10 | Function exercise_suite\n",
      "\u001b[37m 7\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mwith\u001b[39;49;00m suite.test(\u001b[33m\"\u001b[39;49;00m\u001b[33mfailing\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "\u001b[37m 8\u001b[0m \u001b[37m|\u001b[0m         \u001b[34massert\u001b[39;49;00m \u001b[34m1\u001b[39;49;00m == \u001b[34m0\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mone vs. zero\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[37m 9\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mwith\u001b[39;49;00m suite.test(\u001b[33m\"\u001b[39;49;00m\u001b[33merror-raising\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "\u001b[37m10\u001b[0m \u001b[37m|\u001b[0m         \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mSome error\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\u001b[37m11\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mwith\u001b[39;49;00m suite.test(\u001b[33m\"\u001b[39;49;00m\u001b[33mnext-to-last\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "\u001b[37m12\u001b[0m \u001b[37m|\u001b[0m         \u001b[34mpass\u001b[39;49;00m\n",
      "\u001b[37m13\u001b[0m \u001b[37m|\u001b[0m     \u001b[34mwith\u001b[39;49;00m suite.test(\u001b[33m\"\u001b[39;49;00m\u001b[33mlast\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "\n",
      "-------------------------\n",
      "Test \u001b[1mnext-to-last\u001b[0m passed.\n",
      "Test \u001b[1mlast\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "exercise_suite(Report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exportation of testing tools to `jupytest.py` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"jupytest.ipynb\", \"r\", encoding=\"utf-8\") as file_notebook:\n",
    "    nb = nbformat.read(file_notebook, nbformat.NO_CONVERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"jupytest.py\", \"w\", encoding=\"utf-8\") as file_module:\n",
    "    print(\n",
    "        \"\\n\\n\".join(\n",
    "            cell.source\n",
    "            for cell in nb.cells\n",
    "            if cell.cell_type == 'code' and \"module\" in cell.metadata.get(\"tags\", [])\n",
    "        ),\n",
    "        file=file_module,\n",
    "        end=\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'jupytest' from '/home/hamelin/jupytest/jupytest.py'>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "# Force the reload of the new jupytest module. If running from a fresh\n",
    "# kernel, the reload is spurious but innocuous.\n",
    "import jupytest\n",
    "importlib.reload(jupytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the exported module has all the tools we defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "for construct in [\n",
    "    \"Result\",\n",
    "    \"Success\",\n",
    "    \"Frame\",\n",
    "    \"Error\",\n",
    "    \"Failure\",\n",
    "    \"TestFailed\",\n",
    "    \"fail\",\n",
    "    \"protect_environment\",\n",
    "    \"Subscriber\",\n",
    "    \"Suite\",\n",
    "    \"Emphasis\",\n",
    "    \"Color\",\n",
    "    \"Plain\",\n",
    "    \"Colorizer\",\n",
    "    \"color\",\n",
    "    \"plain\",\n",
    "    \"TestNameFormatter\",\n",
    "    \"ladder\",\n",
    "    \"name_all\",\n",
    "    \"quoter\",\n",
    "    \"report_results\",\n",
    "    \"summarize_results\",\n",
    "    \"print_frame\",\n",
    "    \"detail_result\",\n",
    "    \"detail_errors\",\n",
    "    \"Report\"\n",
    "]:\n",
    "    getattr(jupytest, construct)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
